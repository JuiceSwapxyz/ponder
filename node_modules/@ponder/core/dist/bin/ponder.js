#!/usr/bin/env node
import {
  buildGraphQLSchema,
  getColumnCasing,
  getPrimaryKeyColumns,
  getSql,
  getTableNames,
  graphql,
  isPgEnumSym,
  never,
  onchain,
  serialize,
  userToReorgTableName,
  userToSqlTableName
} from "../chunk-QF63HG2F.js";

// src/bin/ponder.ts
import { readFileSync as readFileSync4 } from "node:fs";
import { dirname, resolve } from "node:path";
import { fileURLToPath } from "node:url";
import { Command } from "@commander-js/extra-typings";
import dotenv from "dotenv";

// src/utils/extend.ts
var extend = (create3, _methods) => {
  return (...params) => {
    const service = create3(...params);
    if (service instanceof Promise) {
      return service.then((s) => {
        const methods3 = {};
        for (const [methodName, method] of Object.entries(_methods)) {
          methods3[methodName] = (...params2) => method(s, ...params2);
        }
        return {
          ...s,
          ...methods3
        };
      });
    } else {
      const methods3 = {};
      for (const [methodName, method] of Object.entries(_methods)) {
        methods3[methodName] = (...params2) => method(service, ...params2);
      }
      return {
        ...service,
        ...methods3
      };
    }
  };
};

// src/build/service.ts
import { createHash } from "node:crypto";
import crypto from "node:crypto";
import fs from "node:fs";
import path2 from "node:path";

// src/common/errors.ts
var BaseError = class _BaseError extends Error {
  name = "BaseError";
  meta = [];
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _BaseError.prototype);
  }
};
function getBaseError(err) {
  if (err instanceof BaseError)
    return err;
  if (err instanceof Error)
    return new BaseError(err.message);
  if (typeof err?.message === "string")
    return new BaseError(err.message);
  if (typeof err === "string")
    return new BaseError(err);
  return new BaseError("unknown error");
}
var BuildError = class _BuildError extends BaseError {
  name = "BuildError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _BuildError.prototype);
  }
};
var NonRetryableError = class _NonRetryableError extends BaseError {
  name = "NonRetryableError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _NonRetryableError.prototype);
  }
};
var IgnorableError = class _IgnorableError extends BaseError {
  name = "IgnorableError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _IgnorableError.prototype);
  }
};
var UniqueConstraintError = class _UniqueConstraintError extends NonRetryableError {
  name = "UniqueConstraintError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _UniqueConstraintError.prototype);
  }
};
var NotNullConstraintError = class _NotNullConstraintError extends NonRetryableError {
  name = "NotNullConstraintError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _NotNullConstraintError.prototype);
  }
};
var RecordNotFoundError = class _RecordNotFoundError extends NonRetryableError {
  name = "RecordNotFoundError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _RecordNotFoundError.prototype);
  }
};
var CheckConstraintError = class _CheckConstraintError extends NonRetryableError {
  name = "CheckConstraintError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _CheckConstraintError.prototype);
  }
};
var InvalidStoreMethodError = class _InvalidStoreMethodError extends NonRetryableError {
  name = "InvalidStoreMethodError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _InvalidStoreMethodError.prototype);
  }
};
var UndefinedTableError = class _UndefinedTableError extends NonRetryableError {
  name = "UndefinedTableError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _UndefinedTableError.prototype);
  }
};
var BigIntSerializationError = class _BigIntSerializationError extends NonRetryableError {
  name = "BigIntSerializationError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _BigIntSerializationError.prototype);
  }
};
var FlushError = class _FlushError extends NonRetryableError {
  name = "FlushError";
  constructor(message) {
    super(message);
    Object.setPrototypeOf(this, _FlushError.prototype);
  }
};

// src/build/service.ts
import { glob } from "glob";
import { createServer } from "vite";
import { ViteNodeRunner } from "vite-node/client";
import { ViteNodeServer } from "vite-node/server";
import { installSourcemapsSupport } from "vite-node/source-map";
import { normalizeModuleId, toFilePath } from "vite-node/utils";
import viteTsconfigPathsPlugin from "vite-tsconfig-paths";

// src/build/configAndIndexingFunctions.ts
import path from "node:path";

// src/utils/chains.ts
import * as _chains from "viem/chains";
var chains = _chains;

// src/config/networks.ts
function getFinalityBlockCount({ chainId }) {
  let finalityBlockCount;
  switch (chainId) {
    case 1:
    case 3:
    case 4:
    case 5:
    case 42:
    case 11155111:
      finalityBlockCount = 65;
      break;
    case 137:
    case 80001:
      finalityBlockCount = 200;
      break;
    case 42161:
    case 42170:
    case 421611:
    case 421613:
      finalityBlockCount = 240;
      break;
    default:
      finalityBlockCount = 30;
  }
  return finalityBlockCount;
}
async function getRpcUrlsForClient(parameters) {
  const { config, value } = parameters.transport({
    chain: parameters.chain,
    pollingInterval: 4e3,
    // default viem value
    retryCount: 0
  });
  const transport = { ...config, ...value };
  async function getRpcUrlsForTransport(transport2) {
    switch (transport2.type) {
      case "http": {
        return [transport2.url ?? parameters.chain.rpcUrls.default.http[0]];
      }
      case "webSocket": {
        try {
          const socket = await transport2.getSocket();
          return [socket.url];
        } catch (e) {
          const symbol = Object.getOwnPropertySymbols(e).find(
            (symbol2) => symbol2.toString() === "Symbol(kTarget)"
          );
          if (!symbol)
            return [];
          const url = e[symbol]?._url;
          if (!url)
            return [];
          return [url.replace(/\/$/, "")];
        }
      }
      case "fallback": {
        const fallbackTransports = transport2.transports.map((t) => ({
          ...t.config,
          ...t.value
        }));
        const urls = [];
        for (const fallbackTransport of fallbackTransports) {
          urls.push(...await getRpcUrlsForTransport(fallbackTransport));
        }
        return urls;
      }
      default: {
        return [];
      }
    }
  }
  return getRpcUrlsForTransport(transport);
}
var publicRpcUrls = void 0;
function isRpcUrlPublic(rpcUrl) {
  if (rpcUrl === void 0)
    return true;
  if (!publicRpcUrls) {
    publicRpcUrls = Object.values(chains).reduce((acc, chain) => {
      chain.rpcUrls.default.http.forEach((httpRpcUrl) => {
        acc.add(httpRpcUrl);
      });
      (chain.rpcUrls.default.webSocket ?? []).forEach((webSocketRpcUrl) => {
        acc.add(webSocketRpcUrl);
      });
      return acc;
    }, /* @__PURE__ */ new Set());
  }
  return publicRpcUrls.has(rpcUrl);
}

// src/utils/duplicates.ts
function getDuplicateElements(arr) {
  const uniqueElements = /* @__PURE__ */ new Set();
  const duplicates = /* @__PURE__ */ new Set();
  arr.forEach((element) => {
    if (uniqueElements.has(element)) {
      duplicates.add(element);
    } else {
      uniqueElements.add(element);
    }
  });
  return duplicates;
}

// src/sync/abi.ts
import {
  formatAbiItem
} from "abitype";
import {
  encodeEventTopics,
  getAbiItem,
  getEventSelector,
  getFunctionSelector,
  parseAbiItem
} from "viem";
var buildAbiEvents = ({ abi }) => {
  const abiEvents = abi.filter((item) => item.type === "event").filter((item) => item.anonymous === void 0 || item.anonymous === false);
  const overloadedEventNames = getDuplicateElements(
    abiEvents.map((item) => item.name)
  );
  return abiEvents.reduce(
    (acc, item) => {
      const signature = formatAbiItem(item);
      const safeName = overloadedEventNames.has(item.name) ? signature.split("event ")[1] : item.name;
      const selector = getEventSelector(item);
      const abiEventMeta = { safeName, signature, selector, item };
      acc.bySafeName[safeName] = abiEventMeta;
      acc.bySelector[selector] = abiEventMeta;
      return acc;
    },
    { bySafeName: {}, bySelector: {} }
  );
};
function buildTopics(abi, filter) {
  if (Array.isArray(filter.event)) {
    return [
      filter.event.map((event) => getEventSelector(findAbiEvent(abi, event)))
    ];
  } else {
    return encodeEventTopics({
      abi: [findAbiEvent(abi, filter.event)],
      args: filter.args
    });
  }
}
var findAbiEvent = (abi, eventName) => {
  if (eventName.includes("(")) {
    return parseAbiItem(`event ${eventName}`);
  } else {
    return getAbiItem({ abi, name: eventName });
  }
};
var buildAbiFunctions = ({ abi }) => {
  const abiFunctions = abi.filter(
    (item) => item.type === "function"
  );
  const overloadedFunctionNames = getDuplicateElements(
    abiFunctions.map((item) => item.name)
  );
  return abiFunctions.reduce(
    (acc, item) => {
      const signature = formatAbiItem(item);
      const safeName = overloadedFunctionNames.has(item.name) ? signature.split("function ")[1] : `${item.name}()`;
      const selector = getFunctionSelector(item);
      const abiEventMeta = { safeName, signature, selector, item };
      acc.bySafeName[safeName] = abiEventMeta;
      acc.bySelector[selector] = abiEventMeta;
      return acc;
    },
    { bySafeName: {}, bySelector: {} }
  );
};

// src/utils/lowercase.ts
function toLowerCase(value) {
  return value.toLowerCase();
}

// ../common/src/promiseWithResolvers.ts
var promiseWithResolvers = () => {
  let resolve2;
  let reject;
  const promise = new Promise((_resolve, _reject) => {
    resolve2 = _resolve;
    reject = _reject;
  });
  return { resolve: resolve2, reject, promise };
};

// ../common/src/dedupe.ts
function dedupe(arr, getId) {
  const seen = /* @__PURE__ */ new Set();
  return arr.filter((x) => {
    if (seen.has(getId ? getId(x) : x))
      return false;
    seen.add(x);
    return true;
  });
}
dedupe(
  [
    { a: 1, b: 2 },
    { a: 1, b: 2 },
    { a: 2, b: 2 }
  ],
  (e) => `${e.a}_${e.b}`
);

// ../common/src/queue.ts
var validateParameters = ({
  concurrency,
  frequency
}) => {
  if (concurrency === void 0 && frequency === void 0) {
    throw new Error(
      "Invalid queue configuration, must specify either 'concurrency' or 'frequency'."
    );
  }
  if (concurrency !== void 0 && concurrency <= 0) {
    throw new Error(
      `Invalid value for queue 'concurrency' option. Got ${concurrency}, expected a number greater than zero.`
    );
  }
  if (frequency !== void 0 && frequency <= 0) {
    throw new Error(
      `Invalid value for queue 'frequency' option. Got ${frequency}, expected a number greater than zero.`
    );
  }
};
var createQueue = ({
  worker,
  initialStart = false,
  browser = true,
  ..._parameters
}) => {
  validateParameters(_parameters);
  const parameters = _parameters;
  let queue = new Array();
  let pending = 0;
  let timestamp = 0;
  let requests = 0;
  let isStarted = initialStart;
  let timer;
  let emptyPromiseWithResolvers = void 0;
  let idlePromiseWithResolvers = void 0;
  const next = () => {
    if (!isStarted)
      return;
    const _timestamp = Date.now();
    if (Math.floor(_timestamp / 1e3) !== timestamp) {
      requests = 0;
      timestamp = Math.floor(_timestamp / 1e3);
    }
    if (timer)
      return;
    while ((parameters.frequency !== void 0 ? requests < parameters.frequency : true) && (parameters.concurrency !== void 0 ? pending < parameters.concurrency : true) && queue.length > 0) {
      const { task, resolve: resolve2, reject } = queue.shift();
      requests++;
      pending++;
      worker(task).then(resolve2).catch(reject).finally(() => {
        pending--;
        if (idlePromiseWithResolvers !== void 0 && queue.length === 0 && pending === 0) {
          idlePromiseWithResolvers.resolve();
          idlePromiseWithResolvers.completed = true;
        }
        browser ? next() : process.nextTick(next);
      });
      if (emptyPromiseWithResolvers !== void 0 && queue.length === 0) {
        emptyPromiseWithResolvers.resolve();
        emptyPromiseWithResolvers.completed = true;
      }
    }
    if (parameters.frequency !== void 0 && requests >= parameters.frequency) {
      timer = setTimeout(
        () => {
          timer = void 0;
          next();
        },
        1e3 - _timestamp % 1e3
      );
      return;
    }
  };
  return {
    size: () => queue.length,
    pending: () => {
      if (browser) {
        return new Promise(
          (resolve2) => setTimeout(() => resolve2(pending))
        );
      } else {
        return new Promise(
          (resolve2) => setImmediate(() => resolve2(pending))
        );
      }
    },
    add: (task) => {
      const { promise, resolve: resolve2, reject } = promiseWithResolvers();
      queue.push({ task, resolve: resolve2, reject });
      next();
      return promise.catch((error) => {
        Error.captureStackTrace(error);
        throw error;
      });
    },
    clear: () => {
      queue = new Array();
      clearTimeout(timer);
      timer = void 0;
    },
    isStarted: () => isStarted,
    start: () => {
      if (browser) {
        return new Promise(
          (resolve2) => setTimeout(() => resolve2(pending))
        ).then(() => {
          isStarted = true;
          next();
        });
      } else {
        return new Promise(
          (resolve2) => process.nextTick(() => resolve2(pending))
        ).then(() => {
          isStarted = true;
          next();
        });
      }
    },
    pause: () => {
      isStarted = false;
    },
    onIdle: () => {
      if (idlePromiseWithResolvers === void 0 || idlePromiseWithResolvers.completed) {
        if (queue.length === 0 && pending === 0)
          return Promise.resolve();
        idlePromiseWithResolvers = {
          ...promiseWithResolvers(),
          completed: false
        };
      }
      return idlePromiseWithResolvers.promise;
    },
    onEmpty: () => {
      if (emptyPromiseWithResolvers === void 0 || emptyPromiseWithResolvers.completed) {
        if (queue.length === 0)
          return Promise.resolve();
        emptyPromiseWithResolvers = {
          ...promiseWithResolvers(),
          completed: false
        };
      }
      return emptyPromiseWithResolvers.promise;
    },
    setParameters: (_parameters2) => {
      validateParameters(_parameters2);
      if ("frequency" in _parameters2) {
        parameters.frequency = _parameters2.frequency;
      }
      if ("concurrency" in _parameters2) {
        parameters.concurrency = _parameters2.concurrency;
      }
    }
  };
};

// src/build/configAndIndexingFunctions.ts
import parse from "pg-connection-string";

// src/utils/offset.ts
import { InvalidAbiDecodingTypeError } from "viem";
function getBytesConsumedByParam(param) {
  const arrayComponents = getArrayComponents(param.type);
  if (arrayComponents) {
    const [length, innerType] = arrayComponents;
    if (!length || hasDynamicChild(param)) {
      return 32;
    }
    const bytesConsumedByInnerType = getBytesConsumedByParam({
      ...param,
      type: innerType
    });
    return length * bytesConsumedByInnerType;
  }
  if (param.type === "tuple") {
    if (hasDynamicChild(param)) {
      return 32;
    }
    let consumed = 0;
    for (const component of param.components ?? []) {
      consumed += getBytesConsumedByParam(component);
    }
    return consumed;
  }
  if (param.type === "string" || param.type.startsWith("bytes") || param.type.startsWith("uint") || param.type.startsWith("int") || param.type === "address" || param.type === "bool") {
    return 32;
  }
  throw new InvalidAbiDecodingTypeError(param.type, {
    docsPath: "/docs/contract/decodeAbiParameters"
  });
}
function hasDynamicChild(param) {
  const { type } = param;
  if (type === "string")
    return true;
  if (type === "bytes")
    return true;
  if (type.endsWith("[]"))
    return true;
  if (type === "tuple")
    return param.components?.some(hasDynamicChild);
  const arrayComponents = getArrayComponents(param.type);
  if (arrayComponents && hasDynamicChild({ ...param, type: arrayComponents[1] }))
    return true;
  return false;
}
function getArrayComponents(type) {
  const matches = type.match(/^(.*)\[(\d+)?\]$/);
  return matches ? (
    // Return `null` if the array is dynamic.
    [matches[2] ? Number(matches[2]) : null, matches[1]]
  ) : void 0;
}

// src/build/factory.ts
import { getEventSelector as getEventSelector2 } from "viem";
function buildLogFactory({
  address: _address,
  event,
  parameter,
  chainId
}) {
  const address = Array.isArray(_address) ? _address.map(toLowerCase) : toLowerCase(_address);
  const eventSelector = getEventSelector2(event);
  const indexedInputPosition = event.inputs.filter((x) => "indexed" in x && x.indexed).findIndex((input) => input.name === parameter);
  if (indexedInputPosition > -1) {
    return {
      type: "log",
      chainId,
      address,
      eventSelector,
      // Add 1 because inputs will not contain an element for topic0 (the signature).
      childAddressLocation: `topic${indexedInputPosition + 1}`
    };
  }
  const nonIndexedInputs = event.inputs.filter(
    (x) => !("indexed" in x && x.indexed)
  );
  const nonIndexedInputPosition = nonIndexedInputs.findIndex(
    (input) => input.name === parameter
  );
  if (nonIndexedInputPosition === -1) {
    throw new Error(
      `Factory event parameter not found in factory event signature. Got '${parameter}', expected one of [${event.inputs.map((i) => `'${i.name}'`).join(", ")}].`
    );
  }
  let offset = 0;
  for (let i = 0; i < nonIndexedInputPosition; i++) {
    offset += getBytesConsumedByParam(nonIndexedInputs[i]);
  }
  return {
    type: "log",
    chainId,
    address,
    eventSelector,
    childAddressLocation: `offset${offset}`
  };
}

// src/build/configAndIndexingFunctions.ts
async function buildConfigAndIndexingFunctions({
  config,
  rawIndexingFunctions,
  options: { rootDir, ponderDir }
}) {
  const logs = [];
  let databaseConfig;
  const pgliteDir = config.database?.kind === "pglite" && config.database.directory ? config.database.directory === "memory://" ? "memory://" : path.resolve(config.database.directory) : path.join(ponderDir, "pglite");
  const pglitePrintPath = pgliteDir === "memory://" ? "memory://" : path.relative(rootDir, pgliteDir);
  if (config.database?.kind) {
    if (config.database.kind === "postgres") {
      let connectionString = void 0;
      let source = void 0;
      if (config.database.connectionString) {
        connectionString = config.database.connectionString;
        source = "from ponder.config.ts";
      } else if (process.env.DATABASE_PRIVATE_URL) {
        connectionString = process.env.DATABASE_PRIVATE_URL;
        source = "from DATABASE_PRIVATE_URL env var";
      } else if (process.env.DATABASE_URL) {
        connectionString = process.env.DATABASE_URL;
        source = "from DATABASE_URL env var";
      } else {
        throw new Error(
          `Invalid database configuration: 'kind' is set to 'postgres' but no connection string was provided.`
        );
      }
      logs.push({
        level: "info",
        msg: `Using Postgres database '${getDatabaseName(connectionString)}' (${source})`
      });
      const poolConfig = {
        max: config.database.poolConfig?.max ?? 30,
        connectionString
      };
      databaseConfig = { kind: "postgres", poolConfig };
    } else {
      logs.push({
        level: "info",
        msg: `Using PGlite database in '${pglitePrintPath}' (from ponder.config.ts)`
      });
      databaseConfig = { kind: "pglite", options: { dataDir: pgliteDir } };
    }
  } else {
    let connectionString = void 0;
    let source = void 0;
    if (process.env.DATABASE_PRIVATE_URL) {
      connectionString = process.env.DATABASE_PRIVATE_URL;
      source = "from DATABASE_PRIVATE_URL env var";
    } else if (process.env.DATABASE_URL) {
      connectionString = process.env.DATABASE_URL;
      source = "from DATABASE_URL env var";
    }
    if (connectionString !== void 0) {
      logs.push({
        level: "info",
        msg: `Using Postgres database ${getDatabaseName(connectionString)} (${source})`
      });
      const poolConfig = { max: 30, connectionString };
      databaseConfig = { kind: "postgres", poolConfig };
    } else {
      logs.push({
        level: "info",
        msg: `Using PGlite database at ${pglitePrintPath} (default)`
      });
      databaseConfig = { kind: "pglite", options: { dataDir: pgliteDir } };
    }
  }
  const networks = await Promise.all(
    Object.entries(config.networks).map(async ([networkName, network]) => {
      const { chainId, transport } = network;
      const defaultChain = Object.values(chains).find(
        (c) => "id" in c ? c.id === chainId : false
      ) ?? chains.mainnet;
      const chain = { ...defaultChain, name: networkName, id: chainId };
      const rpcUrls = await getRpcUrlsForClient({ transport, chain });
      rpcUrls.forEach((rpcUrl) => {
        if (isRpcUrlPublic(rpcUrl)) {
          logs.push({
            level: "warn",
            msg: `Network '${networkName}' is using a public RPC URL (${rpcUrl}). Most apps require an RPC URL with a higher rate limit.`
          });
        }
      });
      if (network.pollingInterval !== void 0 && network.pollingInterval < 100) {
        throw new Error(
          `Invalid 'pollingInterval' for network '${networkName}. Expected 100 milliseconds or greater, got ${network.pollingInterval} milliseconds.`
        );
      }
      return {
        name: networkName,
        chainId,
        chain,
        transport: network.transport({ chain }),
        maxRequestsPerSecond: network.maxRequestsPerSecond ?? 50,
        pollingInterval: network.pollingInterval ?? 1e3,
        finalityBlockCount: getFinalityBlockCount({ chainId }),
        disableCache: network.disableCache ?? false
      };
    })
  );
  let indexingFunctionCount = 0;
  const indexingFunctions = {};
  for (const { name: eventName, fn } of rawIndexingFunctions) {
    const eventNameComponents = eventName.includes(".") ? eventName.split(".") : eventName.split(":");
    const [sourceName, sourceEventName] = eventNameComponents;
    if (eventNameComponents.length !== 2 || !sourceName || !sourceEventName) {
      throw new Error(
        `Validation failed: Invalid event '${eventName}', expected format '{sourceName}:{eventName}' or '{sourceName}.{eventName}'.`
      );
    }
    if (eventName in indexingFunctions) {
      throw new Error(
        `Validation failed: Multiple indexing functions registered for event '${eventName}'.`
      );
    }
    const matchedSourceName = Object.keys({
      ...config.contracts ?? {},
      ...config.blocks ?? {}
    }).find((_sourceName) => _sourceName === sourceName);
    if (!matchedSourceName) {
      const uniqueSourceNames = dedupe(
        Object.keys({ ...config.contracts ?? {}, ...config.blocks ?? {} })
      );
      throw new Error(
        `Validation failed: Invalid source name '${sourceName}'. Got '${sourceName}', expected one of [${uniqueSourceNames.map((n) => `'${n}'`).join(", ")}].`
      );
    }
    indexingFunctions[eventName] = fn;
    indexingFunctionCount += 1;
  }
  if (indexingFunctionCount === 0) {
    logs.push({ level: "warn", msg: "No indexing functions were registered." });
  }
  const contractSources = Object.entries(
    config.contracts ?? {}
  ).flatMap(([contractName, contract]) => {
    if (contract.network === null || contract.network === void 0) {
      throw new Error(
        `Validation failed: Network for contract '${contractName}' is null or undefined. Expected one of [${networks.map((n) => `'${n.name}'`).join(", ")}].`
      );
    }
    const startBlockMaybeNan = contract.startBlock ?? 0;
    const startBlock = Number.isNaN(startBlockMaybeNan) ? 0 : startBlockMaybeNan;
    const endBlockMaybeNan = contract.endBlock;
    const endBlock = Number.isNaN(endBlockMaybeNan) ? void 0 : endBlockMaybeNan;
    if (endBlock !== void 0 && endBlock < startBlock) {
      throw new Error(
        `Validation failed: Start block for contract '${contractName}' is after end block (${startBlock} > ${endBlock}).`
      );
    }
    if (typeof contract.network === "string") {
      return {
        id: `log_${contractName}_${contract.network}`,
        name: contractName,
        networkName: contract.network,
        abi: contract.abi,
        address: "address" in contract ? contract.address : void 0,
        factory: "factory" in contract ? contract.factory : void 0,
        filter: contract.filter,
        includeTransactionReceipts: contract.includeTransactionReceipts ?? false,
        includeCallTraces: contract.includeCallTraces ?? false,
        startBlock,
        endBlock
      };
    }
    return Object.entries(contract.network).filter((n) => !!n[1]).map(([networkName, overrides]) => {
      const startBlockMaybeNan2 = overrides.startBlock ?? contract.startBlock ?? 0;
      const startBlock2 = Number.isNaN(startBlockMaybeNan2) ? 0 : startBlockMaybeNan2;
      const endBlockMaybeNan2 = overrides.endBlock ?? contract.endBlock;
      const endBlock2 = Number.isNaN(endBlockMaybeNan2) ? void 0 : endBlockMaybeNan2;
      if (endBlock2 !== void 0 && endBlock2 < startBlock2) {
        throw new Error(
          `Validation failed: Start block for contract '${contractName}' is after end block (${startBlock2} > ${endBlock2}).`
        );
      }
      return {
        name: contractName,
        networkName,
        abi: contract.abi,
        address: ("address" in overrides ? overrides?.address : void 0) ?? ("address" in contract ? contract.address : void 0),
        factory: ("factory" in overrides ? overrides.factory : void 0) ?? ("factory" in contract ? contract.factory : void 0),
        filter: overrides.filter ?? contract.filter,
        includeTransactionReceipts: overrides.includeTransactionReceipts ?? contract.includeTransactionReceipts ?? false,
        includeCallTraces: overrides.includeCallTraces ?? contract.includeCallTraces ?? false,
        startBlock: startBlock2,
        endBlock: endBlock2
      };
    });
  }).flatMap((rawContract) => {
    const network = networks.find((n) => n.name === rawContract.networkName);
    if (!network) {
      throw new Error(
        `Validation failed: Invalid network for contract '${rawContract.name}'. Got '${rawContract.networkName}', expected one of [${networks.map((n) => `'${n.name}'`).join(", ")}].`
      );
    }
    const registeredLogEvents = [];
    const registeredCallTraceEvents = [];
    for (const eventName of Object.keys(indexingFunctions)) {
      if (eventName.includes(":")) {
        const [logContractName, logEventName] = eventName.split(":");
        if (logContractName === rawContract.name && logEventName !== "setup") {
          registeredLogEvents.push(logEventName);
        }
      }
      if (eventName.includes(".")) {
        const [functionContractName, functionName] = eventName.split(".");
        if (functionContractName === rawContract.name) {
          registeredCallTraceEvents.push(functionName);
        }
      }
    }
    const abiEvents = buildAbiEvents({ abi: rawContract.abi });
    const abiFunctions = buildAbiFunctions({ abi: rawContract.abi });
    const registeredEventSelectors = [];
    for (const logEvent of registeredLogEvents) {
      const abiEvent = abiEvents.bySafeName[logEvent];
      if (abiEvent === void 0) {
        throw new Error(
          `Validation failed: Event name for event '${logEvent}' not found in the contract ABI. Got '${logEvent}', expected one of [${Object.keys(
            abiEvents.bySafeName
          ).map((eventName) => `'${eventName}'`).join(", ")}].`
        );
      }
      registeredEventSelectors.push(abiEvent.selector);
    }
    const registeredFunctionSelectors = [];
    for (const _function of registeredCallTraceEvents) {
      const abiFunction = abiFunctions.bySafeName[_function];
      if (abiFunction === void 0) {
        throw new Error(
          `Validation failed: Function name for function '${_function}' not found in the contract ABI. Got '${_function}', expected one of [${Object.keys(
            abiFunctions.bySafeName
          ).map((eventName) => `'${eventName}'`).join(", ")}].`
        );
      }
      registeredFunctionSelectors.push(abiFunction.selector);
    }
    let topics = [registeredEventSelectors];
    if (rawContract.filter !== void 0) {
      if (Array.isArray(rawContract.filter.event) && rawContract.filter.args !== void 0) {
        throw new Error(
          `Validation failed: Event filter for contract '${rawContract.name}' cannot contain indexed argument values if multiple events are provided.`
        );
      }
      const filterSafeEventNames = Array.isArray(rawContract.filter.event) ? rawContract.filter.event : [rawContract.filter.event];
      for (const filterSafeEventName of filterSafeEventNames) {
        const abiEvent = abiEvents.bySafeName[filterSafeEventName];
        if (!abiEvent) {
          throw new Error(
            `Validation failed: Invalid filter for contract '${rawContract.name}'. Got event name '${filterSafeEventName}', expected one of [${Object.keys(
              abiEvents.bySafeName
            ).map((n) => `'${n}'`).join(", ")}].`
          );
        }
      }
      const [topic0FromFilter, ...topicsFromFilter] = buildTopics(
        rawContract.abi,
        rawContract.filter
      );
      const filteredEventSelectors = Array.isArray(topic0FromFilter) ? topic0FromFilter : [topic0FromFilter];
      for (const registeredEventSelector of registeredEventSelectors) {
        if (!filteredEventSelectors.includes(registeredEventSelector)) {
          const logEventName = abiEvents.bySelector[registeredEventSelector].safeName;
          throw new Error(
            `Validation failed: Event '${logEventName}' is excluded by the event filter defined on the contract '${rawContract.name}'. Got '${logEventName}', expected one of [${filteredEventSelectors.map((s) => abiEvents.bySelector[s].safeName).map((eventName) => `'${eventName}'`).join(", ")}].`
          );
        }
      }
      topics = [registeredEventSelectors, ...topicsFromFilter];
    }
    const contractMetadata = {
      type: "contract",
      abi: rawContract.abi,
      abiEvents,
      abiFunctions,
      name: rawContract.name,
      networkName: rawContract.networkName
    };
    const resolvedFactory = rawContract?.factory;
    const resolvedAddress = rawContract?.address;
    if (resolvedFactory !== void 0 && resolvedAddress !== void 0) {
      throw new Error(
        `Validation failed: Contract '${contractMetadata.name}' cannot specify both 'factory' and 'address' options.`
      );
    }
    if (resolvedFactory) {
      const logFactory = buildLogFactory({
        chainId: network.chainId,
        ...resolvedFactory
      });
      const logSource2 = {
        ...contractMetadata,
        filter: {
          type: "log",
          chainId: network.chainId,
          address: logFactory,
          topics,
          includeTransactionReceipts: rawContract.includeTransactionReceipts,
          fromBlock: rawContract.startBlock,
          toBlock: rawContract.endBlock
        }
      };
      if (rawContract.includeCallTraces) {
        return [
          logSource2,
          {
            ...contractMetadata,
            filter: {
              type: "callTrace",
              chainId: network.chainId,
              fromAddress: void 0,
              toAddress: logFactory,
              functionSelectors: registeredFunctionSelectors,
              includeTransactionReceipts: rawContract.includeTransactionReceipts,
              fromBlock: rawContract.startBlock,
              toBlock: rawContract.endBlock
            }
          }
        ];
      }
      return [logSource2];
    }
    if (resolvedAddress !== void 0) {
      for (const address of Array.isArray(resolvedAddress) ? resolvedAddress : [resolvedAddress]) {
        if (!address.startsWith("0x"))
          throw new Error(
            `Validation failed: Invalid prefix for address '${address}'. Got '${address.slice(
              0,
              2
            )}', expected '0x'.`
          );
        if (address.length !== 42)
          throw new Error(
            `Validation failed: Invalid length for address '${address}'. Got ${address.length}, expected 42 characters.`
          );
      }
    }
    const validatedAddress = Array.isArray(resolvedAddress) ? resolvedAddress.map((r) => toLowerCase(r)) : resolvedAddress !== void 0 ? toLowerCase(resolvedAddress) : void 0;
    const logSource = {
      ...contractMetadata,
      filter: {
        type: "log",
        chainId: network.chainId,
        address: validatedAddress,
        topics,
        includeTransactionReceipts: rawContract.includeTransactionReceipts,
        fromBlock: rawContract.startBlock,
        toBlock: rawContract.endBlock
      }
    };
    if (rawContract.includeCallTraces) {
      return [
        logSource,
        {
          ...contractMetadata,
          filter: {
            type: "callTrace",
            chainId: network.chainId,
            fromAddress: void 0,
            toAddress: Array.isArray(validatedAddress) ? validatedAddress : validatedAddress === void 0 ? void 0 : [validatedAddress],
            functionSelectors: registeredFunctionSelectors,
            includeTransactionReceipts: rawContract.includeTransactionReceipts,
            fromBlock: rawContract.startBlock,
            toBlock: rawContract.endBlock
          }
        }
      ];
    } else
      return [logSource];
  }).filter((source) => {
    const hasRegisteredIndexingFunctions = source.filter.type === "callTrace" ? source.filter.functionSelectors.length !== 0 : source.filter.topics[0]?.length !== 0;
    if (!hasRegisteredIndexingFunctions) {
      logs.push({
        level: "debug",
        msg: `No indexing functions were registered for '${source.name}' ${source.filter.type === "callTrace" ? "call traces" : "logs"}`
      });
    }
    return hasRegisteredIndexingFunctions;
  });
  const blockSources = Object.entries(config.blocks ?? {}).flatMap(([sourceName, blockSourceConfig]) => {
    const startBlockMaybeNan = blockSourceConfig.startBlock ?? 0;
    const startBlock = Number.isNaN(startBlockMaybeNan) ? 0 : startBlockMaybeNan;
    const endBlockMaybeNan = blockSourceConfig.endBlock;
    const endBlock = Number.isNaN(endBlockMaybeNan) ? void 0 : endBlockMaybeNan;
    if (endBlock !== void 0 && endBlock < startBlock) {
      throw new Error(
        `Validation failed: Start block for block source '${sourceName}' is after end block (${startBlock} > ${endBlock}).`
      );
    }
    if (typeof blockSourceConfig.network === "string") {
      const network = networks.find(
        (n) => n.name === blockSourceConfig.network
      );
      if (!network) {
        throw new Error(
          `Validation failed: Invalid network for block source '${sourceName}'. Got '${blockSourceConfig.network}', expected one of [${networks.map((n) => `'${n.name}'`).join(", ")}].`
        );
      }
      const intervalMaybeNan = blockSourceConfig.interval ?? 1;
      const interval = Number.isNaN(intervalMaybeNan) ? 0 : intervalMaybeNan;
      if (!Number.isInteger(interval) || interval === 0) {
        throw new Error(
          `Validation failed: Invalid interval for block source '${sourceName}'. Got ${interval}, expected a non-zero integer.`
        );
      }
      return {
        type: "block",
        name: sourceName,
        networkName: blockSourceConfig.network,
        filter: {
          type: "block",
          chainId: network.chainId,
          interval,
          offset: startBlock % interval,
          fromBlock: startBlock,
          toBlock: endBlock
        }
      };
    }
    return Object.entries(blockSourceConfig.network).filter((n) => !!n[1]).map(([networkName, overrides]) => {
      const network = networks.find((n) => n.name === networkName);
      if (!network) {
        throw new Error(
          `Validation failed: Invalid network for block source '${sourceName}'. Got '${networkName}', expected one of [${networks.map((n) => `'${n.name}'`).join(", ")}].`
        );
      }
      const startBlockMaybeNan2 = overrides.startBlock ?? blockSourceConfig.startBlock ?? 0;
      const startBlock2 = Number.isNaN(startBlockMaybeNan2) ? 0 : startBlockMaybeNan2;
      const endBlockMaybeNan2 = overrides.endBlock ?? blockSourceConfig.endBlock;
      const endBlock2 = Number.isNaN(endBlockMaybeNan2) ? void 0 : endBlockMaybeNan2;
      if (endBlock2 !== void 0 && endBlock2 < startBlock2) {
        throw new Error(
          `Validation failed: Start block for block source '${sourceName}' is after end block (${startBlock2} > ${endBlock2}).`
        );
      }
      const intervalMaybeNan = overrides.interval ?? blockSourceConfig.interval ?? 0;
      const interval = Number.isNaN(intervalMaybeNan) ? 0 : intervalMaybeNan;
      if (!Number.isInteger(interval) || interval === 0) {
        throw new Error(
          `Validation failed: Invalid interval for block source '${sourceName}'. Got ${interval}, expected a non-zero integer.`
        );
      }
      return {
        type: "block",
        name: sourceName,
        networkName,
        filter: {
          type: "block",
          chainId: network.chainId,
          interval,
          offset: startBlock2 % interval,
          fromBlock: startBlock2,
          toBlock: endBlock2
        }
      };
    });
  }).filter((blockSource) => {
    const hasRegisteredIndexingFunction = indexingFunctions[`${blockSource.name}:block`] !== void 0;
    if (!hasRegisteredIndexingFunction) {
      logs.push({
        level: "debug",
        msg: `No indexing functions were registered for '${blockSource.name}' blocks`
      });
    }
    return hasRegisteredIndexingFunction;
  });
  const sources = [...contractSources, ...blockSources];
  const networksWithSources = networks.filter((network) => {
    const hasSources = sources.some(
      (source) => source.networkName === network.name
    );
    if (!hasSources) {
      logs.push({
        level: "warn",
        msg: `No sources registered for network '${network.name}'`
      });
    }
    return hasSources;
  });
  if (Object.keys(indexingFunctions).length === 0) {
    throw new Error(
      "Validation failed: Found 0 registered indexing functions."
    );
  }
  return {
    databaseConfig,
    networks: networksWithSources,
    sources,
    indexingFunctions,
    logs
  };
}
async function safeBuildConfigAndIndexingFunctions({
  config,
  rawIndexingFunctions,
  options
}) {
  try {
    const result = await buildConfigAndIndexingFunctions({
      config,
      rawIndexingFunctions,
      options
    });
    return {
      status: "success",
      sources: result.sources,
      networks: result.networks,
      indexingFunctions: result.indexingFunctions,
      databaseConfig: result.databaseConfig,
      logs: result.logs
    };
  } catch (_error) {
    const buildError = new BuildError(_error.message);
    buildError.stack = void 0;
    return { status: "error", error: buildError };
  }
}
function getDatabaseName(connectionString) {
  const parsed = parse(connectionString);
  return `${parsed.host}:${parsed.port}/${parsed.database}`;
}

// src/build/plugin.ts
var virtualModule = () => `import { Hono } from "hono";

const ponderHono = {
  routes: [],
  get(...maybePathOrHandlers) {
    this.routes.push({ method: "GET", pathOrHandlers: maybePathOrHandlers });
    return this;
  },
  post(...maybePathOrHandlers) {
    this.routes.push({ method: "POST", pathOrHandlers: maybePathOrHandlers });
    return this;
  },
  use(...maybePathOrHandlers) {
    this.routes.push({ method: "USE", pathOrHandlers: maybePathOrHandlers });
    return this;
  },
};

const ponder = {
  ...ponderHono,
  hono: new Hono(),
  fns: [],
  on(name, fn) {
    this.fns.push({ name, fn });
  },
};

export { ponder };
`;
var vitePluginPonder = () => {
  return {
    name: "ponder",
    load: (id) => {
      if (id === "@/generated")
        return virtualModule();
      return null;
    }
  };
};

// src/build/schema.ts
import { SQL, getTableColumns, is } from "drizzle-orm";
import {
  PgBigSerial53,
  PgBigSerial64,
  PgSchema,
  PgSequence,
  PgSerial,
  PgSmallSerial,
  PgTable,
  PgView,
  getTableConfig
} from "drizzle-orm/pg-core";
var buildSchema = ({
  schema,
  instanceId
}) => {
  const statements = getSql(schema, instanceId);
  let namespace;
  for (const maybeSchema of Object.values(schema)) {
    if (is(maybeSchema, PgSchema)) {
      namespace = maybeSchema.schemaName;
      break;
    }
  }
  if (namespace === void 0) {
    namespace = "public";
  }
  for (const [name, s] of Object.entries(schema)) {
    if (is(s, PgTable)) {
      if (namespace === "public" && getTableConfig(s).schema !== void 0) {
        throw new Error(
          `Schema validation failed: All tables must use the same schema and ${name} uses a different schema '${getTableConfig(s).schema}' than '${namespace}'.`
        );
      }
      if (namespace !== "public" && getTableConfig(s).schema !== namespace) {
        throw new Error(
          `Schema validation failed: All tables  must use the same schema and ${name} uses a different schema '${getTableConfig(s).schema ?? "public"}' than '${namespace}'.`
        );
      }
      let hasPrimaryKey = false;
      for (const [columnName, column] of Object.entries(getTableColumns(s))) {
        if (column.primary) {
          if (hasPrimaryKey) {
            throw new Error(
              `Schema validation failed: '${name}' has multiple primary keys.`
            );
          } else {
            hasPrimaryKey = true;
          }
        }
        if (column instanceof PgSerial || column instanceof PgSmallSerial || column instanceof PgBigSerial53 || column instanceof PgBigSerial64) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' has a serial column and serial columns are unsupported.`
          );
        }
        if (column.isUnique) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' has a unique constraint and unique constraints are unsupported.`
          );
        }
        if (column.generated !== void 0) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' is a generated column and generated columns are unsupported.`
          );
        }
        if (column.generatedIdentity !== void 0) {
          throw new Error(
            `Schema validation failed: '${name}.${columnName}' is a generated column and generated columns are unsupported.`
          );
        }
        if (column.hasDefault) {
          if (column.default && column.default instanceof SQL) {
            throw new Error(
              `Schema validation failed: '${name}.${columnName}' is a default column and default columns with raw sql are unsupported.`
            );
          }
          if (column.defaultFn && column.defaultFn() instanceof SQL) {
            throw new Error(
              `Schema validation failed: '${name}.${columnName}' is a default column and default columns with raw sql are unsupported.`
            );
          }
          if (column.onUpdateFn && column.onUpdateFn() instanceof SQL) {
            throw new Error(
              `Schema validation failed: '${name}.${columnName}' is a default column and default columns with raw sql are unsupported.`
            );
          }
        }
      }
      if (getTableConfig(s).primaryKeys.length > 1) {
        throw new Error(
          `Schema validation failed: '${name}' has multiple primary keys.`
        );
      }
      if (getTableConfig(s).primaryKeys.length === 1 && hasPrimaryKey) {
        throw new Error(
          `Schema validation failed: '${name}' has multiple primary keys.`
        );
      }
      if (getTableConfig(s).primaryKeys.length === 0 && hasPrimaryKey === false) {
        throw new Error(
          `Schema validation failed: '${name}' has no primary key. Declare one with ".primaryKey()".`
        );
      }
      if (getTableConfig(s).foreignKeys.length > 0) {
        throw new Error(
          `Schema validation failed: '${name}' has a foreign key constraint and foreign key constraints are unsupported.`
        );
      }
      if (getTableConfig(s).checks.length > 0) {
        throw new Error(
          `Schema validation failed: '${name}' has a check constraint and check constraints are unsupported.`
        );
      }
      if (getTableConfig(s).uniqueConstraints.length > 0) {
        throw new Error(
          `Schema validation failed: '${name}' has a unique constraint and unique constraints are unsupported.`
        );
      }
    }
    if (is(s, PgSequence)) {
      throw new Error(
        `Schema validation failed: '${name}' is a sequence and sequences are unsupported.`
      );
    }
    if (is(s, PgView)) {
      throw new Error(
        `Schema validation failed: '${name}' is a view and views are unsupported.`
      );
    }
    if (isPgEnumSym in s) {
      if (namespace === "public" && s.schema !== void 0) {
        throw new Error(
          // @ts-ignore
          `Schema validation failed: All enums must use the same schema and ${name} uses a different schema '${s.schema}' than '${namespace}'.`
        );
      }
      if (namespace !== "public" && s.schema !== namespace) {
        throw new Error(
          // @ts-ignore
          `Schema validation failed: All enums must use the same schema and ${name} uses a different schema '${s.schema ?? "public"}' than '${namespace}'.`
        );
      }
    }
  }
  return { statements, namespace };
};
var safeBuildSchema = ({
  schema,
  instanceId
}) => {
  try {
    const result = buildSchema({ schema, instanceId });
    const graphqlSchema = buildGraphQLSchema(schema);
    return {
      status: "success",
      ...result,
      graphqlSchema
    };
  } catch (_error) {
    const buildError = new BuildError(_error.message);
    buildError.stack = void 0;
    return { status: "error", error: buildError };
  }
};

// src/build/stacktrace.ts
import { readFileSync } from "node:fs";
import { codeFrameColumns } from "@babel/code-frame";
import { parse as parseStackTrace } from "stacktrace-parser";
var ESBuildTransformError = class extends Error {
  name = "ESBuildTransformError";
};
var ESBuildBuildError = class extends Error {
  name = "ESBuildBuildError";
};
var ESBuildContextError = class extends Error {
  name = "ESBuildContextError";
};
function parseViteNodeError(file, error) {
  let resolvedError;
  if (/^(Transform failed|Build failed|Context failed)/.test(error.message)) {
    const errorKind = error.message.split(" with ")[0];
    const innerError = error.message.split("\n").slice(1).map((message) => {
      let location = void 0;
      let detail = void 0;
      if (message.includes(": ERROR: ")) {
        const s = message.split(": ERROR: ");
        location = s[0];
        detail = s[1];
      } else {
        detail = message.slice(7);
      }
      return { location, detail };
    })[0];
    if (!innerError)
      return error;
    resolvedError = errorKind === "Transform failed" ? new ESBuildTransformError(innerError.detail) : errorKind === "Build failed" ? new ESBuildBuildError(innerError.detail) : new ESBuildContextError(innerError.detail);
    if (innerError.location)
      resolvedError.stack = `    at ${innerError.location}`;
  } else if (error.stack) {
    const stackFrames = parseStackTrace(error.stack);
    const userStackFrames = [];
    for (const rawStackFrame of stackFrames) {
      if (rawStackFrame.methodName.includes("ViteNodeRunner.runModule"))
        break;
      userStackFrames.push(rawStackFrame);
    }
    const userStack = userStackFrames.map(({ file: file2, lineNumber, column, methodName }) => {
      const prefix = "    at";
      const path9 = `${file2}${lineNumber !== null ? `:${lineNumber}` : ""}${column !== null ? `:${column}` : ""}`;
      if (methodName === null || methodName === "<unknown>") {
        return `${prefix} ${path9}`;
      } else {
        return `${prefix} ${methodName} (${path9})`;
      }
    }).join("\n");
    resolvedError = error;
    resolvedError.stack = userStack;
  } else {
    resolvedError = error;
  }
  if (resolvedError.stack) {
    const userStackFrames = parseStackTrace(resolvedError.stack);
    let codeFrame = void 0;
    for (const { file: file2, lineNumber, column } of userStackFrames) {
      if (file2 !== null && lineNumber !== null) {
        try {
          const sourceFileContents = readFileSync(file2, { encoding: "utf-8" });
          codeFrame = codeFrameColumns(
            sourceFileContents,
            { start: { line: lineNumber, column: column ?? void 0 } },
            { highlightCode: true }
          );
          break;
        } catch (err) {
        }
      }
    }
    resolvedError.stack = `${resolvedError.name}: ${resolvedError.message}
${resolvedError.stack}`;
    if (codeFrame)
      resolvedError.stack += `
${codeFrame}`;
  }
  const verb = resolvedError.name === "ESBuildTransformError" ? "transforming" : resolvedError.name === "ESBuildBuildError" || resolvedError.name === "ESBuildContextError" ? "building" : "executing";
  try {
    resolvedError.message = `Error while ${verb} ${file}: ${resolvedError.message}`;
  } catch (e) {
  }
  return resolvedError;
}

// src/build/service.ts
var BUILD_ID_VERSION = "1";
var create = async ({
  common
}) => {
  const escapeRegex = /[.*+?^${}()|[\]\\]/g;
  const escapedIndexingDir = common.options.indexingDir.replace(/\\/g, "/").replace(escapeRegex, "\\$&");
  const indexingRegex = new RegExp(`^${escapedIndexingDir}/.*\\.(ts|js)$`);
  const escapedApiDir = common.options.apiDir.replace(/\\/g, "/").replace(escapeRegex, "\\$&");
  const apiRegex = new RegExp(`^${escapedApiDir}/.*\\.(ts|js)$`);
  const indexingPattern = path2.join(common.options.indexingDir, "**/*.{js,mjs,ts,mts}").replace(/\\/g, "/");
  const apiPattern = path2.join(common.options.apiDir, "**/*.{js,mjs,ts,mts}").replace(/\\/g, "/");
  const viteLogger = {
    warnedMessages: /* @__PURE__ */ new Set(),
    loggedErrors: /* @__PURE__ */ new WeakSet(),
    hasWarned: false,
    clearScreen() {
    },
    hasErrorLogged: (error) => viteLogger.loggedErrors.has(error),
    info: (msg) => {
      common.logger.trace({ service: "build(vite)", msg });
    },
    warn: (msg) => {
      viteLogger.hasWarned = true;
      common.logger.trace({ service: "build(vite)", msg });
    },
    warnOnce: (msg) => {
      if (viteLogger.warnedMessages.has(msg))
        return;
      viteLogger.hasWarned = true;
      common.logger.trace({ service: "build(vite)", msg });
      viteLogger.warnedMessages.add(msg);
    },
    error: (msg) => {
      viteLogger.hasWarned = true;
      common.logger.trace({ service: "build(vite)", msg });
    }
  };
  const viteDevServer = await createServer({
    root: common.options.rootDir,
    cacheDir: path2.join(common.options.ponderDir, "vite"),
    publicDir: false,
    customLogger: viteLogger,
    server: { hmr: false },
    plugins: [viteTsconfigPathsPlugin(), vitePluginPonder()]
  });
  await viteDevServer.pluginContainer.buildStart({});
  const viteNodeServer = new ViteNodeServer(viteDevServer);
  installSourcemapsSupport({
    getSourceMap: (source) => viteNodeServer.getSourceMap(source)
  });
  const viteNodeRunner = new ViteNodeRunner({
    root: viteDevServer.config.root,
    fetchModule: (id) => viteNodeServer.fetchModule(id, "ssr"),
    resolveId: (id, importer) => viteNodeServer.resolveId(id, importer, "ssr")
  });
  return {
    common,
    indexingRegex,
    apiRegex,
    indexingPattern,
    apiPattern,
    viteDevServer,
    viteNodeServer,
    viteNodeRunner
  };
};
var start = async (buildService, {
  watch,
  onBuild
}) => {
  const { common } = buildService;
  if (common.options.command !== "serve") {
    globalThis.__PONDER_INSTANCE_ID = process.env.PONDER_EXPERIMENTAL_INSTANCE_ID ?? crypto.randomBytes(2).toString("hex");
  }
  const configResult = await executeConfig(buildService);
  const schemaResult = await executeSchema(buildService);
  const indexingResult = await executeIndexingFunctions(buildService);
  const apiResult = await executeApiRoutes(buildService);
  if (configResult.status === "error") {
    return { status: "error", error: configResult.error };
  }
  if (schemaResult.status === "error") {
    return { status: "error", error: schemaResult.error };
  }
  if (indexingResult.status === "error") {
    return { status: "error", error: indexingResult.error };
  }
  if (apiResult.status === "error") {
    return { status: "error", error: apiResult.error };
  }
  let cachedConfigResult = configResult;
  let cachedSchemaResult = schemaResult;
  let cachedIndexingResult = indexingResult;
  let cachedApiResult = apiResult;
  if (watch) {
    const ignoredDirs = [common.options.generatedDir, common.options.ponderDir];
    const ignoredFiles = [
      path2.join(common.options.rootDir, "ponder-env.d.ts"),
      path2.join(common.options.rootDir, ".env.local")
    ];
    const isFileIgnored = (filePath) => {
      const isInIgnoredDir = ignoredDirs.some((dir) => {
        const rel = path2.relative(dir, filePath);
        return !rel.startsWith("..") && !path2.isAbsolute(rel);
      });
      const isIgnoredFile = ignoredFiles.includes(filePath);
      return isInIgnoredDir || isIgnoredFile;
    };
    const onFileChange = async (_file) => {
      if (isFileIgnored(_file))
        return;
      const file = toFilePath(
        normalizeModuleId(_file),
        common.options.rootDir
      ).path;
      const invalidated = [
        ...buildService.viteNodeRunner.moduleCache.invalidateDepTree([file])
      ];
      if (invalidated.length === 0)
        return;
      const hasConfigUpdate = invalidated.includes(
        common.options.configFile.replace(/\\/g, "/")
      );
      const hasSchemaUpdate = invalidated.includes(
        common.options.schemaFile.replace(/\\/g, "/")
      );
      const hasIndexingUpdate = invalidated.some(
        (file2) => buildService.indexingRegex.test(file2) && !buildService.apiRegex.test(file2)
      );
      const hasApiUpdate = invalidated.some(
        (file2) => buildService.apiRegex.test(file2)
      );
      if (!hasConfigUpdate && !hasSchemaUpdate && !hasIndexingUpdate && !hasApiUpdate) {
        return;
      }
      common.logger.info({
        service: "build",
        msg: `Hot reload ${invalidated.map((f) => `'${path2.relative(common.options.rootDir, f)}'`).join(", ")}`
      });
      if (hasIndexingUpdate || hasSchemaUpdate || hasConfigUpdate) {
        globalThis.__PONDER_INSTANCE_ID = process.env.PONDER_EXPERIMENTAL_INSTANCE_ID ?? crypto.randomBytes(2).toString("hex");
        buildService.viteNodeRunner.moduleCache.invalidateDepTree([
          buildService.common.options.configFile
        ]);
        buildService.viteNodeRunner.moduleCache.invalidateDepTree([
          buildService.common.options.schemaFile
        ]);
        buildService.viteNodeRunner.moduleCache.invalidateDepTree(
          glob.sync(buildService.indexingPattern, {
            ignore: buildService.apiPattern
          })
        );
        buildService.viteNodeRunner.moduleCache.deleteByModuleId("@/generated");
        const configResult2 = await executeConfig(buildService);
        const schemaResult2 = await executeSchema(buildService);
        const indexingResult2 = await executeIndexingFunctions(buildService);
        if (configResult2.status === "error") {
          onBuild({
            status: "error",
            kind: "indexing",
            error: configResult2.error
          });
          return;
        }
        if (schemaResult2.status === "error") {
          onBuild({
            status: "error",
            kind: "indexing",
            error: schemaResult2.error
          });
          return;
        }
        if (indexingResult2.status === "error") {
          onBuild({
            status: "error",
            kind: "indexing",
            error: indexingResult2.error
          });
          return;
        }
        cachedConfigResult = configResult2;
        cachedSchemaResult = schemaResult2;
        cachedIndexingResult = indexingResult2;
      }
      if (hasApiUpdate) {
        const files = glob.sync(buildService.apiPattern);
        buildService.viteNodeRunner.moduleCache.invalidateDepTree(files);
        buildService.viteNodeRunner.moduleCache.deleteByModuleId("@/generated");
        const result = await executeApiRoutes(buildService);
        if (result.status === "error") {
          onBuild({ status: "error", kind: "api", error: result.error });
          return;
        }
        cachedApiResult = result;
      }
      const indexingBuildResult = await validateAndBuild(
        buildService,
        cachedConfigResult,
        cachedSchemaResult,
        cachedIndexingResult
      );
      if (indexingBuildResult.status === "error") {
        onBuild({
          status: "error",
          kind: "indexing",
          error: indexingBuildResult.error
        });
        return;
      }
      if (hasConfigUpdate || hasSchemaUpdate || hasIndexingUpdate) {
        const apiBuildResult = validateAndBuildApi(
          buildService,
          indexingBuildResult.build,
          cachedApiResult
        );
        if (apiBuildResult.status === "error") {
          onBuild({
            status: "error",
            kind: "api",
            error: apiBuildResult.error
          });
          return;
        }
        onBuild({
          status: "success",
          kind: "indexing",
          indexingBuild: indexingBuildResult.build,
          apiBuild: apiBuildResult.build
        });
      } else {
        const apiBuildResult = validateAndBuildApi(
          buildService,
          indexingBuildResult.build,
          cachedApiResult
        );
        if (apiBuildResult.status === "error") {
          onBuild({
            status: "error",
            kind: "api",
            error: apiBuildResult.error
          });
          return;
        }
        onBuild({
          status: "success",
          kind: "api",
          apiBuild: apiBuildResult.build
        });
      }
    };
    buildService.viteDevServer.watcher.on("change", onFileChange);
  }
  const initialBuildResult = await validateAndBuild(
    buildService,
    configResult,
    schemaResult,
    indexingResult
  );
  if (initialBuildResult.status === "error") {
    return {
      status: "error",
      error: initialBuildResult.error
    };
  }
  const initialApiBuildResult = validateAndBuildApi(
    buildService,
    initialBuildResult.build,
    apiResult
  );
  if (initialApiBuildResult.status === "error") {
    return {
      status: "error",
      error: initialApiBuildResult.error
    };
  }
  return {
    status: "success",
    indexingBuild: initialBuildResult.build,
    apiBuild: initialApiBuildResult.build
  };
};
var kill = async (buildService) => {
  await buildService.viteDevServer?.close();
  buildService.common.logger.debug({
    service: "build",
    msg: "Killed build service"
  });
};
var executeConfig = async (buildService) => {
  const executeResult = await executeFile(buildService, {
    file: buildService.common.options.configFile
  });
  if (executeResult.status === "error") {
    buildService.common.logger.error({
      service: "build",
      msg: "Error while executing 'ponder.config.ts':",
      error: executeResult.error
    });
    return executeResult;
  }
  const config = executeResult.exports.default;
  const contentHash = createHash("sha256").update(serialize(config)).digest("hex");
  return { status: "success", config, contentHash };
};
var executeSchema = async (buildService) => {
  const executeResult = await executeFile(buildService, {
    file: buildService.common.options.schemaFile
  });
  if (executeResult.status === "error") {
    buildService.common.logger.error({
      service: "build",
      msg: "Error while executing 'ponder.schema.ts':",
      error: executeResult.error
    });
    return executeResult;
  }
  const schema = executeResult.exports;
  const contents = fs.readFileSync(
    buildService.common.options.schemaFile,
    "utf-8"
  );
  return {
    status: "success",
    schema,
    contentHash: createHash("sha256").update(contents).digest("hex")
  };
};
var executeIndexingFunctions = async (buildService) => {
  const files = glob.sync(buildService.indexingPattern, {
    ignore: buildService.apiPattern
  });
  const executeResults = await Promise.all(
    files.map(async (file) => ({
      ...await executeFile(buildService, { file }),
      file
    }))
  );
  for (const executeResult of executeResults) {
    if (executeResult.status === "error") {
      buildService.common.logger.error({
        service: "build",
        msg: `Error while executing '${path2.relative(
          buildService.common.options.rootDir,
          executeResult.file
        )}':`,
        error: executeResult.error
      });
      return executeResult;
    }
  }
  const hash = createHash("sha256");
  for (const file of files) {
    try {
      const contents = fs.readFileSync(file, "utf-8");
      hash.update(contents);
    } catch (e) {
      buildService.common.logger.warn({
        service: "build",
        msg: `Unable to read contents of file '${file}' while constructin build ID`
      });
      hash.update(file);
    }
  }
  const contentHash = hash.digest("hex");
  const exports = await buildService.viteNodeRunner.executeId("@/generated");
  return {
    status: "success",
    indexingFunctions: exports.ponder.fns,
    contentHash
  };
};
var executeApiRoutes = async (buildService) => {
  const files = glob.sync(buildService.apiPattern);
  const executeResults = await Promise.all(
    files.map(async (file) => ({
      ...await executeFile(buildService, { file }),
      file
    }))
  );
  for (const executeResult of executeResults) {
    if (executeResult.status === "error") {
      buildService.common.logger.error({
        service: "build",
        msg: `Error while executing '${path2.relative(
          buildService.common.options.rootDir,
          executeResult.file
        )}':`,
        error: executeResult.error
      });
      return executeResult;
    }
  }
  const exports = await buildService.viteNodeRunner.executeId("@/generated");
  return {
    status: "success",
    app: exports.ponder.hono,
    routes: exports.ponder.routes
  };
};
var validateAndBuild = async ({ common }, config, schema, indexingFunctions) => {
  const buildSchemaResult = safeBuildSchema({
    schema: schema.schema,
    instanceId: process.env.PONDER_EXPERIMENTAL_INSTANCE_ID ?? // @ts-ignore
    globalThis.__PONDER_INSTANCE_ID
  });
  if (buildSchemaResult.status === "error") {
    common.logger.error({
      service: "build",
      msg: "Error while building schema:",
      error: buildSchemaResult.error
    });
    return buildSchemaResult;
  }
  const buildConfigAndIndexingFunctionsResult = await safeBuildConfigAndIndexingFunctions({
    config: config.config,
    rawIndexingFunctions: indexingFunctions.indexingFunctions,
    options: common.options
  });
  if (buildConfigAndIndexingFunctionsResult.status === "error") {
    common.logger.error({
      service: "build",
      msg: "Failed build",
      error: buildConfigAndIndexingFunctionsResult.error
    });
    return buildConfigAndIndexingFunctionsResult;
  }
  for (const log of buildConfigAndIndexingFunctionsResult.logs) {
    common.logger[log.level]({ service: "build", msg: log.msg });
  }
  const buildId = createHash("sha256").update(BUILD_ID_VERSION).update(config.contentHash).update(schema.contentHash).update(indexingFunctions.contentHash).digest("hex").slice(0, 10);
  common.logger.debug({
    service: "build",
    msg: `Completed build with ID '${buildId}' (hash of project file contents)`
  });
  return {
    status: "success",
    build: {
      buildId,
      instanceId: process.env.PONDER_EXPERIMENTAL_INSTANCE_ID ?? // @ts-ignore
      globalThis.__PONDER_INSTANCE_ID,
      databaseConfig: buildConfigAndIndexingFunctionsResult.databaseConfig,
      networks: buildConfigAndIndexingFunctionsResult.networks,
      sources: buildConfigAndIndexingFunctionsResult.sources,
      indexingFunctions: buildConfigAndIndexingFunctionsResult.indexingFunctions,
      schema: schema.schema,
      statements: buildSchemaResult.statements,
      namespace: buildSchemaResult.namespace,
      graphqlSchema: buildSchemaResult.graphqlSchema
    }
  };
};
var validateAndBuildApi = ({ common }, baseBuild, api) => {
  for (const {
    pathOrHandlers: [maybePathOrHandler]
  } of api.routes) {
    if (typeof maybePathOrHandler === "string") {
      if (maybePathOrHandler === "/status" || maybePathOrHandler === "/metrics" || maybePathOrHandler === "/health") {
        const error = new BuildError(
          `Validation failed: API route "${maybePathOrHandler}" is reserved for internal use.`
        );
        error.stack = void 0;
        common.logger.error({ service: "build", msg: "Failed build", error });
        return { status: "error", error };
      }
    }
  }
  return {
    status: "success",
    build: {
      ...baseBuild,
      app: api.app,
      routes: api.routes
    }
  };
};
var executeFile = async ({ common, viteNodeRunner }, { file }) => {
  try {
    const exports = await viteNodeRunner.executeFile(file);
    return { status: "success", exports };
  } catch (error_) {
    const relativePath = path2.relative(common.options.rootDir, file);
    const error = parseViteNodeError(relativePath, error_);
    return { status: "error", error };
  }
};

// src/build/index.ts
var methods = { start, kill };
var createBuildService = extend(create, methods);

// src/common/codegen.ts
import { mkdirSync, writeFileSync } from "node:fs";
import path3 from "node:path";
import { printSchema } from "graphql";
var ponderEnv = `// This file enables type checking and editor autocomplete for this Ponder project.
// After upgrading, you may find that changes have been made to this file.
// If this happens, please commit the changes. Do not manually edit this file.
// See https://ponder.sh/docs/getting-started/installation#typescript for more information.

declare module "@/generated" {
  import type { Virtual } from "@ponder/core";

  type config = typeof import("./ponder.config.ts").default;
  type schema = typeof import("./ponder.schema.ts");

  export const ponder: Virtual.Registry<config, schema>;

  export type EventNames = Virtual.EventNames<config>;
  export type Event<name extends EventNames = EventNames> = Virtual.Event<
    config,
    name
  >;
  export type Context<name extends EventNames = EventNames> = Virtual.Context<
    config,
    schema,
    name
  >;
  export type ApiContext = Virtual.ApiContext<schema>;
  export type IndexingFunctionArgs<name extends EventNames = EventNames> =
    Virtual.IndexingFunctionArgs<config, schema, name>;
}
`;
function runCodegen({
  common,
  graphqlSchema
}) {
  writeFileSync(
    path3.join(common.options.rootDir, "ponder-env.d.ts"),
    ponderEnv,
    "utf8"
  );
  common.logger.debug({
    service: "codegen",
    msg: "Wrote new file at ponder-env.d.ts"
  });
  mkdirSync(common.options.generatedDir, { recursive: true });
  writeFileSync(
    path3.join(common.options.generatedDir, "schema.graphql"),
    printSchema(graphqlSchema),
    "utf-8"
  );
  common.logger.debug({
    service: "codegen",
    msg: "Wrote new file at generated/schema.graphql"
  });
}

// src/common/logger.ts
import pc from "picocolors";
import { pino } from "pino";
function createLogger({
  level,
  mode = "pretty"
}) {
  const stream = {
    write(logString) {
      if (mode === "json") {
        console.log(logString.trimEnd());
        return;
      }
      const log = JSON.parse(logString);
      const prettyLog = format(log);
      console.log(prettyLog);
    }
  };
  const logger = pino(
    {
      level,
      serializers: {
        error: pino.stdSerializers.wrapErrorSerializer((error) => {
          error.meta = Array.isArray(error.meta) ? error.meta.join("\n") : error.meta;
          error.type = void 0;
          return error;
        })
      },
      // Removes "pid" and "hostname" properties from the log.
      base: void 0
    },
    stream
  );
  return {
    fatal(options) {
      logger.fatal(options);
    },
    error(options) {
      logger.error(options);
    },
    warn(options) {
      logger.warn(options);
    },
    info(options) {
      logger.info(options);
    },
    debug(options) {
      logger.debug(options);
    },
    trace(options) {
      logger.trace(options);
    },
    async kill() {
    }
  };
}
var levels = {
  60: { label: "FATAL", colorLabel: pc.bgRed("FATAL") },
  50: { label: "ERROR", colorLabel: pc.red("ERROR") },
  40: { label: "WARN ", colorLabel: pc.yellow("WARN ") },
  30: { label: "INFO ", colorLabel: pc.green("INFO ") },
  20: { label: "DEBUG", colorLabel: pc.blue("DEBUG") },
  10: { label: "TRACE", colorLabel: pc.gray("TRACE") }
};
var timeFormatter = new Intl.DateTimeFormat(void 0, {
  hour: "numeric",
  minute: "numeric",
  second: "numeric"
});
var format = (log) => {
  const time = timeFormatter.format(new Date(log.time));
  const levelObject = levels[log.level ?? 30];
  let prettyLog;
  if (pc.isColorSupported) {
    const level = levelObject.colorLabel;
    const service = log.service ? pc.cyan(log.service.padEnd(10, " ")) : "";
    const messageText = pc.reset(log.msg);
    prettyLog = [`${pc.gray(time)} ${level} ${service} ${messageText}`];
  } else {
    const level = levelObject.label;
    const service = log.service ? log.service.padEnd(10, " ") : "";
    prettyLog = [`${time} ${level} ${service} ${log.msg}`];
  }
  if (log.error) {
    if (log.error.stack) {
      prettyLog.push(log.error.stack);
    } else {
      prettyLog.push(`${log.error.name}: ${log.error.message}`);
    }
    if ("where" in log.error) {
      prettyLog.push(`where: ${log.error.where}`);
    }
    if ("meta" in log.error) {
      prettyLog.push(log.error.meta);
    }
  }
  return prettyLog.join("\n");
};

// src/common/metrics.ts
import prometheus from "prom-client";
var databaseQueryDurationMs = [
  0.05,
  0.1,
  1,
  5,
  10,
  25,
  50,
  75,
  100,
  250,
  500,
  750,
  1e3,
  2500,
  5e3,
  7500,
  1e4,
  25e3
];
var httpRequestDurationMs = [
  5,
  10,
  25,
  50,
  75,
  100,
  250,
  500,
  750,
  1e3,
  2500,
  5e3,
  7500,
  1e4,
  25e3
];
var httpRequestSizeBytes = [
  10,
  100,
  1e3,
  5e3,
  1e4,
  5e4,
  1e5,
  5e5,
  1e6,
  5e6,
  1e7
];
var MetricsService = class {
  registry;
  ponder_indexing_total_seconds;
  ponder_indexing_completed_seconds;
  ponder_indexing_completed_events;
  ponder_indexing_completed_timestamp;
  ponder_indexing_has_error;
  ponder_indexing_function_duration;
  ponder_indexing_abi_decoding_duration;
  ponder_sync_block;
  ponder_sync_is_realtime;
  ponder_sync_is_complete;
  ponder_historical_duration;
  ponder_historical_total_blocks;
  ponder_historical_cached_blocks;
  ponder_historical_completed_blocks;
  ponder_realtime_reorg_total;
  ponder_database_method_duration;
  ponder_database_method_error_total;
  ponder_http_server_port;
  ponder_http_server_active_requests;
  ponder_http_server_request_duration_ms;
  ponder_http_server_request_size_bytes;
  ponder_http_server_response_size_bytes;
  ponder_rpc_request_duration;
  ponder_rpc_request_lag;
  ponder_postgres_query_total;
  ponder_postgres_query_queue_size = null;
  ponder_postgres_pool_connections = null;
  constructor() {
    this.registry = new prometheus.Registry();
    this.ponder_indexing_total_seconds = new prometheus.Gauge({
      name: "ponder_indexing_total_seconds",
      help: "Total number of seconds that are required",
      registers: [this.registry]
    });
    this.ponder_indexing_completed_seconds = new prometheus.Gauge({
      name: "ponder_indexing_completed_seconds",
      help: "Number of seconds that have been completed",
      registers: [this.registry]
    });
    this.ponder_indexing_completed_events = new prometheus.Gauge({
      name: "ponder_indexing_completed_events",
      help: "Number of events that have been processed",
      labelNames: ["network", "event"],
      registers: [this.registry]
    });
    this.ponder_indexing_completed_timestamp = new prometheus.Gauge({
      name: "ponder_indexing_completed_timestamp",
      help: "Timestamp through which all events have been completed",
      registers: [this.registry]
    });
    this.ponder_indexing_has_error = new prometheus.Gauge({
      name: "ponder_indexing_has_error",
      help: "Boolean (0 or 1) indicating if there is an indexing error",
      registers: [this.registry]
    });
    this.ponder_indexing_function_duration = new prometheus.Histogram({
      name: "ponder_indexing_function_duration",
      help: "Duration of indexing function execution",
      labelNames: ["network", "event"],
      buckets: databaseQueryDurationMs,
      registers: [this.registry]
    });
    this.ponder_indexing_abi_decoding_duration = new prometheus.Histogram({
      name: "ponder_indexing_abi_decoding_duration",
      help: "Total time spent decoding log arguments and call trace arguments and results",
      buckets: databaseQueryDurationMs,
      registers: [this.registry]
    });
    this.ponder_sync_block = new prometheus.Gauge({
      name: "ponder_sync_block",
      help: "Closest-to-tip synced block number",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_sync_is_realtime = new prometheus.Gauge({
      name: "ponder_sync_is_realtime",
      help: "Boolean (0 or 1) indicating if the sync is realtime mode",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_sync_is_complete = new prometheus.Gauge({
      name: "ponder_sync_is_complete",
      help: "Boolean (0 or 1) indicating if the sync has synced all blocks",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_historical_duration = new prometheus.Histogram({
      name: "ponder_historical_duration",
      help: "Duration of historical sync execution",
      labelNames: ["network"],
      buckets: httpRequestDurationMs,
      registers: [this.registry]
    });
    this.ponder_historical_total_blocks = new prometheus.Gauge({
      name: "ponder_historical_total_blocks",
      help: "Number of blocks required for the historical sync",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_historical_cached_blocks = new prometheus.Gauge({
      name: "ponder_historical_cached_blocks",
      help: "Number of blocks that were found in the cache for the historical sync",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_historical_completed_blocks = new prometheus.Gauge({
      name: "ponder_historical_completed_blocks",
      help: "Number of blocks that have been processed for the historical sync",
      labelNames: ["network", "source", "type"],
      registers: [this.registry]
    });
    this.ponder_realtime_reorg_total = new prometheus.Counter({
      name: "ponder_realtime_reorg_total",
      help: "Count of how many re-orgs have occurred.",
      labelNames: ["network"],
      registers: [this.registry]
    });
    this.ponder_database_method_duration = new prometheus.Histogram({
      name: "ponder_database_method_duration",
      help: "Duration of database operations",
      labelNames: ["service", "method"],
      buckets: databaseQueryDurationMs,
      registers: [this.registry]
    });
    this.ponder_database_method_error_total = new prometheus.Counter({
      name: "ponder_database_method_error_total",
      help: "Total number of errors encountered during database operations",
      labelNames: ["service", "method"],
      registers: [this.registry]
    });
    this.ponder_http_server_port = new prometheus.Gauge({
      name: "ponder_http_server_port",
      help: "Port that the server is listening on",
      registers: [this.registry]
    });
    this.ponder_http_server_active_requests = new prometheus.Gauge({
      name: "ponder_http_server_active_requests",
      help: "Number of active HTTP server requests",
      labelNames: ["method", "path"],
      registers: [this.registry]
    });
    this.ponder_http_server_request_duration_ms = new prometheus.Histogram({
      name: "ponder_http_server_request_duration_ms",
      help: "Duration of HTTP responses served the server",
      labelNames: ["method", "path", "status"],
      buckets: httpRequestDurationMs,
      registers: [this.registry]
    });
    this.ponder_http_server_request_size_bytes = new prometheus.Histogram({
      name: "ponder_http_server_request_size_bytes",
      help: "Size of HTTP requests received by the server",
      labelNames: ["method", "path", "status"],
      buckets: httpRequestSizeBytes,
      registers: [this.registry]
    });
    this.ponder_http_server_response_size_bytes = new prometheus.Histogram({
      name: "ponder_http_server_response_size_bytes",
      help: "Size of HTTP responses served the server",
      labelNames: ["method", "path", "status"],
      buckets: httpRequestSizeBytes,
      registers: [this.registry]
    });
    this.ponder_rpc_request_duration = new prometheus.Histogram({
      name: "ponder_rpc_request_duration",
      help: "Duration of RPC requests",
      labelNames: ["network", "method"],
      buckets: httpRequestDurationMs,
      registers: [this.registry]
    });
    this.ponder_rpc_request_lag = new prometheus.Histogram({
      name: "ponder_rpc_request_lag",
      help: "Time RPC requests spend waiting in the request queue",
      labelNames: ["network", "method"],
      buckets: databaseQueryDurationMs,
      registers: [this.registry]
    });
    this.ponder_postgres_query_total = new prometheus.Counter({
      name: "ponder_postgres_query_total",
      help: "Total number of queries submitted to the database",
      labelNames: ["pool"],
      registers: [this.registry]
    });
    prometheus.collectDefaultMetrics({ register: this.registry });
  }
  /**
   * Get string representation for all metrics.
   * @returns Metrics encoded using Prometheus v0.0.4 format.
   */
  async getMetrics() {
    return await this.registry.metrics();
  }
  resetIndexingMetrics() {
    this.ponder_indexing_total_seconds.reset();
    this.ponder_indexing_completed_seconds.reset();
    this.ponder_indexing_completed_events.reset();
    this.ponder_indexing_completed_timestamp.reset();
    this.ponder_indexing_has_error.reset();
    this.ponder_indexing_function_duration.reset();
    this.ponder_indexing_abi_decoding_duration.reset();
    this.ponder_sync_block.reset();
    this.ponder_sync_is_realtime.reset();
    this.ponder_sync_is_complete.reset();
    this.ponder_historical_duration.reset();
    this.ponder_historical_total_blocks.reset();
    this.ponder_historical_cached_blocks.reset();
    this.ponder_historical_completed_blocks.reset();
    this.ponder_realtime_reorg_total.reset();
    this.ponder_rpc_request_duration.reset();
    this.ponder_rpc_request_lag.reset();
    this.ponder_database_method_duration.reset();
    this.ponder_database_method_error_total.reset();
    this.ponder_postgres_pool_connections?.reset();
    this.ponder_postgres_query_queue_size?.reset();
    this.ponder_postgres_query_total?.reset();
  }
  resetApiMetrics() {
    this.ponder_http_server_port.reset();
    this.ponder_http_server_active_requests.reset();
    this.ponder_http_server_request_duration_ms.reset();
    this.ponder_http_server_request_size_bytes.reset();
    this.ponder_http_server_response_size_bytes.reset();
    this.ponder_indexing_has_error.reset();
  }
};
var rps = {};
async function getSyncProgress(metrics) {
  const syncDurationMetric = await metrics.ponder_historical_duration.get().then((metrics2) => metrics2.values);
  const syncDurationSum = {};
  for (const m of syncDurationMetric) {
    if (m.metricName === "ponder_historical_duration_sum") {
      syncDurationSum[m.labels.network] = m.value;
    }
  }
  const extractMetric = (metric, network) => {
    return metric.values.find((m) => m.labels.network === network)?.value;
  };
  const totalBlocksMetric = await metrics.ponder_historical_total_blocks.get();
  const cachedBlocksMetric = await metrics.ponder_historical_cached_blocks.get();
  const completedBlocksMetric = await metrics.ponder_historical_completed_blocks.get();
  const syncBlockMetric = await metrics.ponder_sync_block.get();
  const syncIsRealtimeMetrics = await metrics.ponder_sync_is_realtime.get();
  const syncIsCompleteMetrics = await metrics.ponder_sync_is_complete.get();
  const requestCount = {};
  const rpcRequestMetrics = await metrics.ponder_rpc_request_duration.get();
  for (const m of rpcRequestMetrics.values) {
    const network = m.labels.network;
    if (m.metricName === "ponder_rpc_request_duration_count") {
      if (requestCount[network] === void 0) {
        requestCount[network] = 0;
      }
      requestCount[m.labels.network] += m.value;
    }
  }
  for (const [networkName, count] of Object.entries(requestCount)) {
    if (rps[networkName] === void 0) {
      rps[networkName] = [{ count, timestamp: Date.now() }];
    } else {
      rps[networkName].push({ count, timestamp: Date.now() });
    }
    if (rps[networkName].length > 100) {
      rps[networkName].shift();
    }
  }
  return totalBlocksMetric.values.map(({ value, labels }) => {
    const network = labels.network;
    const totalBlocks = value;
    const cachedBlocks = extractMetric(cachedBlocksMetric, network) ?? 0;
    const completedBlocks = extractMetric(completedBlocksMetric, network) ?? 0;
    const syncBlock = extractMetric(syncBlockMetric, network);
    const isRealtime = extractMetric(syncIsRealtimeMetrics, network);
    const isComplete = extractMetric(syncIsCompleteMetrics, network);
    const progress = totalBlocks === 0 ? 1 : (completedBlocks + cachedBlocks) / totalBlocks;
    const elapsed = syncDurationSum[network];
    const total = elapsed / (completedBlocks / (totalBlocks - cachedBlocks));
    const eta = completedBlocks >= 3 ? total - elapsed : void 0;
    const _length = rps[labels.network].length;
    const _firstRps = rps[labels.network][0];
    const _lastRps = rps[labels.network][_length - 1];
    const requests = _lastRps.count - (_length > 1 ? _firstRps.count : 0);
    const seconds = _length === 1 ? 0.1 : (_lastRps.timestamp - _firstRps.timestamp) / 1e3;
    return {
      networkName: network,
      block: syncBlock,
      progress,
      status: isComplete ? "complete" : isRealtime ? "realtime" : "historical",
      eta,
      rps: requests / seconds
    };
  });
}
async function getIndexingProgress(metrics) {
  const hasErrorMetric = (await metrics.ponder_indexing_has_error.get()).values[0]?.value;
  const hasError = hasErrorMetric === 1;
  const totalSeconds = (await metrics.ponder_indexing_total_seconds.get()).values[0]?.value ?? 0;
  const completedSeconds = (await metrics.ponder_indexing_completed_seconds.get()).values[0]?.value ?? 0;
  const completedToTimestamp = (await metrics.ponder_indexing_completed_timestamp.get()).values[0].value ?? 0;
  const progress = totalSeconds === 0 ? 0 : completedSeconds / totalSeconds;
  const indexingCompletedEventsMetric = (await metrics.ponder_indexing_completed_events.get()).values;
  const indexingFunctionDurationMetric = (await metrics.ponder_indexing_function_duration.get()).values;
  const indexingDurationSum = {};
  const indexingDurationCount = {};
  for (const m of indexingFunctionDurationMetric) {
    if (m.metricName === "ponder_indexing_function_duration_sum")
      indexingDurationSum[m.labels.event] = m.value;
    if (m.metricName === "ponder_indexing_function_duration_count")
      indexingDurationCount[m.labels.event] = m.value;
  }
  const events = indexingCompletedEventsMetric.map((m) => {
    const eventName = m.labels.event;
    const count = m.value;
    const durationSum = indexingDurationSum[eventName] ?? 0;
    const durationCount = indexingDurationCount[eventName] ?? 0;
    const averageDuration = durationCount === 0 ? 0 : durationSum / durationCount;
    return { eventName, count, averageDuration };
  });
  const totalEvents = events.reduce((a, e) => a + e.count, 0);
  return {
    hasError,
    overall: {
      completedSeconds,
      totalSeconds,
      progress,
      completedToTimestamp,
      totalEvents
    },
    events
  };
}
async function getAppProgress(metrics) {
  const sync = await getSyncProgress(metrics);
  const indexing = await getIndexingProgress(metrics);
  const decodingSum = await metrics.ponder_indexing_abi_decoding_duration.get().then(
    (m) => m.values.find(
      (v) => v.metricName === "ponder_indexing_abi_decoding_duration_sum"
    )?.value
  );
  const getEventsSum = await metrics.ponder_database_method_duration.get().then(
    (m) => m.values.find(
      (v) => v.labels.method === "getEvents" && v.metricName === "ponder_database_method_duration_sum"
    )?.value
  );
  const indexingSum = indexing.events.reduce(
    (acc, cur) => acc + cur.averageDuration * cur.count,
    0
  );
  let maxSync;
  for (const networkSync of sync) {
    if (maxSync === void 0 || maxSync.eta === void 0 || networkSync.eta && networkSync.eta > maxSync.eta) {
      maxSync = networkSync;
    }
  }
  const remainingSeconds = indexing.overall.totalSeconds - indexing.overall.completedSeconds;
  const indexingEta = indexing.overall.completedSeconds === 0 ? void 0 : ((decodingSum ?? 0) + (getEventsSum ?? 0) + indexingSum) * remainingSeconds / indexing.overall.completedSeconds;
  const eta = sync.every((n) => n.progress === 1) ? indexingEta : maxSync?.eta === void 0 && indexingEta === void 0 ? void 0 : maxSync?.eta === void 0 && maxSync?.progress !== void 0 ? void 0 : Math.max(maxSync?.eta ?? 0, indexingEta ?? 0);
  const indexingProgress = indexing.overall.progress === 0 && indexing.overall.totalEvents > 0 ? 1 : indexing.overall.progress;
  const progress = sync.every((n) => n.progress === 1) ? indexingProgress : maxSync?.progress === void 0 ? 0 : maxSync.progress * indexingProgress;
  return {
    mode: sync.some((n) => n.status === "realtime") ? "realtime" : sync.every((n) => n.status === "complete") ? "complete" : sync.length === 0 ? void 0 : "historical",
    progress,
    eta
  };
}

// src/common/options.ts
import path4 from "node:path";
import v8 from "node:v8";
var buildOptions = ({ cliOptions }) => {
  let rootDir;
  if (cliOptions.root !== void 0) {
    rootDir = path4.resolve(cliOptions.root);
  } else {
    rootDir = path4.resolve(".");
  }
  let logLevel;
  if (cliOptions.logLevel) {
    logLevel = cliOptions.logLevel;
  } else if (cliOptions.trace === true) {
    logLevel = "trace";
  } else if (cliOptions.debug === true) {
    logLevel = "debug";
  } else if (process.env.PONDER_LOG_LEVEL !== void 0 && ["silent", "fatal", "error", "warn", "info", "debug", "trace"].includes(
    process.env.PONDER_LOG_LEVEL
  )) {
    logLevel = process.env.PONDER_LOG_LEVEL;
  } else {
    logLevel = "info";
  }
  const port = process.env.PORT !== void 0 ? Number(process.env.PORT) : cliOptions.port !== void 0 ? cliOptions.port : 42069;
  const hostname = cliOptions.hostname;
  return {
    command: cliOptions.command,
    rootDir,
    configFile: path4.join(rootDir, cliOptions.config),
    schemaFile: path4.join(rootDir, "ponder.schema.ts"),
    indexingDir: path4.join(rootDir, "src"),
    apiDir: path4.join(rootDir, "src", "api"),
    generatedDir: path4.join(rootDir, "generated"),
    ponderDir: path4.join(rootDir, ".ponder"),
    logDir: path4.join(rootDir, ".ponder", "logs"),
    port,
    hostname,
    telemetryUrl: "https://ponder.sh/api/telemetry",
    telemetryDisabled: Boolean(process.env.PONDER_TELEMETRY_DISABLED),
    telemetryConfigDir: void 0,
    logLevel,
    logFormat: cliOptions.logFormat,
    databaseHeartbeatInterval: 10 * 1e3,
    databaseHeartbeatTimeout: 25 * 1e3,
    // Half of the max query parameters for PGlite
    databaseMaxQueryParameters: 16e3,
    factoryAddressCountThreshold: 1e3,
    // v8.getHeapStatistics().heap_size_limit / 8, bucketed closest to 128, 256, 512, 1024, 2048 mB
    indexingCacheMaxBytes: 2 ** Math.min(
      Math.max(
        Math.round(
          Math.log2(
            v8.getHeapStatistics().heap_size_limit / 1024 / 1024 / 8
          )
        ),
        7
      ),
      11
    ) * 1024 * 1024,
    indexingCacheFlushRatio: 0.35,
    syncStoreMaxIntervals: 5e3,
    syncEventsQuerySize: 1e4,
    syncHandoffStaleSeconds: 300
  };
};

// src/common/telemetry.ts
import { exec } from "node:child_process";
import { createHash as createHash2, randomBytes } from "node:crypto";
import { existsSync, readFileSync as readFileSync2 } from "node:fs";
import os from "node:os";
import path5 from "node:path";
import { promisify } from "node:util";

// src/utils/timer.ts
function startClock() {
  const start3 = process.hrtime();
  return () => hrTimeToMs(process.hrtime(start3));
}
function hrTimeToMs(diff) {
  const ns = diff[0] * 10 ** 9 + diff[1];
  return ns / 10 ** 6;
}

// src/utils/wait.ts
async function wait(milliseconds) {
  return new Promise((res) => setTimeout(res, milliseconds));
}

// src/common/telemetry.ts
import Conf from "conf";
import { detect, getNpmVersion } from "detect-package-manager";
var HEARTBEAT_INTERVAL_MS = 6e4;
function createTelemetry({
  options,
  logger
}) {
  if (options.telemetryDisabled) {
    return {
      record: (_event) => {
      },
      flush: async () => {
      },
      kill: async () => {
      }
    };
  }
  const conf = new Conf({
    projectName: "ponder",
    cwd: options.telemetryConfigDir
  });
  if (conf.get("notifiedAt") === void 0) {
    conf.set("notifiedAt", Date.now().toString());
    logger.info({
      service: "telemetry",
      msg: "Ponder collects anonymous telemetry data to identify issues and prioritize features. See https://ponder.sh/docs/advanced/telemetry for more information."
    });
  }
  const sessionId = randomBytes(8).toString("hex");
  let anonymousId = conf.get("anonymousId");
  if (anonymousId === void 0) {
    anonymousId = randomBytes(8).toString("hex");
    conf.set("anonymousId", anonymousId);
  }
  if (anonymousId.length > 16)
    anonymousId = anonymousId.slice(0, 16);
  let salt = conf.get("salt");
  if (salt === void 0) {
    salt = randomBytes(8).toString("hex");
    conf.set("salt", salt);
  }
  const oneWayHash = (value) => {
    const hash = createHash2("sha256");
    hash.update(salt);
    hash.update(value);
    return hash.digest("hex").slice(0, 16);
  };
  const buildContext = async () => {
    const gitRemoteUrl = await getGitRemoteUrl();
    const projectIdRaw = gitRemoteUrl ?? process.cwd();
    const projectId = oneWayHash(projectIdRaw);
    const { packageManager, packageManagerVersion } = await getPackageManager();
    const packageJson2 = getPackageJson(options.rootDir);
    const ponderCoreVersion = packageJson2?.dependencies?.["@ponder/core"] ?? "unknown";
    const viemVersion = packageJson2?.dependencies?.viem ?? "unknown";
    const isInternal = ponderCoreVersion === "workspace:*";
    const cpus = os.cpus();
    return {
      common: {
        session_id: sessionId,
        project_id: projectId,
        is_internal: isInternal
      },
      session: {
        ponder_core_version: ponderCoreVersion,
        viem_version: viemVersion,
        package_manager: packageManager,
        package_manager_version: packageManagerVersion,
        node_version: process.versions.node,
        system_platform: os.platform(),
        system_release: os.release(),
        system_architecture: os.arch(),
        cpu_count: cpus.length,
        cpu_model: cpus.length > 0 ? cpus[0].model : "unknown",
        cpu_speed: cpus.length > 0 ? cpus[0].speed : 0,
        total_memory_bytes: os.totalmem()
      }
    };
  };
  let context = void 0;
  const contextPromise = buildContext();
  const controller = new AbortController();
  let isKilled = false;
  const queue = createQueue({
    initialStart: true,
    concurrency: 10,
    worker: async (event) => {
      const endClock = startClock();
      try {
        if (context === void 0)
          context = await contextPromise;
        const properties = event.name === "lifecycle:session_start" ? { ...event.properties, ...context.common, ...context.session } : { ...event.properties, ...context.common };
        const body = JSON.stringify({
          distinctId: anonymousId,
          event: event.name,
          properties
        });
        await fetch(options.telemetryUrl, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body,
          signal: controller.signal
        });
        logger.trace({
          service: "telemetry",
          msg: `Sent '${event.name}' event in ${endClock()}ms`
        });
      } catch (error_) {
        const error = error_;
        logger.trace({
          service: "telemetry",
          msg: `Failed to send '${event.name}' event after ${endClock()}ms`,
          error
        });
      }
    }
  });
  const record = (event) => {
    if (isKilled)
      return;
    queue.add(event);
  };
  const heartbeatInterval = setInterval(() => {
    record({
      name: "lifecycle:heartbeat_send",
      properties: { duration_seconds: process.uptime() }
    });
  }, HEARTBEAT_INTERVAL_MS);
  const flush = async () => {
    await queue.onIdle();
  };
  const kill3 = async () => {
    clearInterval(heartbeatInterval);
    isKilled = true;
    queue.clear();
    await Promise.race([queue.onIdle(), wait(1e3)]);
  };
  return { record, flush, kill: kill3 };
}
async function getPackageManager() {
  let packageManager = "unknown";
  let packageManagerVersion = "unknown";
  try {
    packageManager = await detect();
    packageManagerVersion = await getNpmVersion(packageManager);
  } catch (e) {
  }
  return { packageManager, packageManagerVersion };
}
var execa = promisify(exec);
async function getGitRemoteUrl() {
  const result = await execa("git config --local --get remote.origin.url", {
    timeout: 250,
    windowsHide: true
  }).catch(() => void 0);
  return result?.stdout.trim();
}
function getPackageJson(rootDir) {
  try {
    const rootPath = path5.join(rootDir, "package.json");
    const cwdPath = path5.join(process.cwd(), "package.json");
    const packageJsonPath2 = existsSync(rootPath) ? rootPath : existsSync(cwdPath) ? cwdPath : void 0;
    if (packageJsonPath2 === void 0)
      return void 0;
    const packageJsonString = readFileSync2(packageJsonPath2, "utf8");
    const packageJson2 = JSON.parse(packageJsonString);
    return packageJson2;
  } catch (e) {
    return void 0;
  }
}
function buildPayload(build) {
  const table_count = Object.keys(build.schema).length;
  const indexing_function_count = Object.values(build.indexingFunctions).reduce(
    (acc, f) => acc + Object.keys(f).length,
    0
  );
  return {
    database_kind: build.databaseConfig.kind,
    contract_count: build.sources.length,
    network_count: build.networks.length,
    table_count,
    indexing_function_count
  };
}

// src/bin/utils/shutdown.ts
import os2 from "node:os";
import readline from "node:readline";
var SHUTDOWN_GRACE_PERIOD_MS = 5e3;
function setupShutdown({
  common,
  cleanup
}) {
  let isShuttingDown = false;
  const shutdown = async ({
    reason,
    code
  }) => {
    if (isShuttingDown)
      return;
    isShuttingDown = true;
    setTimeout(async () => {
      common.logger.fatal({
        service: "process",
        msg: "Failed to shutdown within 5 seconds, terminating (exit code 1)"
      });
      await common.logger.kill();
      process.exit(1);
    }, SHUTDOWN_GRACE_PERIOD_MS);
    if (reason !== void 0) {
      common.logger.warn({
        service: "process",
        msg: `${reason}, starting shutdown sequence`
      });
    }
    common.telemetry.record({
      name: "lifecycle:session_end",
      properties: { duration_seconds: process.uptime() }
    });
    await cleanup();
    const level = code === 0 ? "info" : "fatal";
    common.logger[level]({
      service: "process",
      msg: `Finished shutdown sequence, terminating (exit code ${code})`
    });
    await common.logger.kill();
    process.exit(code);
  };
  if (os2.platform() === "win32") {
    const readlineInterface = readline.createInterface({
      input: process.stdin,
      output: process.stdout
    });
    readlineInterface.on(
      "SIGINT",
      () => shutdown({ reason: "Received SIGINT", code: 0 })
    );
  }
  process.on("SIGINT", () => shutdown({ reason: "Received SIGINT", code: 0 }));
  process.on(
    "SIGTERM",
    () => shutdown({ reason: "Received SIGTERM", code: 0 })
  );
  process.on(
    "SIGQUIT",
    () => shutdown({ reason: "Received SIGQUIT", code: 0 })
  );
  process.on("uncaughtException", (error) => {
    if (error instanceof IgnorableError)
      return;
    common.logger.error({
      service: "process",
      msg: "Caught uncaughtException event",
      error
    });
    shutdown({ reason: "Received uncaughtException", code: 1 });
  });
  process.on("unhandledRejection", (error) => {
    if (error instanceof IgnorableError)
      return;
    common.logger.error({
      service: "process",
      msg: "Caught unhandledRejection event",
      error
    });
    shutdown({ reason: "Received unhandledRejection", code: 1 });
  });
  return shutdown;
}

// src/bin/commands/codegen.ts
async function codegen({ cliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat
  });
  const [major, minor, _patch] = process.versions.node.split(".").map(Number);
  if (major < 18 || major === 18 && minor < 14) {
    logger.fatal({
      service: "process",
      msg: `Invalid Node.js version. Expected >=18.14, detected ${major}.${minor}.`
    });
    await logger.kill();
    process.exit(1);
  }
  const metrics = new MetricsService();
  const telemetry = createTelemetry({ options, logger });
  const common = { options, logger, metrics, telemetry };
  const buildService = await createBuildService({ common });
  const cleanup = async () => {
    await buildService.kill();
    await telemetry.kill();
  };
  const shutdown = setupShutdown({ common, cleanup });
  const buildResult = await buildService.start({ watch: false });
  if (buildResult.status === "error") {
    logger.error({
      service: "process",
      msg: "Failed schema build",
      error: buildResult.error
    });
    await shutdown({ reason: "Failed schema build", code: 1 });
    return;
  }
  telemetry.record({
    name: "lifecycle:session_start",
    properties: { cli_command: "codegen" }
  });
  const graphqlSchema = buildResult.indexingBuild.graphqlSchema;
  runCodegen({ common, graphqlSchema });
  logger.info({ service: "codegen", msg: "Wrote ponder-env.d.ts" });
  logger.info({ service: "codegen", msg: "Wrote schema.graphql" });
  await shutdown({ reason: "Success", code: 0 });
}

// src/bin/commands/dev.ts
import { existsSync as existsSync2 } from "node:fs";
import path6 from "node:path";

// src/database/index.ts
import crypto2 from "node:crypto";

// src/sync-store/migrations.ts
import { sql } from "kysely";
var migrations = {
  "2023_05_15_0_initial": {
    async up(db) {
      await db.schema.createTable("blocks").addColumn("baseFeePerGas", sql`bytea`).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("difficulty", sql`bytea`, (col) => col.notNull()).addColumn("extraData", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("gasLimit", sql`bytea`, (col) => col.notNull()).addColumn("gasUsed", sql`bytea`, (col) => col.notNull()).addColumn("hash", "text", (col) => col.notNull().primaryKey()).addColumn("logsBloom", "text", (col) => col.notNull()).addColumn("miner", "text", (col) => col.notNull()).addColumn("mixHash", "text", (col) => col.notNull()).addColumn("nonce", "text", (col) => col.notNull()).addColumn("number", sql`bytea`, (col) => col.notNull()).addColumn("parentHash", "text", (col) => col.notNull()).addColumn("receiptsRoot", "text", (col) => col.notNull()).addColumn("sha3Uncles", "text", (col) => col.notNull()).addColumn("size", sql`bytea`, (col) => col.notNull()).addColumn("stateRoot", "text", (col) => col.notNull()).addColumn("timestamp", sql`bytea`, (col) => col.notNull()).addColumn("totalDifficulty", sql`bytea`, (col) => col.notNull()).addColumn("transactionsRoot", "text", (col) => col.notNull()).execute();
      await db.schema.createTable("transactions").addColumn("accessList", "text").addColumn("blockHash", "text", (col) => col.notNull()).addColumn("blockNumber", sql`bytea`, (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("from", "text", (col) => col.notNull()).addColumn("gas", sql`bytea`, (col) => col.notNull()).addColumn("gasPrice", sql`bytea`).addColumn("hash", "text", (col) => col.notNull().primaryKey()).addColumn("input", "text", (col) => col.notNull()).addColumn("maxFeePerGas", sql`bytea`).addColumn("maxPriorityFeePerGas", sql`bytea`).addColumn("nonce", "integer", (col) => col.notNull()).addColumn("r", "text", (col) => col.notNull()).addColumn("s", "text", (col) => col.notNull()).addColumn("to", "text").addColumn("transactionIndex", "integer", (col) => col.notNull()).addColumn("type", "text", (col) => col.notNull()).addColumn("value", sql`bytea`, (col) => col.notNull()).addColumn("v", sql`bytea`, (col) => col.notNull()).execute();
      await db.schema.createTable("logs").addColumn("address", "text", (col) => col.notNull()).addColumn("blockHash", "text", (col) => col.notNull()).addColumn("blockNumber", sql`bytea`, (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("logIndex", "integer", (col) => col.notNull()).addColumn("topic0", "text").addColumn("topic1", "text").addColumn("topic2", "text").addColumn("topic3", "text").addColumn("transactionHash", "text", (col) => col.notNull()).addColumn("transactionIndex", "integer", (col) => col.notNull()).execute();
      await db.schema.createTable("contractReadResults").addColumn("address", "text", (col) => col.notNull()).addColumn("blockNumber", sql`bytea`, (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("finalized", "integer", (col) => col.notNull()).addColumn("result", "text", (col) => col.notNull()).addPrimaryKeyConstraint("contractReadResultPrimaryKey", [
        "chainId",
        "blockNumber",
        "address",
        "data"
      ]).execute();
      await db.schema.createTable("logFilterCachedRanges").addColumn("endBlock", sql`bytea`, (col) => col.notNull()).addColumn("endBlockTimestamp", sql`bytea`, (col) => col.notNull()).addColumn("filterKey", "text", (col) => col.notNull()).addColumn("id", "serial", (col) => col.notNull().primaryKey()).addColumn("startBlock", sql`bytea`, (col) => col.notNull()).execute();
    }
  },
  "2023_06_20_0_indices": {
    async up(db) {
      await db.schema.createIndex("log_events_index").on("logs").columns(["address", "chainId", "blockHash"]).execute();
      await db.schema.createIndex("blocks_index").on("blocks").columns(["timestamp", "number"]).execute();
      await db.schema.createIndex("logFilterCachedRanges_index").on("logFilterCachedRanges").columns(["filterKey"]).execute();
    }
  },
  "2023_07_18_0_better_indices": {
    async up(db) {
      await db.schema.dropIndex("log_events_index").execute();
      await db.schema.dropIndex("blocks_index").execute();
      await db.schema.createIndex("log_block_hash_index").on("logs").column("blockHash").execute();
      await db.schema.createIndex("log_chain_id_index").on("logs").column("chainId").execute();
      await db.schema.createIndex("log_address_index").on("logs").column("address").execute();
      await db.schema.createIndex("log_topic0_index").on("logs").column("topic0").execute();
      await db.schema.createIndex("block_timestamp_index").on("blocks").column("timestamp").execute();
      await db.schema.createIndex("block_number_index").on("blocks").column("number").execute();
    }
  },
  "2023_07_24_0_drop_finalized": {
    async up(db) {
      await db.schema.alterTable("blocks").dropColumn("finalized").execute();
      await db.schema.alterTable("transactions").dropColumn("finalized").execute();
      await db.schema.alterTable("logs").dropColumn("finalized").execute();
      await db.schema.alterTable("contractReadResults").dropColumn("finalized").execute();
    }
  },
  "2023_09_19_0_new_sync_design": {
    async up(db) {
      await db.schema.dropTable("logFilterCachedRanges").execute();
      await db.schema.dropTable("blocks").execute();
      await db.schema.createTable("blocks").addColumn("baseFeePerGas", "numeric(78, 0)").addColumn("chainId", "integer", (col) => col.notNull()).addColumn("difficulty", "numeric(78, 0)", (col) => col.notNull()).addColumn("extraData", "text", (col) => col.notNull()).addColumn("gasLimit", "numeric(78, 0)", (col) => col.notNull()).addColumn("gasUsed", "numeric(78, 0)", (col) => col.notNull()).addColumn("hash", "varchar(66)", (col) => col.notNull().primaryKey()).addColumn("logsBloom", "varchar(514)", (col) => col.notNull()).addColumn("miner", "varchar(42)", (col) => col.notNull()).addColumn("mixHash", "varchar(66)", (col) => col.notNull()).addColumn("nonce", "varchar(18)", (col) => col.notNull()).addColumn("number", "numeric(78, 0)", (col) => col.notNull()).addColumn("parentHash", "varchar(66)", (col) => col.notNull()).addColumn("receiptsRoot", "varchar(66)", (col) => col.notNull()).addColumn("sha3Uncles", "varchar(66)", (col) => col.notNull()).addColumn("size", "numeric(78, 0)", (col) => col.notNull()).addColumn("stateRoot", "varchar(66)", (col) => col.notNull()).addColumn("timestamp", "numeric(78, 0)", (col) => col.notNull()).addColumn("totalDifficulty", "numeric(78, 0)", (col) => col.notNull()).addColumn("transactionsRoot", "varchar(66)", (col) => col.notNull()).execute();
      await db.schema.createIndex("blockTimestampIndex").on("blocks").column("timestamp").execute();
      await db.schema.createIndex("blockNumberIndex").on("blocks").column("number").execute();
      await db.schema.dropTable("transactions").execute();
      await db.schema.createTable("transactions").addColumn("accessList", "text").addColumn("blockHash", "varchar(66)", (col) => col.notNull()).addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("from", "varchar(42)", (col) => col.notNull()).addColumn("gas", "numeric(78, 0)", (col) => col.notNull()).addColumn("gasPrice", "numeric(78, 0)").addColumn("hash", "varchar(66)", (col) => col.notNull().primaryKey()).addColumn("input", "text", (col) => col.notNull()).addColumn("maxFeePerGas", "numeric(78, 0)").addColumn("maxPriorityFeePerGas", "numeric(78, 0)").addColumn("nonce", "integer", (col) => col.notNull()).addColumn("r", "varchar(66)", (col) => col.notNull()).addColumn("s", "varchar(66)", (col) => col.notNull()).addColumn("to", "varchar(42)").addColumn("transactionIndex", "integer", (col) => col.notNull()).addColumn("type", "text", (col) => col.notNull()).addColumn("value", "numeric(78, 0)", (col) => col.notNull()).addColumn("v", "numeric(78, 0)", (col) => col.notNull()).execute();
      await db.schema.dropTable("logs").execute();
      await db.schema.createTable("logs").addColumn("address", "varchar(42)", (col) => col.notNull()).addColumn("blockHash", "varchar(66)", (col) => col.notNull()).addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("logIndex", "integer", (col) => col.notNull()).addColumn("topic0", "varchar(66)").addColumn("topic1", "varchar(66)").addColumn("topic2", "varchar(66)").addColumn("topic3", "varchar(66)").addColumn("transactionHash", "varchar(66)", (col) => col.notNull()).addColumn("transactionIndex", "integer", (col) => col.notNull()).execute();
      await db.schema.createIndex("logBlockHashIndex").on("logs").column("blockHash").execute();
      await db.schema.createIndex("logChainIdIndex").on("logs").column("chainId").execute();
      await db.schema.createIndex("logAddressIndex").on("logs").column("address").execute();
      await db.schema.createIndex("logTopic0Index").on("logs").column("topic0").execute();
      await db.schema.dropTable("contractReadResults").execute();
      await db.schema.createTable("contractReadResults").addColumn("address", "varchar(42)", (col) => col.notNull()).addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("data", "text", (col) => col.notNull()).addColumn("result", "text", (col) => col.notNull()).addPrimaryKeyConstraint("contractReadResultPrimaryKey", [
        "chainId",
        "blockNumber",
        "address",
        "data"
      ]).execute();
      await db.schema.createTable("logFilters").addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("address", "varchar(66)").addColumn("topic0", "varchar(66)").addColumn("topic1", "varchar(66)").addColumn("topic2", "varchar(66)").addColumn("topic3", "varchar(66)").execute();
      await db.schema.createTable("logFilterIntervals").addColumn("id", "serial", (col) => col.notNull().primaryKey()).addColumn(
        "logFilterId",
        "text",
        (col) => col.notNull().references("logFilters.id")
      ).addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull()).addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull()).execute();
      await db.schema.createIndex("logFilterIntervalsLogFilterId").on("logFilterIntervals").column("logFilterId").execute();
      await db.schema.createTable("factories").addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("address", "varchar(42)", (col) => col.notNull()).addColumn("eventSelector", "varchar(66)", (col) => col.notNull()).addColumn("childAddressLocation", "text", (col) => col.notNull()).addColumn("topic0", "varchar(66)").addColumn("topic1", "varchar(66)").addColumn("topic2", "varchar(66)").addColumn("topic3", "varchar(66)").execute();
      await db.schema.createTable("factoryLogFilterIntervals").addColumn("id", "serial", (col) => col.notNull().primaryKey()).addColumn(
        "factoryId",
        "text",
        (col) => col.notNull().references("factories.id")
      ).addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull()).addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull()).execute();
      await db.schema.createIndex("factoryLogFilterIntervalsFactoryId").on("factoryLogFilterIntervals").column("factoryId").execute();
    }
  },
  "2023_11_06_0_new_rpc_cache_design": {
    async up(db) {
      await db.schema.dropTable("contractReadResults").execute();
      await db.schema.createTable("rpcRequestResults").addColumn("request", "text", (col) => col.notNull()).addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("result", "text", (col) => col.notNull()).addPrimaryKeyConstraint("rpcRequestResultPrimaryKey", [
        "request",
        "chainId",
        "blockNumber"
      ]).execute();
    }
  },
  "2024_01_30_0_change_chain_id_type": {
    async up(db) {
      await db.schema.alterTable("blocks").alterColumn("chainId", (col) => col.setDataType("int8")).execute();
      await db.schema.alterTable("transactions").alterColumn("chainId", (col) => col.setDataType("int8")).execute();
      await db.schema.alterTable("logs").alterColumn("chainId", (col) => col.setDataType("int8")).execute();
      await db.schema.alterTable("logFilters").alterColumn("chainId", (col) => col.setDataType("int8")).execute();
      await db.schema.alterTable("factories").alterColumn("chainId", (col) => col.setDataType("int8")).execute();
      await db.schema.alterTable("rpcRequestResults").alterColumn("chainId", (col) => col.setDataType("int8")).execute();
    }
  },
  "2024_02_1_0_nullable_block_columns": {
    async up(db) {
      await db.schema.alterTable("blocks").alterColumn("mixHash", (col) => col.dropNotNull()).execute();
      await db.schema.alterTable("blocks").alterColumn("nonce", (col) => col.dropNotNull()).execute();
    }
  },
  "2024_03_00_0_log_transaction_hash_index": {
    async up(db) {
      await db.schema.createIndex("log_transaction_hash_index").on("logs").column("transactionHash").execute();
    }
  },
  "2024_03_13_0_nullable_block_columns_sha3uncles": {
    async up(db) {
      await db.schema.alterTable("blocks").alterColumn("sha3Uncles", (col) => col.dropNotNull()).execute();
    }
  },
  "2024_03_14_0_nullable_transaction_rsv": {
    async up(db) {
      await db.schema.alterTable("transactions").alterColumn("r", (col) => col.dropNotNull()).execute();
      await db.schema.alterTable("transactions").alterColumn("s", (col) => col.dropNotNull()).execute();
      await db.schema.alterTable("transactions").alterColumn("v", (col) => col.dropNotNull()).execute();
    }
  },
  "2024_03_20_0_checkpoint_in_logs_table": {
    async up(_db) {
      return;
    }
  },
  "2024_04_04_0_log_events_indexes": {
    async up(db) {
      await db.schema.dropIndex("blockNumberIndex").ifExists().execute();
      await db.schema.dropIndex("blockTimestampIndex").ifExists().execute();
      await db.schema.createIndex("logBlockNumberIndex").on("logs").column("blockNumber").execute();
    }
  },
  "2024_04_14_0_nullable_block_total_difficulty": {
    async up(db) {
      await db.schema.alterTable("blocks").alterColumn("totalDifficulty", (col) => col.dropNotNull()).execute();
    }
  },
  "2024_04_14_1_add_checkpoint_column_to_logs_table": {
    async up(db) {
      await db.executeQuery(
        sql`
        ALTER TABLE ponder_sync.logs 
        ADD COLUMN IF NOT EXISTS 
        checkpoint varchar(75)`.compile(db)
      );
    }
  },
  "2024_04_14_2_set_checkpoint_in_logs_table": {
    async up(db) {
      await db.executeQuery(sql`SET statement_timeout = 3600000;`.compile(db));
      await db.executeQuery(
        sql`
        CREATE TEMP TABLE cp_vals AS 
        SELECT
          logs.id,
          (lpad(blocks.timestamp::text, 10, '0') ||
          lpad(blocks."chainId"::text, 16, '0') ||
          lpad(blocks.number::text, 16, '0') ||
          lpad(logs."transactionIndex"::text, 16, '0') ||
          '5' ||
          lpad(logs."logIndex"::text, 16, '0')) AS checkpoint
        FROM ponder_sync.logs logs
        JOIN ponder_sync.blocks blocks ON logs."blockHash" = blocks.hash;
        `.compile(db)
      );
      await db.executeQuery(
        sql`
        CREATE INDEX ON cp_vals(id)
        `.compile(db)
      );
      await db.executeQuery(
        sql`
          UPDATE ponder_sync.logs
          SET checkpoint=cp_vals.checkpoint
          FROM cp_vals
          WHERE ponder_sync.logs.id = cp_vals.id
        `.compile(db)
      );
      await db.executeQuery(
        sql`DROP TABLE IF EXISTS cp_vals CASCADE;`.compile(db)
      );
    }
  },
  "2024_04_14_3_index_on_logs_checkpoint": {
    async up(db) {
      await db.schema.createIndex("logs_checkpoint_index").ifNotExists().on("logs").column("checkpoint").execute();
    }
  },
  "2024_04_22_0_transaction_receipts": {
    async up(db) {
      await db.schema.alterTable("logFilterIntervals").dropConstraint("logFilterIntervals_logFilterId_fkey").execute();
      await db.updateTable("logFilters").set({ id: sql`"id" || '_0'` }).execute();
      await db.updateTable("logFilterIntervals").set({ logFilterId: sql`"logFilterId" || '_0'` }).execute();
      await db.schema.alterTable("logFilters").addColumn(
        "includeTransactionReceipts",
        "integer",
        (col) => col.notNull().defaultTo(0)
      ).execute();
      await db.schema.alterTable("logFilters").alterColumn("includeTransactionReceipts", (col) => col.dropDefault()).execute();
      await db.schema.alterTable("factoryLogFilterIntervals").dropConstraint("factoryLogFilterIntervals_factoryId_fkey").execute();
      await db.updateTable("factories").set({ id: sql`"id" || '_0'` }).execute();
      await db.updateTable("factoryLogFilterIntervals").set({ factoryId: sql`"factoryId" || '_0'` }).execute();
      await db.schema.alterTable("factories").addColumn(
        "includeTransactionReceipts",
        "integer",
        (col) => col.notNull().defaultTo(0)
      ).execute();
      await db.schema.alterTable("factories").alterColumn("includeTransactionReceipts", (col) => col.dropDefault()).execute();
      await db.schema.createTable("transactionReceipts").addColumn("blockHash", "varchar(66)", (col) => col.notNull()).addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("contractAddress", "varchar(66)").addColumn(
        "cumulativeGasUsed",
        "numeric(78, 0)",
        (col) => col.notNull()
      ).addColumn(
        "effectiveGasPrice",
        "numeric(78, 0)",
        (col) => col.notNull()
      ).addColumn("from", "varchar(42)", (col) => col.notNull()).addColumn("gasUsed", "numeric(78, 0)", (col) => col.notNull()).addColumn("logs", "text", (col) => col.notNull()).addColumn("logsBloom", "varchar(514)", (col) => col.notNull()).addColumn("status", "text", (col) => col.notNull()).addColumn("to", "varchar(42)").addColumn(
        "transactionHash",
        "varchar(66)",
        (col) => col.notNull().primaryKey()
      ).addColumn("transactionIndex", "integer", (col) => col.notNull()).addColumn("type", "text", (col) => col.notNull()).execute();
    }
  },
  "2024_04_23_0_block_filters": {
    async up(db) {
      await db.schema.createTable("blockFilters").addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("interval", "integer", (col) => col.notNull()).addColumn("offset", "integer", (col) => col.notNull()).execute();
      await db.schema.createTable("blockFilterIntervals").addColumn("id", "serial", (col) => col.notNull().primaryKey()).addColumn(
        "blockFilterId",
        "text",
        (col) => col.notNull().references("blockFilters.id")
      ).addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull()).addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull()).execute();
      await db.schema.createIndex("blockFilterIntervalsBlockFilterId").on("blockFilterIntervals").column("blockFilterId").execute();
      await db.schema.alterTable("blocks").addColumn("checkpoint", "varchar(75)").execute();
      await db.executeQuery(
        sql`
          CREATE TEMP TABLE bcp_vals AS 
          SELECT
            blocks.hash,
            (lpad(blocks.timestamp::text, 10, '0') ||
            lpad(blocks."chainId"::text, 16, '0') ||
            lpad(blocks.number::text, 16, '0') ||
            '9999999999999999' ||
            '5' ||
            '0000000000000000') AS checkpoint
          FROM ponder_sync.blocks
          `.compile(db)
      );
      await db.executeQuery(
        sql`
          UPDATE ponder_sync.blocks
          SET checkpoint=bcp_vals.checkpoint
          FROM bcp_vals
          WHERE ponder_sync.blocks.hash = bcp_vals.hash
        `.compile(db)
      );
      await db.executeQuery(
        sql`DROP TABLE IF EXISTS bcp_vals CASCADE;`.compile(db)
      );
      await db.schema.alterTable("blocks").alterColumn("checkpoint", (col) => col.setNotNull()).execute();
      await db.schema.createIndex("blockNumberIndex").on("blocks").column("number").execute();
      await db.schema.createIndex("blockChainIdIndex").on("blocks").column("chainId").execute();
      await db.schema.createIndex("blockCheckpointIndex").on("blocks").column("checkpoint").execute();
    }
  },
  "2024_05_07_0_trace_filters": {
    async up(db) {
      await db.schema.createTable("traceFilters").addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("fromAddress", "varchar(42)").addColumn("toAddress", "varchar(42)").execute();
      await db.schema.createTable("traceFilterIntervals").addColumn("id", "serial", (col) => col.notNull().primaryKey()).addColumn("traceFilterId", "text", (col) => col.notNull()).addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull()).addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull()).execute();
      await db.schema.createIndex("traceFilterIntervalsTraceFilterId").on("traceFilterIntervals").column("traceFilterId").execute();
      await db.schema.createTable("callTraces").addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("callType", "text", (col) => col.notNull()).addColumn("from", "varchar(42)", (col) => col.notNull()).addColumn("gas", "numeric(78, 0)", (col) => col.notNull()).addColumn("input", "text", (col) => col.notNull()).addColumn("to", "varchar(42)", (col) => col.notNull()).addColumn("value", "numeric(78, 0)", (col) => col.notNull()).addColumn("blockHash", "varchar(66)", (col) => col.notNull()).addColumn("blockNumber", "numeric(78, 0)", (col) => col.notNull()).addColumn("error", "text").addColumn("gasUsed", "numeric(78, 0)").addColumn("output", "text").addColumn("subtraces", "integer", (col) => col.notNull()).addColumn("traceAddress", "text", (col) => col.notNull()).addColumn("transactionHash", "varchar(66)", (col) => col.notNull()).addColumn("transactionPosition", "integer", (col) => col.notNull()).addColumn("functionSelector", "varchar(10)", (col) => col.notNull()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("checkpoint", "varchar(75)", (col) => col.notNull()).execute();
      await db.schema.createIndex("callTracesBlockNumberIndex").on("callTraces").column("blockNumber").execute();
      await db.schema.createIndex("callTracesFunctionSelectorIndex").on("callTraces").column("functionSelector").execute();
      await db.schema.createIndex("callTracesErrorIndex").on("callTraces").column("error").execute();
      await db.schema.createIndex("callTracesBlockHashIndex").on("callTraces").column("blockHash").execute();
      await db.schema.createIndex("callTracesTransactionHashIndex").on("callTraces").column("transactionHash").execute();
      await db.schema.createIndex("callTracesCheckpointIndex").on("callTraces").column("checkpoint").execute();
      await db.schema.createIndex("callTracesChainIdIndex").on("callTraces").column("chainId").execute();
      await db.schema.createIndex("callTracesFromIndex").on("callTraces").column("from").execute();
      await db.schema.createIndex("callTracesToIndex").on("callTraces").column("to").execute();
      await db.schema.alterTable("factories").renameTo("factoryLogFilters").execute();
      await db.schema.createTable("factoryTraceFilters").addColumn("id", "text", (col) => col.notNull().primaryKey()).addColumn("chainId", "integer", (col) => col.notNull()).addColumn("address", "varchar(42)", (col) => col.notNull()).addColumn("eventSelector", "varchar(66)", (col) => col.notNull()).addColumn("childAddressLocation", "text", (col) => col.notNull()).addColumn("fromAddress", "varchar(42)").execute();
      await db.schema.createTable("factoryTraceFilterIntervals").addColumn("id", "serial", (col) => col.notNull().primaryKey()).addColumn("factoryId", "text").addColumn("startBlock", "numeric(78, 0)", (col) => col.notNull()).addColumn("endBlock", "numeric(78, 0)", (col) => col.notNull()).execute();
      await db.schema.createIndex("factoryTraceFilterIntervalsFactoryId").on("factoryTraceFilterIntervals").column("factoryId").execute();
    }
  }
};
var StaticMigrationProvider = class {
  async getMigrations() {
    return migrations;
  }
};
var migrationProvider = new StaticMigrationProvider();
async function moveLegacyTables({
  common,
  db,
  newSchemaName
}) {
  let hasLegacyMigrations = false;
  try {
    const { rows } = await db.executeQuery(
      sql`SELECT * FROM public.kysely_migration LIMIT 1`.compile(db)
    );
    if (rows[0]?.name === "2023_05_15_0_initial")
      hasLegacyMigrations = true;
  } catch (e) {
    const error = e;
    if (!error.message.includes("does not exist"))
      throw error;
  }
  if (!hasLegacyMigrations)
    return;
  common.logger.warn({
    service: "database",
    msg: "Detected legacy sync migrations. Moving tables from 'public' schema to 'ponder_sync'."
  });
  async function moveOrDeleteTable(tableName) {
    try {
      await db.schema.alterTable(`public.${tableName}`).setSchema(newSchemaName).execute();
    } catch (e) {
      const error = e;
      switch (error.message) {
        case `relation "${tableName}" already exists in schema "${newSchemaName}"`: {
          await db.schema.dropTable(`public.${tableName}`).execute().catch(() => {
          });
          break;
        }
        case `relation "public.${tableName}" does not exist`: {
          break;
        }
        default: {
          common.logger.warn({
            service: "database",
            msg: `Failed to migrate table "${tableName}" to "ponder_sync" schema: ${error.message}`
          });
        }
      }
    }
    common.logger.warn({
      service: "database",
      msg: `Successfully moved 'public.${tableName}' table to 'ponder_sync' schema.`
    });
  }
  const tableNames = [
    "kysely_migration",
    "kysely_migration_lock",
    "blocks",
    "logs",
    "transactions",
    "rpcRequestResults",
    // Note that logFilterIntervals has a constraint that uses logFilters,
    // so the order here matters. Same story with factoryLogFilterIntervals.
    "logFilterIntervals",
    "logFilters",
    "factoryLogFilterIntervals",
    "factories",
    // Old ones that are no longer being used, but should still be moved
    // so that older migrations work as expected.
    "contractReadResults",
    "logFilterCachedRanges"
  ];
  for (const tableName of tableNames) {
    await moveOrDeleteTable(tableName);
  }
}

// src/utils/checkpoint.ts
var BLOCK_TIMESTAMP_DIGITS = 10;
var CHAIN_ID_DIGITS = 16;
var BLOCK_NUMBER_DIGITS = 16;
var TRANSACTION_INDEX_DIGITS = 16;
var EVENT_TYPE_DIGITS = 1;
var EVENT_INDEX_DIGITS = 16;
var CHECKPOINT_LENGTH = BLOCK_TIMESTAMP_DIGITS + CHAIN_ID_DIGITS + BLOCK_NUMBER_DIGITS + TRANSACTION_INDEX_DIGITS + EVENT_TYPE_DIGITS + EVENT_INDEX_DIGITS;
var EVENT_TYPES = {
  blocks: 5,
  logs: 5,
  callTraces: 7
};
var encodeCheckpoint = (checkpoint) => {
  const {
    blockTimestamp,
    chainId,
    blockNumber,
    transactionIndex,
    eventType,
    eventIndex
  } = checkpoint;
  if (eventType < 0 || eventType > 9)
    throw new Error(
      `Got invalid event type ${eventType}, expected a number from 0 to 9`
    );
  const result = blockTimestamp.toString().padStart(BLOCK_TIMESTAMP_DIGITS, "0") + chainId.toString().padStart(CHAIN_ID_DIGITS, "0") + blockNumber.toString().padStart(BLOCK_NUMBER_DIGITS, "0") + transactionIndex.toString().padStart(TRANSACTION_INDEX_DIGITS, "0") + eventType.toString() + eventIndex.toString().padStart(EVENT_INDEX_DIGITS, "0");
  if (result.length !== CHECKPOINT_LENGTH)
    throw new Error(`Invalid stringified checkpoint: ${result}`);
  return result;
};
var decodeCheckpoint = (checkpoint) => {
  let offset = 0;
  const blockTimestamp = +checkpoint.slice(
    offset,
    offset + BLOCK_TIMESTAMP_DIGITS
  );
  offset += BLOCK_TIMESTAMP_DIGITS;
  const chainId = BigInt(checkpoint.slice(offset, offset + CHAIN_ID_DIGITS));
  offset += CHAIN_ID_DIGITS;
  const blockNumber = BigInt(
    checkpoint.slice(offset, offset + BLOCK_NUMBER_DIGITS)
  );
  offset += BLOCK_NUMBER_DIGITS;
  const transactionIndex = BigInt(
    checkpoint.slice(offset, offset + TRANSACTION_INDEX_DIGITS)
  );
  offset += TRANSACTION_INDEX_DIGITS;
  const eventType = +checkpoint.slice(offset, offset + EVENT_TYPE_DIGITS);
  offset += EVENT_TYPE_DIGITS;
  const eventIndex = BigInt(
    checkpoint.slice(offset, offset + EVENT_INDEX_DIGITS)
  );
  offset += EVENT_INDEX_DIGITS;
  return {
    blockTimestamp,
    chainId,
    blockNumber,
    transactionIndex,
    eventType,
    eventIndex
  };
};
var zeroCheckpoint = {
  blockTimestamp: 0,
  chainId: 0n,
  blockNumber: 0n,
  transactionIndex: 0n,
  eventType: 0,
  eventIndex: 0n
};
var maxCheckpoint = {
  blockTimestamp: 9999999999,
  chainId: 9999999999999999n,
  blockNumber: 9999999999999999n,
  transactionIndex: 9999999999999999n,
  eventType: 9,
  eventIndex: 9999999999999999n
};
var LATEST = encodeCheckpoint(maxCheckpoint);

// src/utils/format.ts
var formatEta = (ms) => {
  if (ms < 1e3)
    return `${Math.round(ms)}ms`;
  const seconds = Math.floor(ms / 1e3);
  const h = Math.floor(seconds / 3600);
  const m = Math.floor((seconds - h * 3600) / 60);
  const s = seconds - h * 3600 - m * 60;
  const hstr = h > 0 ? `${h}h ` : "";
  const mstr = m > 0 || h > 0 ? `${m < 10 && h > 0 ? "0" : ""}${m}m ` : "";
  const sstr = s > 0 || m > 0 ? `${s < 10 && m > 0 ? "0" : ""}${s}s` : "";
  return `${hstr}${mstr}${sstr}`;
};
var formatPercentage = (cacheRate) => {
  const decimal = Math.round(cacheRate * 1e3) / 10;
  return Number.isInteger(decimal) && decimal < 100 ? `${decimal}.0%` : `${decimal}%`;
};

// src/utils/pg.ts
import pg from "pg";

// src/utils/print.ts
function prettyPrint(args) {
  const entries = Object.entries(args).map(([key, value]) => {
    if (value === void 0)
      return null;
    const trimmedValue = typeof value === "string" && value.length > 80 ? value.slice(0, 80).concat("...") : value;
    return [key, trimmedValue];
  }).filter(Boolean);
  const maxLength = entries.reduce(
    (acc, [key]) => Math.max(acc, key.length),
    0
  );
  return entries.map(([key, value]) => `  ${`${key}`.padEnd(maxLength + 1)}  ${value}`).join("\n");
}

// src/utils/pg.ts
var originalClientQuery = pg.Client.prototype.query;
pg.Client.prototype.query = function query(...args) {
  try {
    return originalClientQuery.apply(this, args);
  } catch (error_) {
    const error = error_;
    const [statement, parameters_] = args ?? ["empty", []];
    error.name = "PostgresError";
    let parameters = parameters_ ?? [];
    parameters = parameters.length <= 25 ? parameters : parameters.slice(0, 26).concat(["..."]);
    const params = parameters.reduce(
      (acc, parameter, idx) => {
        acc[idx + 1] = parameter;
        return acc;
      },
      {}
    );
    error.meta = Array.isArray(error.meta) ? error.meta : [];
    if (error.detail)
      error.meta.push(`Detail:
  ${error.detail}`);
    error.meta.push(`Statement:
  ${statement}`);
    error.meta.push(`Parameters:
${prettyPrint(params)}`);
    throw error;
  }
};
var ReadonlyClient = class extends pg.Client {
  // @ts-expect-error
  connect(callback) {
    if (callback) {
      super.connect(() => {
        this.query(
          "SET SESSION CHARACTERISTICS AS TRANSACTION READ ONLY",
          callback
        );
      });
    } else {
      return super.connect().then(async () => {
        await this.query(
          "SET SESSION CHARACTERISTICS AS TRANSACTION READ ONLY"
        );
      });
    }
  }
};
function createPool(config) {
  return new pg.Pool({
    // https://stackoverflow.com/questions/59155572/how-to-set-query-timeout-in-relation-to-statement-timeout
    statement_timeout: 2 * 60 * 1e3,
    // 2 minutes
    ...config
  });
}

// src/utils/pglite.ts
import { mkdirSync as mkdirSync2 } from "node:fs";
import { PGlite } from "@electric-sql/pglite";
function createPglite(options) {
  mkdirSync2(options.dataDir, { recursive: true });
  return new PGlite(options);
}

// src/database/index.ts
import { getTableColumns as getTableColumns2 } from "drizzle-orm";
import { drizzle as drizzleNodePg } from "drizzle-orm/node-postgres";
import { drizzle as drizzlePglite } from "drizzle-orm/pglite";
import {
  Migrator,
  PostgresDialect,
  WithSchemaPlugin,
  sql as sql2
} from "kysely";
import { KyselyPGlite } from "kysely-pglite";
import prometheus2 from "prom-client";

// src/database/kysely.ts
import { Kysely } from "kysely";
var RETRY_COUNT = 9;
var BASE_DURATION = 125;
var HeadlessKysely = class extends Kysely {
  common;
  name;
  isKilled = false;
  constructor({
    common,
    name,
    ...args
  }) {
    super(args);
    this.common = common;
    this.name = name;
  }
  async destroy() {
    this.isKilled = true;
  }
  wrap = async (options, fn) => {
    let firstError;
    let hasError = false;
    for (let i = 0; i <= RETRY_COUNT; i++) {
      const endClock = startClock();
      try {
        const result = await fn();
        this.common.metrics.ponder_database_method_duration.observe(
          { service: this.name, method: options.method },
          endClock()
        );
        return result;
      } catch (_error) {
        const error = _error;
        this.common.metrics.ponder_database_method_duration.observe(
          { service: this.name, method: options.method },
          endClock()
        );
        this.common.metrics.ponder_database_method_error_total.inc({
          service: this.name,
          method: options.method
        });
        if (this.isKilled) {
          this.common.logger.trace({
            service: this.name,
            msg: `Ignored error during '${options.method}' database method (service is killed)`
          });
          throw new IgnorableError();
        }
        if (!hasError) {
          hasError = true;
          firstError = error;
        }
        if (error instanceof NonRetryableError || options.shouldRetry?.(error) === false) {
          this.common.logger.warn({
            service: this.name,
            msg: `Failed '${options.method}' database method `
          });
          throw error;
        }
        if (i === RETRY_COUNT) {
          this.common.logger.warn({
            service: this.name,
            msg: `Failed '${options.method}' database method after '${i + 1}' attempts`,
            error
          });
          throw firstError;
        }
        const duration = BASE_DURATION * 2 ** i;
        this.common.logger.debug({
          service: this.name,
          msg: `Failed '${options.method}' database method, retrying after ${duration} milliseconds`,
          error
        });
        await wait(duration);
      }
    }
  };
};

// src/database/index.ts
var createDatabase = (args) => {
  let heartbeatInterval;
  let driver;
  let qb;
  const dialect = args.databaseConfig.kind;
  if (dialect === "pglite" || dialect === "pglite_test") {
    driver = {
      instance: dialect === "pglite" ? createPglite(args.databaseConfig.options) : args.databaseConfig.instance
    };
    const kyselyDialect = new KyselyPGlite(driver.instance).dialect;
    qb = {
      internal: new HeadlessKysely({
        name: "internal",
        common: args.common,
        dialect: kyselyDialect,
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "internal"
            });
          }
        },
        plugins: [new WithSchemaPlugin(args.namespace)]
      }),
      user: new HeadlessKysely({
        name: "user",
        common: args.common,
        dialect: kyselyDialect,
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "user"
            });
          }
        },
        plugins: [new WithSchemaPlugin(args.namespace)]
      }),
      readonly: new HeadlessKysely({
        name: "readonly",
        common: args.common,
        dialect: kyselyDialect,
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "readonly"
            });
          }
        },
        plugins: [new WithSchemaPlugin(args.namespace)]
      }),
      sync: new HeadlessKysely({
        name: "sync",
        common: args.common,
        dialect: kyselyDialect,
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "sync"
            });
          }
        },
        plugins: [new WithSchemaPlugin("ponder_sync")]
      })
    };
  } else {
    const internalMax = 2;
    const equalMax = Math.floor(
      (args.databaseConfig.poolConfig.max - internalMax) / 3
    );
    const [readonlyMax, userMax, syncMax] = args.common.options.command === "serve" ? [args.databaseConfig.poolConfig.max - internalMax, 0, 0] : [equalMax, equalMax, equalMax];
    driver = {
      internal: createPool({
        ...args.databaseConfig.poolConfig,
        application_name: `${args.namespace}_internal`,
        max: internalMax,
        statement_timeout: 10 * 60 * 1e3
        // 10 minutes to accommodate slow sync store migrations.
      }),
      user: createPool({
        ...args.databaseConfig.poolConfig,
        application_name: `${args.namespace}_user`,
        max: userMax
      }),
      readonly: createPool({
        ...args.databaseConfig.poolConfig,
        application_name: `${args.namespace}_readonly`,
        max: readonlyMax
      }),
      sync: createPool({
        ...args.databaseConfig.poolConfig,
        application_name: "ponder_sync",
        max: syncMax
      })
    };
    qb = {
      internal: new HeadlessKysely({
        name: "internal",
        common: args.common,
        dialect: new PostgresDialect({ pool: driver.internal }),
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "internal"
            });
          }
        },
        plugins: [new WithSchemaPlugin(args.namespace)]
      }),
      user: new HeadlessKysely({
        name: "user",
        common: args.common,
        dialect: new PostgresDialect({ pool: driver.user }),
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "user"
            });
          }
        },
        plugins: [new WithSchemaPlugin(args.namespace)]
      }),
      readonly: new HeadlessKysely({
        name: "readonly",
        common: args.common,
        dialect: new PostgresDialect({ pool: driver.readonly }),
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "readonly"
            });
          }
        },
        plugins: [new WithSchemaPlugin(args.namespace)]
      }),
      sync: new HeadlessKysely({
        name: "sync",
        common: args.common,
        dialect: new PostgresDialect({ pool: driver.sync }),
        log(event) {
          if (event.level === "query") {
            args.common.metrics.ponder_postgres_query_total.inc({
              pool: "sync"
            });
          }
        },
        plugins: [new WithSchemaPlugin("ponder_sync")]
      })
    };
    const d = driver;
    args.common.metrics.registry.removeSingleMetric(
      "ponder_postgres_pool_connections"
    );
    args.common.metrics.ponder_postgres_pool_connections = new prometheus2.Gauge(
      {
        name: "ponder_postgres_pool_connections",
        help: "Number of connections in the pool",
        labelNames: ["pool", "kind"],
        registers: [args.common.metrics.registry],
        collect() {
          this.set({ pool: "internal", kind: "idle" }, d.internal.idleCount);
          this.set({ pool: "internal", kind: "total" }, d.internal.totalCount);
          this.set({ pool: "sync", kind: "idle" }, d.sync.idleCount);
          this.set({ pool: "sync", kind: "total" }, d.sync.totalCount);
          this.set({ pool: "user", kind: "idle" }, d.user.idleCount);
          this.set({ pool: "user", kind: "total" }, d.user.totalCount);
          this.set({ pool: "readonly", kind: "idle" }, d.readonly.idleCount);
          this.set({ pool: "readonly", kind: "total" }, d.readonly.totalCount);
        }
      }
    );
    args.common.metrics.registry.removeSingleMetric(
      "ponder_postgres_query_queue_size"
    );
    args.common.metrics.ponder_postgres_query_queue_size = new prometheus2.Gauge(
      {
        name: "ponder_postgres_query_queue_size",
        help: "Number of queries waiting for an available connection",
        labelNames: ["pool"],
        registers: [args.common.metrics.registry],
        collect() {
          this.set({ pool: "internal" }, d.internal.waitingCount);
          this.set({ pool: "sync" }, d.sync.waitingCount);
          this.set({ pool: "user" }, d.user.waitingCount);
          this.set({ pool: "readonly" }, d.readonly.waitingCount);
        }
      }
    );
  }
  const drizzle3 = dialect === "pglite" || dialect === "pglite_test" ? drizzlePglite(driver.instance, {
    casing: "snake_case",
    schema: args.schema
  }) : drizzleNodePg(driver.user, {
    casing: "snake_case",
    schema: args.schema
  });
  const revert = async ({
    tableName,
    checkpoint,
    tx
  }) => {
    const primaryKeyColumns = getPrimaryKeyColumns(
      args.schema[tableName.js]
    );
    const rows = await tx.deleteFrom(tableName.reorg).returningAll().where("checkpoint", ">", checkpoint).execute();
    const reversed = rows.sort((a, b) => b.operation_id - a.operation_id);
    for (const log of reversed) {
      if (log.operation === 0) {
        await tx.deleteFrom(tableName.sql).$call((qb2) => {
          for (const { sql: sql5 } of primaryKeyColumns) {
            qb2 = qb2.where(sql5, "=", log[sql5]);
          }
          return qb2;
        }).execute();
      } else if (log.operation === 1) {
        log.operation_id = void 0;
        log.checkpoint = void 0;
        log.operation = void 0;
        await tx.updateTable(tableName.sql).set(log).$call((qb2) => {
          for (const { sql: sql5 } of primaryKeyColumns) {
            qb2 = qb2.where(sql5, "=", log[sql5]);
          }
          return qb2;
        }).execute();
      } else {
        log.operation_id = void 0;
        log.checkpoint = void 0;
        log.operation = void 0;
        await tx.insertInto(tableName.sql).values(log).onConflict(
          (oc) => oc.columns(primaryKeyColumns.map(({ sql: sql5 }) => sql5)).doNothing()
        ).execute();
      }
    }
    args.common.logger.info({
      service: "database",
      msg: `Reverted ${rows.length} unfinalized operations from '${tableName.user}' table`
    });
  };
  const database = {
    qb,
    drizzle: drizzle3,
    async migrateSync() {
      await qb.sync.wrap({ method: "migrateSyncStore" }, async () => {
        await moveLegacyTables({
          common: args.common,
          // @ts-expect-error
          db: qb.internal,
          newSchemaName: "ponder_sync"
        });
        const migrator = new Migrator({
          db: qb.sync,
          provider: migrationProvider,
          migrationTableSchema: "ponder_sync"
        });
        const { error } = await migrator.migrateToLatest();
        if (error)
          throw error;
      });
    },
    async setup() {
      const hasPonderSchema = await qb.internal.selectFrom("information_schema.schemata").select("schema_name").where("schema_name", "=", "ponder").executeTakeFirst().then((schema) => schema?.schema_name === "ponder");
      if (hasPonderSchema) {
        const hasNamespaceLockTable = await qb.internal.selectFrom("information_schema.tables").select(["table_name", "table_schema"]).where("table_name", "=", "namespace_lock").where("table_schema", "=", "ponder").executeTakeFirst().then((table) => table !== void 0);
        if (hasNamespaceLockTable) {
          await qb.internal.wrap({ method: "migrate" }, async () => {
            const namespaceCount = await qb.internal.withSchema("ponder").selectFrom("namespace_lock").select(sql2`count(*)`.as("count")).executeTakeFirst();
            const tableNames = await qb.internal.withSchema("ponder").selectFrom("namespace_lock").select("schema").where("namespace", "=", args.namespace).executeTakeFirst().then(
              (schema) => schema === void 0 ? void 0 : Object.keys(schema.schema.tables)
            );
            if (tableNames) {
              for (const tableName of tableNames) {
                await qb.internal.schema.dropTable(tableName).ifExists().cascade().execute();
              }
              await qb.internal.withSchema("ponder").deleteFrom("namespace_lock").where("namespace", "=", args.namespace).execute();
              if (namespaceCount.count === 1) {
                await qb.internal.schema.dropSchema("ponder").cascade().execute();
                args.common.logger.debug({
                  service: "database",
                  msg: `Removed 'ponder' schema`
                });
              }
            }
          });
        }
      }
      const hasPonderMetaTable = await qb.internal.selectFrom("information_schema.tables").select(["table_name", "table_schema"]).where("table_name", "=", "_ponder_meta").where("table_schema", "=", args.namespace).executeTakeFirst().then((table) => table !== void 0);
      if (hasPonderMetaTable) {
        await qb.internal.wrap(
          { method: "migrate" },
          () => qb.internal.transaction().execute(async (tx) => {
            const previousApp = await tx.selectFrom("_ponder_meta").where("key", "=", "app").select("value").executeTakeFirst().then(
              (row) => row === void 0 ? void 0 : row.value
            );
            if (previousApp) {
              const instanceId = crypto2.randomBytes(2).toString("hex");
              await tx.deleteFrom("_ponder_meta").where("key", "=", "app").execute();
              await tx.deleteFrom("_ponder_meta").where("key", "=", "status").execute();
              for (const tableName of previousApp.table_names) {
                await tx.schema.alterTable(tableName).renameTo(userToSqlTableName(tableName, instanceId)).execute();
                await tx.schema.alterTable(`_ponder_reorg__${tableName}`).renameTo(userToReorgTableName(tableName, instanceId)).execute();
              }
              await tx.insertInto("_ponder_meta").values({
                key: `app_${instanceId}`,
                value: { ...previousApp, instance_id: instanceId }
              }).execute();
              args.common.logger.debug({
                service: "database",
                msg: "Migrated previous app to v0.7"
              });
            }
          })
        );
      }
      await qb.internal.wrap({ method: "setup" }, async () => {
        for (const statement of args.statements.schema.sql) {
          await sql2.raw(statement).execute(qb.internal);
        }
        await qb.internal.schema.createTable("_ponder_meta").addColumn("key", "text", (col) => col.primaryKey()).addColumn("value", "jsonb").ifNotExists().execute();
      });
      const attempt = async ({ isFirstAttempt }) => qb.internal.wrap(
        { method: "setup" },
        () => qb.internal.transaction().execute(async (tx) => {
          const previousApps = await tx.selectFrom("_ponder_meta").where("key", "like", "app_%").select("value").execute().then((rows) => rows.map(({ value }) => value));
          const previousAppsWithBuildId = previousApps.filter(
            (app) => app.build_id === args.buildId && app.is_dev === 0
          );
          const newApp = {
            is_locked: 1,
            is_dev: args.common.options.command === "dev" ? 1 : 0,
            heartbeat_at: Date.now(),
            instance_id: args.instanceId,
            build_id: args.buildId,
            checkpoint: encodeCheckpoint(zeroCheckpoint),
            table_names: getTableNames(args.schema, args.instanceId).map(
              (tableName) => tableName.user
            )
          };
          if (previousAppsWithBuildId.length === 0) {
            await tx.insertInto("_ponder_meta").values({ key: `status_${args.instanceId}`, value: null }).onConflict(
              (oc) => oc.column("key").doUpdateSet({ value: null })
            ).execute();
            await tx.insertInto("_ponder_meta").values({
              key: `app_${args.instanceId}`,
              value: newApp
            }).onConflict(
              (oc) => oc.column("key").doUpdateSet({ value: newApp })
            ).execute();
            for (const tableName of getTableNames(
              args.schema,
              newApp.instance_id
            )) {
              await tx.schema.dropTable(tableName.sql).cascade().ifExists().execute();
              await tx.schema.dropTable(tableName.reorg).cascade().ifExists().execute();
            }
            for (let i = 0; i < args.statements.enums.sql.length; i++) {
              await sql2.raw(args.statements.enums.sql[i]).execute(tx).catch((_error) => {
                const error = _error;
                if (!error.message.includes("already exists"))
                  throw error;
                throw new NonRetryableError(
                  `Unable to create enum '${args.namespace}'.'${args.statements.enums.json[i].name}' because an enum with that name already exists.`
                );
              });
            }
            for (let i = 0; i < args.statements.tables.sql.length; i++) {
              await sql2.raw(args.statements.tables.sql[i]).execute(tx).catch((_error) => {
                const error = _error;
                if (!error.message.includes("already exists"))
                  throw error;
                throw new NonRetryableError(
                  `Unable to create table '${args.namespace}'.'${args.statements.tables.json[i].tableName}' because a table with that name already exists.`
                );
              });
            }
            args.common.logger.info({
              service: "database",
              msg: `Created tables [${newApp.table_names.join(", ")}]`
            });
            return {
              status: "success",
              checkpoint: encodeCheckpoint(zeroCheckpoint)
            };
          }
          const crashRecoveryApp = previousAppsWithBuildId.filter(
            (app) => app.is_locked === 0 || app.heartbeat_at + args.common.options.databaseHeartbeatTimeout <= Date.now()
          ).sort((a, b) => a.checkpoint > b.checkpoint ? -1 : 1)[0] ?? void 0;
          if (crashRecoveryApp && crashRecoveryApp.checkpoint > encodeCheckpoint(zeroCheckpoint) && args.common.options.command !== "dev") {
            await tx.insertInto("_ponder_meta").values({ key: `status_${args.instanceId}`, value: null }).execute();
            await tx.insertInto("_ponder_meta").values({
              key: `app_${args.instanceId}`,
              value: {
                ...newApp,
                checkpoint: crashRecoveryApp.checkpoint
              }
            }).execute();
            args.common.logger.info({
              service: "database",
              msg: `Detected cache hit for build '${args.buildId}' in schema '${args.namespace}' last active ${formatEta(Date.now() - crashRecoveryApp.heartbeat_at)} ago`
            });
            for (const tableName of getTableNames(
              args.schema,
              crashRecoveryApp.instance_id
            )) {
              await sql2.raw(
                `DROP TRIGGER IF EXISTS "${tableName.trigger}" ON "${args.namespace}"."${tableName.sql}"`
              ).execute(tx);
            }
            for (const indexStatement of args.statements.indexes.json) {
              await tx.schema.dropIndex(indexStatement.data.name).ifExists().execute();
              args.common.logger.info({
                service: "database",
                msg: `Dropped index '${indexStatement.data.name}' in schema '${args.namespace}'`
              });
            }
            for (const tableName of crashRecoveryApp.table_names) {
              await tx.schema.alterTable(
                userToSqlTableName(tableName, crashRecoveryApp.instance_id)
              ).renameTo(userToSqlTableName(tableName, args.instanceId)).execute();
              await tx.schema.alterTable(
                userToReorgTableName(
                  tableName,
                  crashRecoveryApp.instance_id
                )
              ).renameTo(userToReorgTableName(tableName, args.instanceId)).execute();
            }
            await tx.deleteFrom("_ponder_meta").where("key", "=", `status_${crashRecoveryApp.instance_id}`).execute();
            await tx.deleteFrom("_ponder_meta").where("key", "=", `app_${crashRecoveryApp.instance_id}`).execute();
            const { blockTimestamp, chainId, blockNumber } = decodeCheckpoint(
              crashRecoveryApp.checkpoint
            );
            args.common.logger.info({
              service: "database",
              msg: `Reverting operations after finalized checkpoint (timestamp=${blockTimestamp} chainId=${chainId} block=${blockNumber})`
            });
            for (const tableName of getTableNames(
              args.schema,
              args.instanceId
            )) {
              await revert({
                tableName,
                checkpoint: crashRecoveryApp.checkpoint,
                tx,
                instanceId: args.instanceId
              });
            }
            return {
              status: "success",
              checkpoint: crashRecoveryApp.checkpoint
            };
          }
          const nextAvailableApp = previousAppsWithBuildId.sort(
            (a, b) => a.heartbeat_at < b.heartbeat_at ? -1 : 1
          )[0];
          if (isFirstAttempt && args.common.options.command !== "dev" && (crashRecoveryApp === void 0 || crashRecoveryApp.is_locked === 1)) {
            return {
              status: "locked",
              expiry: nextAvailableApp.heartbeat_at + args.common.options.databaseHeartbeatTimeout
            };
          }
          await tx.insertInto("_ponder_meta").values({ key: `status_${args.instanceId}`, value: null }).onConflict((oc) => oc.column("key").doUpdateSet({ value: null })).execute();
          await tx.insertInto("_ponder_meta").values({
            key: `app_${args.instanceId}`,
            value: newApp
          }).onConflict(
            (oc) => oc.column("key").doUpdateSet({ value: newApp })
          ).execute();
          for (const tableName of getTableNames(
            args.schema,
            newApp.instance_id
          )) {
            await tx.schema.dropTable(tableName.sql).cascade().ifExists().execute();
            await tx.schema.dropTable(tableName.reorg).cascade().ifExists().execute();
          }
          for (let i = 0; i < args.statements.enums.sql.length; i++) {
            await sql2.raw(args.statements.enums.sql[i]).execute(tx).catch((_error) => {
              const error = _error;
              if (!error.message.includes("already exists"))
                throw error;
              throw new NonRetryableError(
                `Unable to create enum '${args.namespace}'.'${args.statements.enums.json[i].name}' because an enum with that name already exists.`
              );
            });
          }
          for (let i = 0; i < args.statements.tables.sql.length; i++) {
            await sql2.raw(args.statements.tables.sql[i]).execute(tx).catch((_error) => {
              const error = _error;
              if (!error.message.includes("already exists"))
                throw error;
              throw new NonRetryableError(
                `Unable to create table '${args.namespace}'.'${args.statements.tables.json[i].tableName}' because a table with that name already exists.`
              );
            });
          }
          args.common.logger.info({
            service: "database",
            msg: `Created tables [${newApp.table_names.join(", ")}]`
          });
          return {
            status: "success",
            checkpoint: encodeCheckpoint(zeroCheckpoint)
          };
        })
      );
      let result = await attempt({ isFirstAttempt: true });
      if (result.status === "locked") {
        const duration = result.expiry - Date.now();
        args.common.logger.warn({
          service: "database",
          msg: `Schema '${args.namespace}' is locked by a different Ponder app`
        });
        args.common.logger.warn({
          service: "database",
          msg: `Waiting ${formatEta(duration)} for lock on schema '${args.namespace} to expire...`
        });
        await wait(duration);
        result = await attempt({ isFirstAttempt: false });
        if (result.status === "locked") {
          throw new NonRetryableError(
            `Failed to acquire lock on schema '${args.namespace}'. A different Ponder app is actively using this database.`
          );
        }
      }
      if (process.env.PONDER_EXPERIMENTAL_DB !== "platform") {
        const apps = await qb.internal.selectFrom("_ponder_meta").where("key", "like", "app_%").select("value").execute().then((rows) => rows.map(({ value }) => value));
        const removedDevApps = apps.filter(
          (app) => app.is_dev === 1 && (app.is_locked === 0 || app.heartbeat_at + args.common.options.databaseHeartbeatTimeout < Date.now())
        );
        const removedStartApps = apps.filter(
          (app) => app.is_dev === 0 && (app.is_locked === 0 || app.heartbeat_at + args.common.options.databaseHeartbeatTimeout < Date.now())
        ).sort((a, b) => a.heartbeat_at > b.heartbeat_at ? -1 : 1).slice(2);
        const removedApps = [...removedDevApps, ...removedStartApps];
        for (const app of removedApps) {
          for (const table of app.table_names) {
            await qb.internal.schema.dropTable(userToSqlTableName(table, app.instance_id)).cascade().ifExists().execute();
            await qb.internal.schema.dropTable(userToReorgTableName(table, app.instance_id)).cascade().ifExists().execute();
          }
          await qb.internal.deleteFrom("_ponder_meta").where("key", "=", `status_${app.instance_id}`).execute();
          await qb.internal.deleteFrom("_ponder_meta").where("key", "=", `app_${app.instance_id}`).execute();
        }
        if (removedApps.length > 0) {
          args.common.logger.debug({
            service: "database",
            msg: `Removed tables corresponding to apps [${removedApps.map((app) => app.instance_id)}]`
          });
        }
        if (apps.length === 1 || args.common.options.command === "dev") {
          await this.createLiveViews();
        }
      }
      heartbeatInterval = setInterval(async () => {
        try {
          const heartbeat = Date.now();
          await qb.internal.updateTable("_ponder_meta").where("key", "=", `app_${args.instanceId}`).set({
            value: sql2`jsonb_set(value, '{heartbeat_at}', ${heartbeat})`
          }).execute();
          args.common.logger.debug({
            service: "database",
            msg: `Updated heartbeat timestamp to ${heartbeat} (build_id=${args.buildId})`
          });
        } catch (err) {
          const error = err;
          args.common.logger.error({
            service: "database",
            msg: `Failed to update heartbeat timestamp, retrying in ${formatEta(
              args.common.options.databaseHeartbeatInterval
            )}`,
            error
          });
        }
      }, args.common.options.databaseHeartbeatInterval);
      return { checkpoint: result.checkpoint };
    },
    async createIndexes() {
      for (const statement of args.statements.indexes.sql) {
        await sql2.raw(statement).execute(qb.internal);
      }
    },
    async createLiveViews() {
      if (process.env.PONDER_EXPERIMENTAL_DB === "platform")
        return;
      await qb.internal.wrap({ method: "createLiveViews" }, async () => {
        const previousLiveInstanceId = await qb.internal.selectFrom("_ponder_meta").select("value").where("key", "=", "live").executeTakeFirst().then((row) => row?.value?.instance_id ?? void 0);
        if (previousLiveInstanceId) {
          const previousTableNames = await qb.internal.selectFrom("_ponder_meta").select("value").where("key", "=", `app_${previousLiveInstanceId}`).executeTakeFirst().then((row) => row ? row.value.table_names : []);
          await Promise.all(
            previousTableNames.map(
              (name) => qb.internal.schema.dropView(name).ifExists().execute()
            )
          );
        }
        await qb.internal.insertInto("_ponder_meta").values({
          key: "live",
          value: { instance_id: args.instanceId }
        }).onConflict(
          (oc) => oc.column("key").doUpdateSet({ value: { instance_id: args.instanceId } })
        ).execute();
        for (const tableName of getTableNames(args.schema, args.instanceId)) {
          await qb.internal.schema.createView(tableName.user).orReplace().as(qb.internal.selectFrom(tableName.sql).selectAll()).execute();
          args.common.logger.info({
            service: "database",
            msg: `Created view '${args.namespace}'.'${tableName.user}'`
          });
        }
      });
    },
    async createTriggers() {
      await qb.internal.wrap({ method: "createTriggers" }, async () => {
        for (const tableName of getTableNames(args.schema, args.instanceId)) {
          const columns = getTableColumns2(
            args.schema[tableName.js]
          );
          const columnNames = Object.values(columns).map(
            (column) => `"${getColumnCasing(column, "snake_case")}"`
          );
          await sql2.raw(`
CREATE OR REPLACE FUNCTION ${tableName.triggerFn}
RETURNS TRIGGER AS $$
BEGIN
  IF TG_OP = 'INSERT' THEN
    INSERT INTO "${args.namespace}"."${tableName.reorg}" (${columnNames.join(",")}, operation, checkpoint)
    VALUES (${columnNames.map((name) => `NEW.${name}`).join(",")}, 0, '${encodeCheckpoint(maxCheckpoint)}');
  ELSIF TG_OP = 'UPDATE' THEN
    INSERT INTO "${args.namespace}"."${tableName.reorg}" (${columnNames.join(",")}, operation, checkpoint)
    VALUES (${columnNames.map((name) => `OLD.${name}`).join(",")}, 1, '${encodeCheckpoint(maxCheckpoint)}');
  ELSIF TG_OP = 'DELETE' THEN
    INSERT INTO "${args.namespace}"."${tableName.reorg}" (${columnNames.join(",")}, operation, checkpoint)
    VALUES (${columnNames.map((name) => `OLD.${name}`).join(",")}, 2, '${encodeCheckpoint(maxCheckpoint)}');
  END IF;
  RETURN NULL;
END;
$$ LANGUAGE plpgsql
`).execute(qb.internal);
          await sql2.raw(`
          CREATE TRIGGER "${tableName.trigger}"
          AFTER INSERT OR UPDATE OR DELETE ON "${args.namespace}"."${tableName.sql}"
          FOR EACH ROW EXECUTE FUNCTION ${tableName.triggerFn};
          `).execute(qb.internal);
        }
      });
    },
    async removeTriggers() {
      await qb.internal.wrap({ method: "removeTriggers" }, async () => {
        for (const tableName of getTableNames(args.schema, args.instanceId)) {
          await sql2.raw(
            `DROP TRIGGER IF EXISTS "${tableName.trigger}" ON "${args.namespace}"."${tableName.sql}"`
          ).execute(qb.internal);
        }
      });
    },
    async revert({ checkpoint }) {
      await qb.internal.wrap(
        { method: "revert" },
        () => Promise.all(
          getTableNames(args.schema, args.instanceId).map(
            (tableName) => qb.internal.transaction().execute(
              (tx) => revert({
                tableName,
                checkpoint,
                tx,
                instanceId: args.instanceId
              })
            )
          )
        )
      );
    },
    async finalize({ checkpoint }) {
      await qb.internal.wrap({ method: "finalize" }, async () => {
        await qb.internal.updateTable("_ponder_meta").where("key", "=", `app_${args.instanceId}`).set({
          value: sql2`jsonb_set(value, '{checkpoint}', to_jsonb(${checkpoint}::varchar(75)))`
        }).execute();
        await Promise.all(
          getTableNames(args.schema, args.instanceId).map(
            (tableName) => qb.internal.deleteFrom(tableName.reorg).where("checkpoint", "<=", checkpoint).execute()
          )
        );
      });
      const decoded = decodeCheckpoint(checkpoint);
      args.common.logger.debug({
        service: "database",
        msg: `Updated finalized checkpoint to (timestamp=${decoded.blockTimestamp} chainId=${decoded.chainId} block=${decoded.blockNumber})`
      });
    },
    async complete({ checkpoint }) {
      await Promise.all(
        getTableNames(args.schema, args.instanceId).map(
          (tableName) => qb.internal.wrap({ method: "complete" }, async () => {
            await qb.internal.updateTable(tableName.reorg).set({ checkpoint }).where("checkpoint", "=", encodeCheckpoint(maxCheckpoint)).execute();
          })
        )
      );
    },
    async unlock() {
      clearInterval(heartbeatInterval);
      await qb.internal.wrap({ method: "unlock" }, async () => {
        await qb.internal.updateTable("_ponder_meta").where("key", "=", `app_${args.instanceId}`).set({
          value: sql2`jsonb_set(value, '{is_locked}', to_jsonb(0))`
        }).execute();
      });
    },
    async kill() {
      await qb.internal.destroy();
      await qb.user.destroy();
      await qb.readonly.destroy();
      await qb.sync.destroy();
      if (dialect === "pglite") {
        const d = driver;
        await d.instance.close();
      }
      if (dialect === "pglite_test") {
      }
      if (dialect === "postgres") {
        const d = driver;
        await d.internal.end();
        await d.user.end();
        await d.readonly.end();
        await d.sync.end();
      }
      args.common.logger.debug({
        service: "database",
        msg: "Closed connection to database"
      });
    }
  };
  return database;
};

// src/ui/app.tsx
import { Box as Box2, Text as Text3, render as inkRender } from "ink";
import React3 from "react";

// src/ui/ProgressBar.tsx
import { Text } from "ink";
import React from "react";
var ProgressBar = ({ current = 5, end = 10, width = 36 }) => {
  const maxCount = width || process.stdout.columns || 80;
  const fraction = current / end;
  const count = Math.min(Math.floor(maxCount * fraction), maxCount);
  return /* @__PURE__ */ React.createElement(Text, null, /* @__PURE__ */ React.createElement(Text, null, "\u2588".repeat(count)), /* @__PURE__ */ React.createElement(Text, null, "\u2591".repeat(maxCount - count)));
};

// src/ui/Table.tsx
import { Box, Text as Text2 } from "ink";
import React2 from "react";
var MAX_COLUMN_WIDTH = 24;
function Table(props) {
  const { columns, rows } = props;
  const formattedRows = rows.map(
    (row) => columns.reduce(
      (acc, column) => ({
        ...acc,
        [column.key.toString()]: column.format ? column.format(row[column.key], row) : row[column.key]
      }),
      {}
    )
  );
  const columnWidths = columns.map((column) => {
    let maxWidth = Math.max(
      ...formattedRows.map(
        (row) => row[column.key] !== void 0 ? row[column.key].toString().length : 9
      ),
      column.title.length
    );
    maxWidth = Math.min(maxWidth, MAX_COLUMN_WIDTH);
    return maxWidth;
  });
  return /* @__PURE__ */ React2.createElement(Box, { flexDirection: "column" }, /* @__PURE__ */ React2.createElement(Box, { flexDirection: "row", key: "title" }, columns.map(({ title }, index) => /* @__PURE__ */ React2.createElement(React2.Fragment, { key: `title-${title}` }, /* @__PURE__ */ React2.createElement(Text2, null, "\u2502"), /* @__PURE__ */ React2.createElement(
    Box,
    {
      width: columnWidths[index],
      justifyContent: "flex-start",
      marginX: 1
    },
    /* @__PURE__ */ React2.createElement(Text2, { bold: true, wrap: "truncate-end" }, title)
  ))), /* @__PURE__ */ React2.createElement(Text2, null, "\u2502")), /* @__PURE__ */ React2.createElement(Box, { flexDirection: "row", key: "separator" }, /* @__PURE__ */ React2.createElement(Text2, null, "\u251C"), columnWidths.map((width, index) => (
    // biome-ignore lint/suspicious/noArrayIndexKey: <explanation>
    /* @__PURE__ */ React2.createElement(Text2, { key: index }, "\u2500".repeat(width + 2), index < columns.length - 1 ? "\u253C" : "\u2524")
  ))), formattedRows.map((row, rowIndex) => /* @__PURE__ */ React2.createElement(
    Box,
    {
      flexDirection: "row",
      key: rowIndex
    },
    columns.map(({ key, align }, index) => (
      // biome-ignore lint/suspicious/noArrayIndexKey: <explanation>
      /* @__PURE__ */ React2.createElement(React2.Fragment, { key: index }, /* @__PURE__ */ React2.createElement(Text2, null, "\u2502"), /* @__PURE__ */ React2.createElement(
        Box,
        {
          width: columnWidths[index],
          justifyContent: align === "left" ? "flex-start" : "flex-end",
          marginX: 1
        },
        /* @__PURE__ */ React2.createElement(Text2, { wrap: "truncate-end" }, row[key])
      ))
    )),
    /* @__PURE__ */ React2.createElement(Text2, null, "\u2502")
  )));
}
var Table_default = Table;

// src/ui/app.tsx
var buildUiState = () => {
  return {
    port: 42069,
    hostname: "localhost",
    sync: [],
    indexing: {
      hasError: false,
      overall: {
        completedSeconds: 0,
        totalSeconds: 0,
        progress: 0,
        completedToTimestamp: 0,
        totalEvents: 0
      },
      events: []
    },
    app: {
      progress: 0,
      eta: void 0,
      mode: void 0
    }
  };
};
var App = (ui) => {
  const { sync, indexing, app, port, hostname } = ui;
  return /* @__PURE__ */ React3.createElement(Box2, { flexDirection: "column" }, /* @__PURE__ */ React3.createElement(Text3, null, " "), indexing.hasError ? /* @__PURE__ */ React3.createElement(Text3, { color: "cyan" }, "Resolve the error and save your changes to reload the server.") : /* @__PURE__ */ React3.createElement(React3.Fragment, null, /* @__PURE__ */ React3.createElement(Text3, { bold: true }, "Sync"), /* @__PURE__ */ React3.createElement(Text3, null, " "), sync.length === 0 ? /* @__PURE__ */ React3.createElement(Text3, null, "Waiting to start...") : /* @__PURE__ */ React3.createElement(
    Table_default,
    {
      rows: sync,
      columns: [
        {
          title: "Network",
          key: "networkName",
          align: "left"
        },
        {
          title: "Status",
          key: "status",
          align: "left",
          format: (_, row) => row.status === "historical" ? `${row.status} (${formatPercentage(row.progress)})` : row.status
        },
        {
          title: "Block",
          key: "block",
          align: "right"
        },
        {
          title: "RPC (req/s)",
          key: "rps",
          align: "right",
          format: (_, row) => row.rps.toFixed(1)
        }
      ]
    }
  ), /* @__PURE__ */ React3.createElement(Text3, null, " "), /* @__PURE__ */ React3.createElement(Text3, { bold: true }, "Indexing"), /* @__PURE__ */ React3.createElement(Text3, null, " "), indexing.events.length === 0 ? /* @__PURE__ */ React3.createElement(Text3, null, "Waiting to start...") : /* @__PURE__ */ React3.createElement(
    Table_default,
    {
      rows: indexing.events,
      columns: [
        { title: "Event", key: "eventName", align: "left" },
        { title: "Count", key: "count", align: "right" },
        {
          title: "Duration (ms)",
          key: "averageDuration",
          align: "right",
          format: (v) => v > 0 ? v < 1e-3 ? "<0.001" : v.toFixed(3) : "-"
        }
      ]
    }
  ), /* @__PURE__ */ React3.createElement(Text3, null, " "), /* @__PURE__ */ React3.createElement(Box2, { flexDirection: "row" }, /* @__PURE__ */ React3.createElement(Text3, { bold: true }, "Progress "), app.mode === void 0 || app.progress === 0 ? null : /* @__PURE__ */ React3.createElement(Text3, null, "(", app.mode === "historical" ? /* @__PURE__ */ React3.createElement(Text3, { color: "yellowBright" }, "historical") : app.mode === "realtime" ? /* @__PURE__ */ React3.createElement(Text3, { color: "greenBright" }, "live") : /* @__PURE__ */ React3.createElement(Text3, { color: "greenBright" }, "complete"), ")")), /* @__PURE__ */ React3.createElement(Text3, null, " "), /* @__PURE__ */ React3.createElement(Box2, { flexDirection: "row" }, /* @__PURE__ */ React3.createElement(ProgressBar, { current: app.progress, end: 1, width: 48 }), /* @__PURE__ */ React3.createElement(Text3, null, " ", formatPercentage(app.progress), app.eta === void 0 || app.eta === 0 ? null : ` (${formatEta(app.eta)} eta)`)), /* @__PURE__ */ React3.createElement(Text3, null, " "), /* @__PURE__ */ React3.createElement(Box2, { flexDirection: "column" }, /* @__PURE__ */ React3.createElement(Text3, { bold: true }, "GraphQL "), /* @__PURE__ */ React3.createElement(Box2, { flexDirection: "row" }, /* @__PURE__ */ React3.createElement(Text3, null, "Server live at http://", hostname, ":", port)))));
};
var setupInkApp = (ui) => {
  const app = inkRender(/* @__PURE__ */ React3.createElement(App, { ...ui }));
  return {
    render: (newUi) => {
      app.rerender(/* @__PURE__ */ React3.createElement(App, { ...newUi }));
    },
    unmount: () => {
      app.clear();
      app.unmount();
    }
  };
};

// src/ui/service.ts
function createUi({ common }) {
  const ui = buildUiState();
  const { render, unmount } = setupInkApp(ui);
  let isKilled = false;
  const renderInterval = setInterval(async () => {
    if (isKilled)
      return;
    ui.sync = await getSyncProgress(common.metrics);
    ui.indexing = await getIndexingProgress(common.metrics);
    ui.app = await getAppProgress(common.metrics);
    if (common.options.hostname)
      ui.hostname = common.options.hostname;
    ui.port = (await common.metrics.ponder_http_server_port.get()).values[0].value;
    render(ui);
  }, 100);
  const kill3 = () => {
    isKilled = true;
    clearInterval(renderInterval);
    unmount();
  };
  return {
    kill: kill3
  };
}

// src/indexing-store/historical.ts
import {
  and,
  eq,
  getTableColumns as getTableColumns3,
  sql as sql3
} from "drizzle-orm";
import { getTableConfig as getTableConfig2 } from "drizzle-orm/pg-core";
import { drizzle } from "drizzle-orm/pg-proxy";

// src/indexing-store/index.ts
var parseSqlError = (e) => {
  let error = getBaseError(e);
  if (error?.message?.includes("violates not-null constraint")) {
    error = new NotNullConstraintError(error.message);
  } else if (error?.message?.includes("violates unique constraint")) {
    error = new UniqueConstraintError(error.message);
  } else if (error?.message.includes("violates check constraint")) {
    error = new CheckConstraintError(error.message);
  } else if (error?.message?.includes("Do not know how to serialize a BigInt")) {
    error = new BigIntSerializationError(error.message);
    error.meta.push(
      "Hint:\n  The JSON column type does not support BigInt values. Use the replaceBigInts() helper function before inserting into the database. Docs: https://ponder.sh/docs/utilities/replace-bigints"
    );
  }
  return error;
};

// src/indexing-store/historical.ts
var checkOnchainTable = (table, method) => {
  if (table === void 0)
    throw new UndefinedTableError(
      `Table object passed to db.${method}() is undefined`
    );
  if (onchain in table)
    return;
  throw new InvalidStoreMethodError(
    method === "find" ? `db.find() can only be used with onchain tables, and '${getTableConfig2(table).name}' is an offchain table.` : `Indexing functions can only write to onchain tables, and '${getTableConfig2(table).name}' is an offchain table.`
  );
};
var hasEmptyValue = (column) => {
  return column.hasDefault;
};
var getEmptyValue = (column, type) => {
  if (type === 1 /* UPDATE */ && column.onUpdateFn) {
    return column.onUpdateFn();
  }
  if (column.default !== void 0)
    return column.default;
  if (column.defaultFn !== void 0)
    return column.defaultFn();
  if (column.onUpdateFn !== void 0)
    return column.onUpdateFn();
  return void 0;
};
var normalizeColumn = (column, value, type) => {
  if (value === void 0) {
    if (hasEmptyValue(column))
      return getEmptyValue(column, type);
    return null;
  }
  if (column.mapToDriverValue === void 0)
    return value;
  try {
    return column.mapFromDriverValue(column.mapToDriverValue(value));
  } catch (e) {
    if (e?.message?.includes("Do not know how to serialize a BigInt")) {
      const error = new BigIntSerializationError(e.message);
      error.meta.push(
        "Hint:\n  The JSON column type does not support BigInt values. Use the replaceBigInts() helper function before inserting into the database. Docs: https://ponder.sh/docs/utilities/replace-bigints"
      );
      throw error;
    }
  }
};
var createHistoricalIndexingStore = ({
  common,
  database,
  schema,
  initialCheckpoint
}) => {
  const queue = createQueue({
    browser: false,
    initialStart: true,
    concurrency: 1,
    worker: (fn) => {
      return fn();
    }
  });
  const tableNameCache = /* @__PURE__ */ new Map();
  const primaryKeysCache = /* @__PURE__ */ new Map();
  const cache = /* @__PURE__ */ new Map();
  for (const tableName of getTableNames(schema, "")) {
    primaryKeysCache.set(
      schema[tableName.js],
      getPrimaryKeyColumns(schema[tableName.js])
    );
    cache.set(schema[tableName.js], /* @__PURE__ */ new Map());
    tableNameCache.set(schema[tableName.js], tableName.user);
  }
  const getCacheKey = (table, row) => {
    const primaryKeys = primaryKeysCache.get(table);
    return primaryKeys.map((pk) => normalizeColumn(table[pk.js], row[pk.js])).join("_");
  };
  const getCacheEntry = (table, row) => {
    return cache.get(table).get(getCacheKey(table, row));
  };
  const setCacheEntry = (table, userRow, entryType, existingRow) => {
    let row = structuredClone(userRow);
    if (existingRow) {
      for (const [key, value] of Object.entries(row)) {
        existingRow[key] = value;
      }
      existingRow = normalizeRow(table, existingRow, entryType);
      const bytes = getBytes(existingRow);
      cacheBytes += bytes;
      cache.get(table).set(getCacheKey(table, existingRow), {
        type: entryType,
        row: existingRow,
        operationIndex: totalCacheOps++,
        bytes
      });
      return structuredClone(existingRow);
    } else {
      row = normalizeRow(table, row, entryType);
      const bytes = getBytes(row);
      cacheBytes += bytes;
      cache.get(table).set(getCacheKey(table, row), {
        type: entryType,
        bytes,
        operationIndex: totalCacheOps++,
        row
      });
      return structuredClone(row);
    }
  };
  const deleteCacheEntry = (table, row) => {
    const entry = getCacheEntry(table, row);
    if (entry) {
      cacheBytes -= entry.bytes;
    }
    return cache.get(table).delete(getCacheKey(table, row));
  };
  const normalizeRow = (table, row, type) => {
    for (const [columnName, column] of Object.entries(getTableColumns3(table))) {
      if (type === 0 /* INSERT */ && (row[columnName] === void 0 || row[columnName] === null) && column.notNull && hasEmptyValue(column) === false) {
        const error = new NotNullConstraintError(
          `Column '${tableNameCache.get(table)}.${columnName}' violates not-null constraint.`
        );
        error.meta.push(
          `db.${type === 0 /* INSERT */ ? "insert" : "update"} arguments:
${prettyPrint(row)}`
        );
        throw error;
      }
      row[columnName] = normalizeColumn(column, row[columnName], type);
    }
    return row;
  };
  const getBytes = (value) => {
    let size = 13;
    if (typeof value === "number") {
      size += 8;
    } else if (typeof value === "string") {
      size += 2 * value.length;
    } else if (typeof value === "boolean") {
      size += 4;
    } else if (typeof value === "bigint") {
      size += 48;
    } else if (value === null || value === void 0) {
      size += 8;
    } else if (Array.isArray(value)) {
      for (const e of value) {
        size += getBytes(e);
      }
    } else {
      for (const col of Object.values(value)) {
        size += getBytes(col);
      }
    }
    return size;
  };
  let isDatabaseEmpty = initialCheckpoint === encodeCheckpoint(zeroCheckpoint);
  let cacheBytes = 0;
  let totalCacheOps = 0;
  const maxBytes = common.options.indexingCacheMaxBytes;
  common.logger.debug({
    service: "indexing",
    msg: `Using a ${Math.round(maxBytes / (1024 * 1024))} MB indexing cache`
  });
  const getWhereCondition = (table, key) => {
    primaryKeysCache.get(table);
    const conditions = [];
    for (const { js } of primaryKeysCache.get(table)) {
      conditions.push(eq(table[js], key[js]));
    }
    return and(...conditions);
  };
  const find = (table, key) => {
    return database.drizzle.select().from(table).where(getWhereCondition(table, key)).then((res) => res.length === 0 ? null : res[0]);
  };
  const indexingStore = {
    // @ts-ignore
    find: (table, key) => queue.add(
      () => database.qb.user.wrap(
        { method: `${tableNameCache.get(table) ?? "unknown"}.find()` },
        async () => {
          checkOnchainTable(table, "find");
          const entry = getCacheEntry(table, key);
          if (entry) {
            getCacheEntry(table, key).operationIndex = totalCacheOps++;
            return entry.row;
          } else {
            if (isDatabaseEmpty)
              return null;
            const row = await find(table, key);
            const bytes = getBytes(row);
            cacheBytes += bytes;
            cache.get(table).set(getCacheKey(table, key), {
              type: 2 /* FIND */,
              bytes,
              operationIndex: totalCacheOps++,
              row
            });
            return find(table, key);
          }
        }
      )
    ),
    // @ts-ignore
    insert(table) {
      return {
        values: (values) => {
          const inner = {
            onConflictDoNothing: () => queue.add(
              () => database.qb.user.wrap(
                {
                  method: `${tableNameCache.get(table) ?? "unknown"}.insert()`
                },
                async () => {
                  checkOnchainTable(table, "insert");
                  if (Array.isArray(values)) {
                    const rows = [];
                    for (const value of values) {
                      const entry = getCacheEntry(table, value);
                      let row;
                      if (entry?.row) {
                        row = entry.row;
                      } else {
                        if (isDatabaseEmpty)
                          row = null;
                        else
                          row = await find(table, value);
                      }
                      if (row === null) {
                        rows.push(
                          setCacheEntry(table, value, 0 /* INSERT */)
                        );
                      } else {
                        rows.push(null);
                      }
                    }
                    return rows;
                  } else {
                    const entry = getCacheEntry(table, values);
                    let row;
                    if (entry?.row) {
                      row = entry.row;
                    } else {
                      if (isDatabaseEmpty)
                        row = null;
                      else
                        row = await find(table, values);
                    }
                    if (row === null) {
                      return setCacheEntry(table, values, 0 /* INSERT */);
                    }
                    return null;
                  }
                }
              )
            ),
            onConflictDoUpdate: (valuesU) => queue.add(
              () => database.qb.user.wrap(
                {
                  method: `${tableNameCache.get(table) ?? "unknown"}.insert()`
                },
                async () => {
                  checkOnchainTable(table, "insert");
                  if (Array.isArray(values)) {
                    const rows = [];
                    for (const value of values) {
                      const entry = getCacheEntry(table, value);
                      deleteCacheEntry(table, value);
                      let row;
                      if (entry?.row) {
                        row = entry.row;
                      } else {
                        if (isDatabaseEmpty)
                          row = null;
                        else
                          row = await find(table, value);
                      }
                      if (row === null) {
                        rows.push(
                          setCacheEntry(table, value, 0 /* INSERT */)
                        );
                      } else {
                        if (typeof valuesU === "function") {
                          rows.push(
                            setCacheEntry(
                              table,
                              valuesU(row),
                              entry?.type === 0 /* INSERT */ ? 0 /* INSERT */ : 1 /* UPDATE */,
                              row
                            )
                          );
                        } else {
                          rows.push(
                            setCacheEntry(
                              table,
                              valuesU,
                              entry?.type === 0 /* INSERT */ ? 0 /* INSERT */ : 1 /* UPDATE */,
                              row
                            )
                          );
                        }
                      }
                    }
                    return rows;
                  } else {
                    const entry = getCacheEntry(table, values);
                    deleteCacheEntry(table, values);
                    let row;
                    if (entry?.row) {
                      row = entry.row;
                    } else {
                      if (isDatabaseEmpty)
                        row = null;
                      else
                        row = await find(table, values);
                    }
                    if (row === null) {
                      return setCacheEntry(table, values, 0 /* INSERT */);
                    } else {
                      if (typeof valuesU === "function") {
                        return setCacheEntry(
                          table,
                          valuesU(row),
                          entry?.type === 0 /* INSERT */ ? 0 /* INSERT */ : 1 /* UPDATE */,
                          row
                        );
                      } else {
                        return setCacheEntry(
                          table,
                          valuesU,
                          entry?.type === 0 /* INSERT */ ? 0 /* INSERT */ : 1 /* UPDATE */,
                          row
                        );
                      }
                    }
                  }
                }
              )
            ),
            // biome-ignore lint/suspicious/noThenProperty: <explanation>
            then: (onFulfilled, onRejected) => queue.add(
              () => database.qb.user.wrap(
                {
                  method: `${tableNameCache.get(table) ?? "unknown"}.insert()`
                },
                async () => {
                  checkOnchainTable(table, "insert");
                  if (Array.isArray(values)) {
                    const rows = [];
                    for (const value of values) {
                      if (getCacheEntry(table, value)?.row) {
                        const error = new UniqueConstraintError(
                          `Unique constraint failed for '${tableNameCache.get(table)}'.`
                        );
                        error.meta.push(
                          `db.insert arguments:
${prettyPrint(value)}`
                        );
                        throw error;
                      } else if (isDatabaseEmpty === false) {
                        const findResult = await find(table, value);
                        if (findResult) {
                          const error = new UniqueConstraintError(
                            `Unique constraint failed for '${tableNameCache.get(table)}'.`
                          );
                          error.meta.push(
                            `db.insert arguments:
${prettyPrint(value)}`
                          );
                          throw error;
                        }
                      }
                      rows.push(
                        setCacheEntry(table, value, 0 /* INSERT */)
                      );
                    }
                    return rows;
                  } else {
                    if (getCacheEntry(table, values)?.row) {
                      const error = new UniqueConstraintError(
                        `Unique constraint failed for '${tableNameCache.get(table)}'.`
                      );
                      error.meta.push(
                        `db.insert arguments:
${prettyPrint(values)}`
                      );
                      throw error;
                    } else if (isDatabaseEmpty === false) {
                      const findResult = await find(table, values);
                      if (findResult) {
                        const error = new UniqueConstraintError(
                          `Unique constraint failed for '${tableNameCache.get(table)}'.`
                        );
                        error.meta.push(
                          `db.insert arguments:
${prettyPrint(values)}`
                        );
                        throw error;
                      }
                    }
                    return setCacheEntry(table, values, 0 /* INSERT */);
                  }
                }
              )
            ).then(onFulfilled, onRejected),
            catch: (onRejected) => inner.then(void 0, onRejected),
            finally: (onFinally) => inner.then(
              (value) => {
                onFinally?.();
                return value;
              },
              (reason) => {
                onFinally?.();
                throw reason;
              }
            )
            // @ts-ignore
          };
          return inner;
        }
      };
    },
    // @ts-ignore
    update(table, key) {
      return {
        set: (values) => queue.add(
          () => database.qb.user.wrap(
            { method: `${tableNameCache.get(table) ?? "unknown"}.update()` },
            async () => {
              checkOnchainTable(table, "update");
              const entry = getCacheEntry(table, key);
              deleteCacheEntry(table, key);
              let row;
              if (entry?.row) {
                row = entry.row;
              } else {
                if (isDatabaseEmpty) {
                  const error = new RecordNotFoundError(
                    `No existing record found in table '${tableNameCache.get(table)}'`
                  );
                  error.meta.push(
                    `db.update arguments:
${prettyPrint(key)}`
                  );
                  throw error;
                }
                const findResult = await find(table, key);
                if (findResult) {
                  row = findResult;
                } else {
                  const error = new RecordNotFoundError(
                    `No existing record found in table '${tableNameCache.get(table)}'`
                  );
                  error.meta.push(
                    `db.update arguments:
${prettyPrint(key)}`
                  );
                  throw error;
                }
              }
              if (typeof values === "function") {
                return setCacheEntry(
                  table,
                  values(row),
                  entry?.type === 0 /* INSERT */ ? 0 /* INSERT */ : 1 /* UPDATE */,
                  row
                );
              } else {
                return setCacheEntry(
                  table,
                  values,
                  entry?.type === 0 /* INSERT */ ? 0 /* INSERT */ : 1 /* UPDATE */,
                  row
                );
              }
            }
          )
        )
      };
    },
    // @ts-ignore
    delete: (table, key) => queue.add(
      () => database.qb.user.wrap(
        { method: `${tableNameCache.get(table) ?? "unknown"}.delete()` },
        async () => {
          checkOnchainTable(table, "delete");
          const entry = getCacheEntry(table, key);
          deleteCacheEntry(table, key);
          if (entry?.row) {
            if (entry.type === 0 /* INSERT */) {
              return true;
            }
            await database.drizzle.delete(table).where(getWhereCondition(table, key));
            return true;
          } else {
            if (isDatabaseEmpty) {
              return false;
            }
            const deleteResult = await database.drizzle.delete(table).where(getWhereCondition(table, key)).returning();
            return deleteResult.length > 0;
          }
        }
      )
    ),
    // @ts-ignore
    sql: drizzle(
      async (_sql, params, method, typings) => {
        await database.createTriggers();
        await indexingStore.flush();
        await database.removeTriggers();
        const query2 = { sql: _sql, params, typings };
        const res = await database.qb.user.wrap({ method: "sql" }, async () => {
          try {
            return await database.drizzle._.session.prepareQuery(query2, void 0, void 0, method === "all").execute();
          } catch (e) {
            throw parseSqlError(e);
          }
        });
        return { rows: res.rows.map((row) => Object.values(row)) };
      },
      { schema, casing: "snake_case" }
    ),
    async flush() {
      await queue.add(async () => {
        let cacheSize = 0;
        for (const c of cache.values())
          cacheSize += c.size;
        const flushIndex = totalCacheOps - cacheSize * (1 - common.options.indexingCacheFlushRatio);
        const shouldDelete = cacheBytes > maxBytes;
        if (shouldDelete)
          isDatabaseEmpty = false;
        const promises = [];
        for (const [table, tableCache] of cache) {
          const batchSize = Math.round(
            common.options.databaseMaxQueryParameters / Object.keys(getTableColumns3(table)).length
          );
          const insertValues = [];
          const updateValues = [];
          for (const [key, entry] of tableCache) {
            if (entry.type === 0 /* INSERT */) {
              insertValues.push(entry.row);
            }
            if (entry.type === 1 /* UPDATE */) {
              updateValues.push(entry.row);
            }
            if (shouldDelete && entry.operationIndex < flushIndex) {
              tableCache.delete(key);
              cacheBytes -= entry.bytes;
            }
            entry.type = 2 /* FIND */;
          }
          if (insertValues.length > 0) {
            common.logger.debug({
              service: "indexing",
              msg: `Inserting ${insertValues.length} cached '${tableNameCache.get(table)}' rows into the database`
            });
            while (insertValues.length > 0) {
              const values = insertValues.splice(0, batchSize);
              promises.push(
                database.qb.user.wrap(
                  { method: `${tableNameCache.get(table)}.flush()` },
                  async () => {
                    await database.drizzle.insert(table).values(values).catch((_error) => {
                      const error = _error;
                      common.logger.error({
                        service: "indexing",
                        msg: "Internal error occurred while flushing cache. Please report this error here: https://github.com/ponder-sh/ponder/issues"
                      });
                      throw new FlushError(error.message);
                    });
                  }
                )
              );
            }
          }
          if (updateValues.length > 0) {
            common.logger.debug({
              service: "indexing",
              msg: `Updating ${updateValues.length} cached '${tableNameCache.get(table)}' rows in the database`
            });
            const primaryKeys = primaryKeysCache.get(table);
            const set = {};
            for (const [columnName, column] of Object.entries(
              getTableColumns3(table)
            )) {
              set[columnName] = sql3.raw(
                `excluded."${getColumnCasing(column, "snake_case")}"`
              );
            }
            while (updateValues.length > 0) {
              const values = updateValues.splice(0, batchSize);
              promises.push(
                database.qb.user.wrap(
                  {
                    method: `${tableNameCache.get(table)}.flush()`
                  },
                  async () => {
                    await database.drizzle.insert(table).values(values).onConflictDoUpdate({
                      // @ts-ignore
                      target: primaryKeys.map(({ js }) => table[js]),
                      set
                    }).catch((_error) => {
                      const error = _error;
                      common.logger.error({
                        service: "indexing",
                        msg: "Internal error occurred while flushing cache. Please report this error here: https://github.com/ponder-sh/ponder/issues"
                      });
                      throw new FlushError(error.message);
                    });
                  }
                )
              );
            }
          }
        }
        await Promise.all(promises);
      });
    },
    isCacheFull() {
      return cacheBytes > maxBytes;
    }
  };
  return indexingStore;
};

// src/indexing-store/metadata.ts
import { sql as sql4 } from "kysely";
var getLiveMetadataStore = ({
  db
}) => ({
  getStatus: async () => {
    return db.wrap({ method: "_ponder_meta.getStatus()" }, async () => {
      const metadata = await sql4.raw(`  
WITH live AS (
    SELECT value->>'instance_id' as instance_id FROM _ponder_meta WHERE key = 'live'
)
SELECT value 
FROM _ponder_meta 
WHERE key = 'status_' || (SELECT instance_id FROM live); 
        `).execute(db);
      if (!metadata.rows[0]?.value === void 0) {
        return null;
      }
      return metadata.rows[0].value;
    });
  }
});
var getMetadataStore = ({
  db,
  instanceId
}) => ({
  getStatus: async () => {
    return db.wrap({ method: "_ponder_meta.getStatus()" }, async () => {
      const metadata = await db.selectFrom("_ponder_meta").select("value").where("key", "=", `status_${instanceId}`).executeTakeFirst();
      if (metadata.value === null)
        return null;
      return metadata.value;
    });
  },
  setStatus: (status) => {
    return db.wrap({ method: "_ponder_meta.setStatus()" }, async () => {
      await db.insertInto("_ponder_meta").values({
        key: `status_${instanceId}`,
        value: status
      }).onConflict(
        (oc) => oc.column("key").doUpdateSet({
          value: status
        })
      ).execute();
    });
  }
});

// src/indexing-store/realtime.ts
import {
  and as and2,
  eq as eq2
} from "drizzle-orm";
import { getTableConfig as getTableConfig3 } from "drizzle-orm/pg-core";
import { drizzle as drizzle2 } from "drizzle-orm/pg-proxy";
var checkOnchainTable2 = (table, method) => {
  if (table === void 0)
    throw new UndefinedTableError(
      `Table object passed to db.${method}() is undefined`
    );
  if (onchain in table)
    return;
  throw new InvalidStoreMethodError(
    method === "find" ? `db.find() can only be used with onchain tables, and '${getTableConfig3(table).name}' is an offchain table.` : `Indexing functions can only write to onchain tables, and '${getTableConfig3(table).name}' is an offchain table.`
  );
};
var createRealtimeIndexingStore = ({
  database,
  schema
}) => {
  const queue = createQueue({
    browser: false,
    initialStart: true,
    concurrency: 1,
    worker: (fn) => {
      return fn();
    }
  });
  const tableNameCache = /* @__PURE__ */ new Map();
  const primaryKeysCache = /* @__PURE__ */ new Map();
  for (const tableName of getTableNames(schema, "")) {
    primaryKeysCache.set(
      schema[tableName.js],
      getPrimaryKeyColumns(schema[tableName.js])
    );
    tableNameCache.set(schema[tableName.js], tableName.user);
  }
  const getCacheKey = (table, row) => {
    const primaryKeys = primaryKeysCache.get(table);
    return primaryKeys.map((pk) => normalizeColumn(table[pk.js], row[pk.js])).join("_");
  };
  const getWhereCondition = (table, key) => {
    primaryKeysCache.get(table);
    const conditions = [];
    for (const { js } of primaryKeysCache.get(table)) {
      conditions.push(eq2(table[js], key[js]));
    }
    return and2(...conditions);
  };
  const find = (table, key) => {
    return database.drizzle.select().from(table).where(getWhereCondition(table, key)).then((res) => res.length === 0 ? null : res[0]);
  };
  const indexingStore = {
    // @ts-ignore
    find: (table, key) => queue.add(
      () => database.qb.user.wrap(
        { method: `${tableNameCache.get(table) ?? "unknown"}.find()` },
        async () => {
          checkOnchainTable2(table, "find");
          return find(table, key);
        }
      )
    ),
    // @ts-ignore
    insert(table) {
      return {
        values: (values) => {
          const inner = {
            onConflictDoNothing: () => queue.add(
              () => database.qb.user.wrap(
                {
                  method: `${tableNameCache.get(table) ?? "unknown"}.insert()`
                },
                async () => {
                  checkOnchainTable2(table, "insert");
                  const parseResult = (result) => {
                    if (Array.isArray(values) === false) {
                      return result.length === 1 ? result[0] : null;
                    }
                    const rows = [];
                    let resultIndex = 0;
                    for (let i = 0; i < values.length; i++) {
                      if (getCacheKey(table, values[i]) === getCacheKey(table, result[resultIndex])) {
                        rows.push(result[resultIndex++]);
                      } else {
                        rows.push(null);
                      }
                    }
                    return rows;
                  };
                  try {
                    return await database.drizzle.insert(table).values(values).onConflictDoNothing().returning().then(parseResult);
                  } catch (e) {
                    throw parseSqlError(e);
                  }
                }
              )
            ),
            onConflictDoUpdate: (valuesU) => queue.add(
              () => database.qb.user.wrap(
                {
                  method: `${tableNameCache.get(table) ?? "unknown"}.insert()`
                },
                async () => {
                  checkOnchainTable2(table, "insert");
                  if (typeof valuesU === "object") {
                    try {
                      return await database.drizzle.insert(table).values(values).onConflictDoUpdate({
                        target: primaryKeysCache.get(table).map(({ js }) => table[js]),
                        set: valuesU
                      }).returning().then(
                        (res) => Array.isArray(values) ? res : res[0]
                      );
                    } catch (e) {
                      throw parseSqlError(e);
                    }
                  }
                  if (Array.isArray(values)) {
                    const rows = [];
                    for (const value of values) {
                      const row = await find(table, value);
                      if (row === null) {
                        try {
                          rows.push(
                            await database.drizzle.insert(table).values(value).returning().then((res) => res[0])
                          );
                        } catch (e) {
                          throw parseSqlError(e);
                        }
                      } else {
                        try {
                          rows.push(
                            await database.drizzle.update(table).set(valuesU(row)).where(getWhereCondition(table, value)).returning().then((res) => res[0])
                          );
                        } catch (e) {
                          throw parseSqlError(e);
                        }
                      }
                    }
                    return rows;
                  } else {
                    const row = await find(table, values);
                    if (row === null) {
                      try {
                        return await database.drizzle.insert(table).values(values).returning().then((res) => res[0]);
                      } catch (e) {
                        throw parseSqlError(e);
                      }
                    } else {
                      try {
                        return await database.drizzle.update(table).set(valuesU(row)).where(getWhereCondition(table, values)).returning().then((res) => res[0]);
                      } catch (e) {
                        throw parseSqlError(e);
                      }
                    }
                  }
                }
              )
            ),
            // biome-ignore lint/suspicious/noThenProperty: <explanation>
            then: (onFulfilled, onRejected) => queue.add(
              () => database.qb.user.wrap(
                {
                  method: `${tableNameCache.get(table) ?? "unknown"}.insert()`
                },
                async () => {
                  checkOnchainTable2(table, "insert");
                  try {
                    return await database.drizzle.insert(table).values(values).returning().then(
                      (res) => Array.isArray(values) ? res : res[0]
                    );
                  } catch (e) {
                    throw parseSqlError(e);
                  }
                }
              )
            ).then(onFulfilled, onRejected),
            catch: (onRejected) => inner.then(void 0, onRejected),
            finally: (onFinally) => inner.then(
              (value) => {
                onFinally?.();
                return value;
              },
              (reason) => {
                onFinally?.();
                throw reason;
              }
            )
            // @ts-ignore
          };
          return inner;
        }
      };
    },
    // @ts-ignore
    update(table, key) {
      return {
        set: (values) => queue.add(
          () => database.qb.user.wrap(
            { method: `${tableNameCache.get(table) ?? "unknown"}.update()` },
            async () => {
              checkOnchainTable2(table, "update");
              if (typeof values === "function") {
                const row = await find(table, key);
                if (row === null) {
                  const error = new RecordNotFoundError(
                    `No existing record found in table '${tableNameCache.get(table)}'`
                  );
                  error.meta.push(
                    `db.update arguments:
${prettyPrint(key)}`
                  );
                  throw error;
                }
                try {
                  return await database.drizzle.update(table).set(values(row)).where(getWhereCondition(table, key)).returning().then((res) => res[0]);
                } catch (e) {
                  throw parseSqlError(e);
                }
              } else {
                try {
                  return await database.drizzle.update(table).set(values).where(getWhereCondition(table, key)).returning().then((res) => res[0]);
                } catch (e) {
                  throw parseSqlError(e);
                }
              }
            }
          )
        )
      };
    },
    // @ts-ignore
    delete: (table, key) => queue.add(
      () => database.qb.user.wrap(
        { method: `${tableNameCache.get(table) ?? "unknown"}.delete()` },
        async () => {
          checkOnchainTable2(table, "delete");
          const deleted = await database.drizzle.delete(table).where(getWhereCondition(table, key)).returning();
          return deleted.length > 0;
        }
      )
    ),
    // @ts-ignore
    sql: drizzle2(
      (_sql, params, method, typings) => (
        // @ts-ignore
        queue.add(async () => {
          const query2 = { sql: _sql, params, typings };
          const res = await database.qb.user.wrap(
            { method: "sql" },
            async () => {
              try {
                return await database.drizzle._.session.prepareQuery(query2, void 0, void 0, method === "all").execute();
              } catch (e) {
                throw parseSqlError(e);
              }
            }
          );
          return { rows: res.rows.map((row) => Object.values(row)) };
        })
      ),
      { schema, casing: "snake_case" }
    )
  };
  return indexingStore;
};

// src/sync/source.ts
var isAddressFactory = (address) => {
  if (address === void 0 || address === null || typeof address === "string")
    return false;
  return Array.isArray(address) ? isAddressFactory(address[0]) : true;
};
var getChildAddress = ({
  log,
  factory
}) => {
  if (factory.childAddressLocation.startsWith("offset")) {
    const childAddressOffset = Number(
      factory.childAddressLocation.substring(6)
    );
    const start3 = 2 + 12 * 2 + childAddressOffset * 2;
    const length = 20 * 2;
    return `0x${log.data.substring(start3, start3 + length)}`;
  } else {
    const start3 = 2 + 12 * 2;
    const length = 20 * 2;
    const topicIndex = factory.childAddressLocation === "topic1" ? 1 : factory.childAddressLocation === "topic2" ? 2 : 3;
    return `0x${log.topics[topicIndex].substring(start3, start3 + length)}`;
  }
};

// src/indexing/service.ts
import { checksumAddress, createClient } from "viem";

// src/indexing/addStackTrace.ts
import { readFileSync as readFileSync3 } from "node:fs";
import { codeFrameColumns as codeFrameColumns2 } from "@babel/code-frame";
import { parse as parseStackTrace2 } from "stacktrace-parser";
var addStackTrace = (error, options) => {
  if (!error.stack)
    return;
  const stackTrace = parseStackTrace2(error.stack);
  let codeFrame;
  let userStackTrace;
  const firstUserFrameIndex = stackTrace.findIndex(
    (frame) => frame.file?.includes(options.indexingDir)
  );
  if (firstUserFrameIndex >= 0) {
    userStackTrace = stackTrace.filter(
      (frame) => frame.file?.includes(options.indexingDir)
    );
    const firstUserFrame = stackTrace[firstUserFrameIndex];
    if (firstUserFrame?.file && firstUserFrame?.lineNumber) {
      try {
        const sourceContent = readFileSync3(firstUserFrame.file, {
          encoding: "utf-8"
        });
        codeFrame = codeFrameColumns2(
          sourceContent,
          {
            start: {
              line: firstUserFrame.lineNumber,
              column: firstUserFrame.column ?? void 0
            }
          },
          { highlightCode: true }
        );
      } catch (err) {
      }
    }
  } else {
    userStackTrace = stackTrace;
  }
  const formattedStackTrace = [
    `${error.name}: ${error.message}`,
    ...userStackTrace.map(({ file, lineNumber, column, methodName }) => {
      const prefix = "    at";
      const path9 = `${file}${lineNumber !== null ? `:${lineNumber}` : ""}${column !== null ? `:${column}` : ""}`;
      if (methodName === null || methodName === "<unknown>") {
        return `${prefix} ${path9}`;
      } else {
        return `${prefix} ${methodName} (${path9})`;
      }
    }),
    codeFrame
  ].join("\n");
  error.stack = formattedStackTrace;
};

// src/indexing/ponderActions.ts
import {
  getBalance as viemGetBalance,
  getCode as viemGetCode,
  getEnsName as viemGetEnsName,
  getStorageAt as viemGetStorageAt,
  multicall as viemMulticall,
  readContract as viemReadContract
} from "viem/actions";
var buildCachedActions = (contextState) => {
  return (client) => ({
    getBalance: ({
      cache,
      blockNumber: userBlockNumber,
      ...args
    }) => viemGetBalance(client, {
      ...args,
      ...cache === "immutable" ? { blockTag: "latest" } : { blockNumber: userBlockNumber ?? contextState.blockNumber }
    }),
    getCode: ({
      cache,
      blockNumber: userBlockNumber,
      ...args
    }) => viemGetCode(client, {
      ...args,
      ...cache === "immutable" ? { blockTag: "latest" } : { blockNumber: userBlockNumber ?? contextState.blockNumber }
    }),
    getStorageAt: ({
      cache,
      blockNumber: userBlockNumber,
      ...args
    }) => viemGetStorageAt(client, {
      ...args,
      ...cache === "immutable" ? { blockTag: "latest" } : { blockNumber: userBlockNumber ?? contextState.blockNumber }
    }),
    multicall: ({
      cache,
      blockNumber: userBlockNumber,
      ...args
    }) => viemMulticall(client, {
      ...args,
      ...cache === "immutable" ? { blockTag: "latest" } : { blockNumber: userBlockNumber ?? contextState.blockNumber }
    }),
    // @ts-ignore
    readContract: ({
      cache,
      blockNumber: userBlockNumber,
      ...args
    }) => viemReadContract(client, {
      ...args,
      ...cache === "immutable" ? { blockTag: "latest" } : { blockNumber: userBlockNumber ?? contextState.blockNumber }
    }),
    getEnsName: ({
      cache,
      blockNumber: userBlockNumber,
      ...args
    }) => viemGetEnsName(client, {
      ...args,
      ...cache === "immutable" ? { blockTag: "latest" } : { blockNumber: userBlockNumber ?? contextState.blockNumber }
    })
  });
};

// src/indexing/service.ts
var create2 = ({
  indexingFunctions,
  common,
  sources,
  networks,
  sync
}) => {
  const contextState = {
    blockNumber: void 0
  };
  const clientByChainId = {};
  const contractsByChainId = {};
  const networkByChainId = networks.reduce(
    (acc, cur) => {
      acc[cur.chainId] = cur;
      return acc;
    },
    {}
  );
  for (const source of sources) {
    if (source.type === "block")
      continue;
    let address;
    if (source.filter.type === "log") {
      const _address = source.filter.address;
      if (isAddressFactory(_address) === false && Array.isArray(_address) === false && _address !== void 0) {
        address = _address;
      }
    } else {
      const _address = source.filter.toAddress;
      if (isAddressFactory(_address) === false && _address !== void 0) {
        address = _address[0];
      }
    }
    if (contractsByChainId[source.filter.chainId] === void 0) {
      contractsByChainId[source.filter.chainId] = {};
    }
    if (contractsByChainId[source.filter.chainId][source.name] !== void 0)
      continue;
    contractsByChainId[source.filter.chainId][source.name] = {
      abi: source.abi,
      address: address ? checksumAddress(address) : address,
      startBlock: source.filter.fromBlock,
      endBlock: source.filter.toBlock
    };
  }
  const cachedActions = buildCachedActions(contextState);
  for (const network of networks) {
    const transport = sync.getCachedTransport(network);
    clientByChainId[network.chainId] = createClient({
      transport,
      chain: network.chain
      // @ts-ignore
    }).extend(cachedActions);
  }
  const eventCount = {};
  for (const eventName of Object.keys(indexingFunctions)) {
    eventCount[eventName] = 0;
  }
  return {
    common,
    indexingFunctions,
    isKilled: false,
    eventCount,
    startCheckpoint: decodeCheckpoint(sync.getStartCheckpoint()),
    currentEvent: {
      contextState,
      context: {
        network: { name: void 0, chainId: void 0 },
        contracts: void 0,
        client: void 0,
        db: void 0
      }
    },
    networkByChainId,
    clientByChainId,
    contractsByChainId
  };
};
var processSetupEvents = async (indexingService, {
  sources,
  networks
}) => {
  for (const eventName of Object.keys(indexingService.indexingFunctions)) {
    if (!eventName.endsWith(":setup"))
      continue;
    const [contractName] = eventName.split(":");
    for (const network of networks) {
      const source = sources.find(
        (s) => s.type === "contract" && s.name === contractName && s.filter.chainId === network.chainId
      );
      if (indexingService.isKilled)
        return { status: "killed" };
      indexingService.eventCount[eventName]++;
      const result = await executeSetup(indexingService, {
        event: {
          type: "setup",
          chainId: network.chainId,
          checkpoint: encodeCheckpoint({
            ...zeroCheckpoint,
            chainId: BigInt(network.chainId),
            blockNumber: BigInt(source.filter.fromBlock)
          }),
          name: eventName,
          block: BigInt(source.filter.fromBlock)
        }
      });
      if (result.status !== "success") {
        return result;
      }
    }
  }
  return { status: "success" };
};
var processEvents = async (indexingService, { events }) => {
  for (let i = 0; i < events.length; i++) {
    if (indexingService.isKilled)
      return { status: "killed" };
    const event = events[i];
    switch (event.type) {
      case "log": {
        indexingService.eventCount[event.name]++;
        indexingService.common.logger.trace({
          service: "indexing",
          msg: `Started indexing function (event="${event.name}", checkpoint=${event.checkpoint})`
        });
        const result = await executeLog(indexingService, { event });
        if (result.status !== "success") {
          return result;
        }
        indexingService.common.logger.trace({
          service: "indexing",
          msg: `Completed indexing function (event="${event.name}", checkpoint=${event.checkpoint})`
        });
        break;
      }
      case "block": {
        indexingService.eventCount[event.name]++;
        indexingService.common.logger.trace({
          service: "indexing",
          msg: `Started indexing function (event="${event.name}", checkpoint=${event.checkpoint})`
        });
        const result = await executeBlock(indexingService, { event });
        if (result.status !== "success") {
          return result;
        }
        indexingService.common.logger.trace({
          service: "indexing",
          msg: `Completed indexing function (event="${event.name}", checkpoint=${event.checkpoint})`
        });
        break;
      }
      case "callTrace": {
        indexingService.eventCount[event.name]++;
        indexingService.common.logger.trace({
          service: "indexing",
          msg: `Started indexing function (event="${event.name}", checkpoint=${event.checkpoint})`
        });
        const result = await executeCallTrace(indexingService, { event });
        if (result.status !== "success") {
          return result;
        }
        indexingService.common.logger.trace({
          service: "indexing",
          msg: `Completed indexing function (event="${event.name}", checkpoint=${event.checkpoint})`
        });
        break;
      }
      default:
        never(event);
    }
    if (i % 93 === 0) {
      updateCompletedEvents(indexingService);
      const eventTimestamp = decodeCheckpoint(event.checkpoint).blockTimestamp;
      indexingService.common.metrics.ponder_indexing_completed_seconds.set(
        eventTimestamp - indexingService.startCheckpoint.blockTimestamp
      );
      indexingService.common.metrics.ponder_indexing_completed_timestamp.set(
        eventTimestamp
      );
      await new Promise(setImmediate);
    }
  }
  if (events.length > 0) {
    const lastEventInBatchTimestamp = decodeCheckpoint(
      events[events.length - 1].checkpoint
    ).blockTimestamp;
    indexingService.common.metrics.ponder_indexing_completed_seconds.set(
      lastEventInBatchTimestamp - indexingService.startCheckpoint.blockTimestamp
    );
    indexingService.common.metrics.ponder_indexing_completed_timestamp.set(
      lastEventInBatchTimestamp
    );
  }
  updateCompletedEvents(indexingService);
  return { status: "success" };
};
var setIndexingStore = (indexingService, indexingStore) => {
  indexingService.currentEvent.context.db = {
    find: indexingStore.find,
    insert: indexingStore.insert,
    update: indexingStore.update,
    delete: indexingStore.delete,
    sql: indexingStore.sql
  };
};
var kill2 = (indexingService) => {
  indexingService.common.logger.debug({
    service: "indexing",
    msg: "Killed indexing service"
  });
  indexingService.isKilled = true;
};
var updateTotalSeconds = (indexingService, endCheckpoint) => {
  indexingService.common.metrics.ponder_indexing_total_seconds.set(
    endCheckpoint.blockTimestamp - indexingService.startCheckpoint.blockTimestamp
  );
};
var updateCompletedEvents = (indexingService) => {
  for (const event of Object.keys(indexingService.eventCount)) {
    const metricLabel = {
      event
    };
    indexingService.common.metrics.ponder_indexing_completed_events.set(
      metricLabel,
      indexingService.eventCount[event]
    );
  }
};
var executeSetup = async (indexingService, { event }) => {
  const {
    common,
    indexingFunctions,
    currentEvent,
    networkByChainId,
    contractsByChainId,
    clientByChainId
  } = indexingService;
  const indexingFunction = indexingFunctions[event.name];
  const metricLabel = { event: event.name };
  try {
    currentEvent.context.network.chainId = event.chainId;
    currentEvent.context.network.name = networkByChainId[event.chainId].name;
    currentEvent.context.client = clientByChainId[event.chainId];
    currentEvent.context.contracts = contractsByChainId[event.chainId];
    currentEvent.contextState.blockNumber = event.block;
    const endClock = startClock();
    await indexingFunction({
      context: currentEvent.context
    });
    common.metrics.ponder_indexing_function_duration.observe(
      metricLabel,
      endClock()
    );
  } catch (_error) {
    if (indexingService.isKilled)
      return { status: "killed" };
    const error = _error;
    const decodedCheckpoint = decodeCheckpoint(event.checkpoint);
    addStackTrace(error, common.options);
    common.metrics.ponder_indexing_has_error.set(1);
    common.logger.error({
      service: "indexing",
      msg: `Error while processing '${event.name}' event in '${networkByChainId[event.chainId].name}' block ${decodedCheckpoint.blockNumber}`,
      error
    });
    return { status: "error", error };
  }
  return { status: "success" };
};
var executeLog = async (indexingService, { event }) => {
  const {
    common,
    indexingFunctions,
    currentEvent,
    networkByChainId,
    contractsByChainId,
    clientByChainId
  } = indexingService;
  const indexingFunction = indexingFunctions[event.name];
  const metricLabel = { event: event.name };
  try {
    currentEvent.context.network.chainId = event.chainId;
    currentEvent.context.network.name = networkByChainId[event.chainId].name;
    currentEvent.context.client = clientByChainId[event.chainId];
    currentEvent.context.contracts = contractsByChainId[event.chainId];
    currentEvent.contextState.blockNumber = event.event.block.number;
    const endClock = startClock();
    await indexingFunction({
      event: event.event,
      context: currentEvent.context
    });
    common.metrics.ponder_indexing_function_duration.observe(
      metricLabel,
      endClock()
    );
  } catch (_error) {
    if (indexingService.isKilled)
      return { status: "killed" };
    const error = _error;
    const decodedCheckpoint = decodeCheckpoint(event.checkpoint);
    addStackTrace(error, common.options);
    error.meta = Array.isArray(error.meta) ? error.meta : [];
    if (error.meta.length === 0) {
      error.meta.push(`Event arguments:
${prettyPrint(event.event.args)}`);
    }
    common.logger.error({
      service: "indexing",
      msg: `Error while processing '${event.name}' event in '${networkByChainId[event.chainId].name}' block ${decodedCheckpoint.blockNumber}`,
      error
    });
    common.metrics.ponder_indexing_has_error.set(1);
    return { status: "error", error };
  }
  return { status: "success" };
};
var executeBlock = async (indexingService, { event }) => {
  const {
    common,
    indexingFunctions,
    currentEvent,
    networkByChainId,
    contractsByChainId,
    clientByChainId
  } = indexingService;
  const indexingFunction = indexingFunctions[event.name];
  const metricLabel = { event: event.name };
  try {
    currentEvent.context.network.chainId = event.chainId;
    currentEvent.context.network.name = networkByChainId[event.chainId].name;
    currentEvent.context.client = clientByChainId[event.chainId];
    currentEvent.context.contracts = contractsByChainId[event.chainId];
    currentEvent.contextState.blockNumber = event.event.block.number;
    const endClock = startClock();
    await indexingFunction({
      event: event.event,
      context: currentEvent.context
    });
    common.metrics.ponder_indexing_function_duration.observe(
      metricLabel,
      endClock()
    );
  } catch (_error) {
    if (indexingService.isKilled)
      return { status: "killed" };
    const error = _error;
    const decodedCheckpoint = decodeCheckpoint(event.checkpoint);
    addStackTrace(error, common.options);
    error.meta = Array.isArray(error.meta) ? error.meta : [];
    error.meta.push(
      `Block:
${prettyPrint({
        hash: event.event.block.hash,
        number: event.event.block.number,
        timestamp: event.event.block.timestamp
      })}`
    );
    common.logger.error({
      service: "indexing",
      msg: `Error while processing ${event.name} event at chainId=${decodedCheckpoint.chainId}, block=${decodedCheckpoint.blockNumber}`,
      error
    });
    common.metrics.ponder_indexing_has_error.set(1);
    return { status: "error", error };
  }
  return { status: "success" };
};
var executeCallTrace = async (indexingService, { event }) => {
  const {
    common,
    indexingFunctions,
    currentEvent,
    networkByChainId,
    contractsByChainId,
    clientByChainId
  } = indexingService;
  const indexingFunction = indexingFunctions[event.name];
  const metricLabel = { event: event.name };
  try {
    currentEvent.context.network.chainId = event.chainId;
    currentEvent.context.network.name = networkByChainId[event.chainId].name;
    currentEvent.context.client = clientByChainId[event.chainId];
    currentEvent.context.contracts = contractsByChainId[event.chainId];
    currentEvent.contextState.blockNumber = event.event.block.number;
    const endClock = startClock();
    await indexingFunction({
      event: event.event,
      context: currentEvent.context
    });
    common.metrics.ponder_indexing_function_duration.observe(
      metricLabel,
      endClock()
    );
  } catch (_error) {
    if (indexingService.isKilled)
      return { status: "killed" };
    const error = _error;
    const decodedCheckpoint = decodeCheckpoint(event.checkpoint);
    addStackTrace(error, common.options);
    error.meta = Array.isArray(error.meta) ? error.meta : [];
    error.meta.push(`Call trace arguments:
${prettyPrint(event.event.args)}`);
    common.logger.error({
      service: "indexing",
      msg: `Error while processing '${event.name}' event in '${networkByChainId[event.chainId].name}' block ${decodedCheckpoint.blockNumber}`,
      error
    });
    common.metrics.ponder_indexing_has_error.set(1);
    return { status: "error", error };
  }
  return { status: "success" };
};

// src/indexing/index.ts
var methods2 = {
  create: create2,
  kill: kill2,
  processEvents,
  processSetupEvents,
  updateTotalSeconds,
  setIndexingStore
};
var createIndexingService = extend(create2, methods2);

// src/sync/fragments.ts
var buildLogFilterFragments = ({
  chainId,
  address,
  topics,
  includeTransactionReceipts
}) => {
  const fragments = [];
  const { topic0, topic1, topic2, topic3 } = parseTopics(topics);
  const idCallback = ({
    chainId: chainId2,
    address: address_,
    topic0: topic0_,
    topic1: topic1_,
    topic2: topic2_,
    topic3: topic3_,
    includeTransactionReceipts: includeTransactionReceipts2
  }) => {
    return `${chainId2}_${address_}_${topic0_}_${topic1_}_${topic2_}_${topic3_}_${includeTransactionReceipts2}`;
  };
  const factoryIdCallback = ({
    chainId: chainId2,
    address: address_,
    eventSelector: eventSelector_,
    childAddressLocation: childAddressLocation_,
    topic0: topic0_,
    topic1: topic1_,
    topic2: topic2_,
    topic3: topic3_,
    includeTransactionReceipts: includeTransactionReceipts2
  }) => {
    return `${chainId2}_${address_}_${eventSelector_}_${childAddressLocation_}_${topic0_}_${topic1_}_${topic2_}_${topic3_}_${includeTransactionReceipts2}`;
  };
  if (isAddressFactory(address)) {
    for (const factoryAddress_ of Array.isArray(address.address) ? address.address : [address.address]) {
      for (const topic0_ of Array.isArray(topic0) ? topic0 : [topic0]) {
        for (const topic1_ of Array.isArray(topic1) ? topic1 : [topic1]) {
          for (const topic2_ of Array.isArray(topic2) ? topic2 : [topic2]) {
            for (const topic3_ of Array.isArray(topic3) ? topic3 : [topic3]) {
              fragments.push({
                id: factoryIdCallback({
                  chainId,
                  address: factoryAddress_,
                  eventSelector: address.eventSelector,
                  childAddressLocation: address.childAddressLocation,
                  topic0: topic0_,
                  topic1: topic1_,
                  topic2: topic2_,
                  topic3: topic3_,
                  includeTransactionReceipts: includeTransactionReceipts ? 1 : 0
                }),
                chainId,
                address: factoryAddress_,
                eventSelector: address.eventSelector,
                childAddressLocation: address.childAddressLocation,
                topic0: topic0_,
                topic1: topic1_,
                topic2: topic2_,
                topic3: topic3_,
                includeTransactionReceipts: includeTransactionReceipts ? 1 : 0
              });
            }
          }
        }
      }
    }
  } else {
    for (const address_ of Array.isArray(address) ? address : [address ?? null]) {
      for (const topic0_ of Array.isArray(topic0) ? topic0 : [topic0]) {
        for (const topic1_ of Array.isArray(topic1) ? topic1 : [topic1]) {
          for (const topic2_ of Array.isArray(topic2) ? topic2 : [topic2]) {
            for (const topic3_ of Array.isArray(topic3) ? topic3 : [topic3]) {
              fragments.push({
                id: idCallback({
                  chainId,
                  address: address_,
                  topic0: topic0_,
                  topic1: topic1_,
                  topic2: topic2_,
                  topic3: topic3_,
                  includeTransactionReceipts: includeTransactionReceipts ? 1 : 0
                }),
                chainId,
                address: address_,
                topic0: topic0_,
                topic1: topic1_,
                topic2: topic2_,
                topic3: topic3_,
                includeTransactionReceipts: includeTransactionReceipts ? 1 : 0
              });
            }
          }
        }
      }
    }
  }
  return fragments;
};
function parseTopics(topics) {
  return {
    topic0: topics?.[0] ?? null,
    topic1: topics?.[1] ?? null,
    topic2: topics?.[2] ?? null,
    topic3: topics?.[3] ?? null
  };
}
var buildBlockFilterFragment = ({
  chainId,
  interval,
  offset
}) => {
  return {
    id: `${chainId}_${interval}_${offset}`,
    chainId,
    interval,
    offset
  };
};
var buildTraceFilterFragments = ({
  chainId,
  fromAddress,
  toAddress
}) => {
  const fragments = [];
  const idCallback = ({
    chainId: chainId2,
    fromAddress: fromAddress2,
    toAddress: toAddress2
  }) => {
    return `${chainId2}_${fromAddress2}_${toAddress2}`;
  };
  const factoryIdCallback = ({
    chainId: chainId2,
    fromAddress: fromAddress2,
    address,
    eventSelector,
    childAddressLocation
  }) => {
    return `${chainId2}_${address}_${eventSelector}_${childAddressLocation}_${fromAddress2}`;
  };
  if (isAddressFactory(toAddress)) {
    for (const _fromAddress of Array.isArray(fromAddress) ? fromAddress : [null]) {
      for (const _factoryAddress of Array.isArray(toAddress.address) ? toAddress.address : [toAddress.address]) {
        fragments.push({
          id: factoryIdCallback({
            chainId,
            fromAddress: _fromAddress,
            address: _factoryAddress,
            eventSelector: toAddress.eventSelector,
            childAddressLocation: toAddress.childAddressLocation
          }),
          chainId,
          address: _factoryAddress,
          eventSelector: toAddress.eventSelector,
          childAddressLocation: toAddress.childAddressLocation,
          fromAddress: _fromAddress
        });
      }
    }
  } else {
    for (const _fromAddress of Array.isArray(fromAddress) ? fromAddress : [null]) {
      for (const _toAddress of Array.isArray(toAddress) ? toAddress : [null]) {
        fragments.push({
          id: idCallback({
            chainId,
            fromAddress: _fromAddress,
            toAddress: _toAddress
          }),
          chainId,
          toAddress: _toAddress,
          fromAddress: _fromAddress
        });
      }
    }
  }
  return fragments;
};

// src/utils/interval.ts
function intervalSum(intervals) {
  let totalSum = 0;
  for (const [start3, end] of intervals) {
    totalSum += end - start3 + 1;
  }
  return totalSum;
}
function intervalUnion(intervals_) {
  if (intervals_.length === 0)
    return [];
  const intervals = intervals_.map((interval) => [...interval]);
  intervals.sort((a, b) => a[0] - b[0]);
  const result = [];
  let currentInterval = intervals[0];
  for (let i = 1; i < intervals.length; i++) {
    const nextInterval = intervals[i];
    if (currentInterval[1] >= nextInterval[0] - 1) {
      currentInterval[1] = Math.max(currentInterval[1], nextInterval[1]);
    } else {
      result.push(currentInterval);
      currentInterval = nextInterval;
    }
  }
  result.push(currentInterval);
  return result;
}
function intervalIntersection(list1, list2) {
  const result = [];
  let i = 0;
  let j = 0;
  while (i < list1.length && j < list2.length) {
    const [start1, end1] = list1[i];
    const [start22, end2] = list2[j];
    const intersectionStart = Math.max(start1, start22);
    const intersectionEnd = Math.min(end1, end2);
    if (intersectionStart <= intersectionEnd) {
      result.push([intersectionStart, intersectionEnd]);
    }
    if (end1 < end2) {
      i++;
    } else {
      j++;
    }
  }
  return intervalUnion(result);
}
function intervalIntersectionMany(lists) {
  if (lists.length === 0)
    return [];
  if (lists.length === 1)
    return lists[0];
  let result = lists[0];
  for (let i = 1; i < lists.length; i++) {
    result = intervalIntersection(result, lists[i]);
  }
  return intervalUnion(result);
}
function intervalDifference(initial, remove) {
  const initial_ = initial.map((interval) => [...interval]);
  const remove_ = remove.map((interval) => [...interval]);
  const result = [];
  let i = 0;
  let j = 0;
  while (i < initial.length && j < remove.length) {
    const interval1 = initial_[i];
    const interval2 = remove_[j];
    if (interval1[1] < interval2[0]) {
      result.push(interval1);
      i++;
    } else if (interval2[1] < interval1[0]) {
      j++;
    } else {
      if (interval1[0] < interval2[0]) {
        result.push([interval1[0], interval2[0] - 1]);
      }
      if (interval1[1] > interval2[1]) {
        interval1[0] = interval2[1] + 1;
        j++;
      } else {
        i++;
      }
    }
  }
  while (i < initial_.length) {
    result.push(initial_[i]);
    i++;
  }
  return result;
}
function sortIntervals(intervals) {
  return intervals.sort((a, b) => a[0] < b[0] ? -1 : 1);
}
function getChunks({
  interval,
  maxChunkSize
}) {
  const _chunks = [];
  const [startBlock, endBlock] = interval;
  let fromBlock = startBlock;
  let toBlock = Math.min(fromBlock + maxChunkSize - 1, endBlock);
  while (fromBlock <= endBlock) {
    _chunks.push([fromBlock, toBlock]);
    fromBlock = toBlock + 1;
    toBlock = Math.min(fromBlock + maxChunkSize - 1, endBlock);
  }
  return _chunks;
}

// src/sync-store/index.ts
import {
  sql as ksql
} from "kysely";
import {
  checksumAddress as checksumAddress2,
  hexToBigInt as hexToBigInt2,
  hexToNumber as hexToNumber2
} from "viem";

// src/sync-store/encoding.ts
import { hexToBigInt, hexToNumber } from "viem";
var encodeBlock = ({
  block,
  chainId
}) => {
  return {
    hash: block.hash,
    chainId,
    checkpoint: encodeCheckpoint({
      blockTimestamp: hexToNumber(block.timestamp),
      chainId: BigInt(chainId),
      blockNumber: hexToBigInt(block.number),
      transactionIndex: maxCheckpoint.transactionIndex,
      eventType: EVENT_TYPES.blocks,
      eventIndex: zeroCheckpoint.eventIndex
    }),
    baseFeePerGas: block.baseFeePerGas ? hexToBigInt(block.baseFeePerGas) : null,
    difficulty: hexToBigInt(block.difficulty),
    number: hexToBigInt(block.number),
    timestamp: hexToBigInt(block.timestamp),
    extraData: block.extraData,
    gasLimit: hexToBigInt(block.gasLimit),
    gasUsed: hexToBigInt(block.gasUsed),
    logsBloom: block.logsBloom,
    miner: toLowerCase(block.miner),
    mixHash: block.mixHash ?? null,
    nonce: block.nonce ?? null,
    parentHash: block.parentHash,
    receiptsRoot: block.receiptsRoot,
    sha3Uncles: block.sha3Uncles ?? null,
    size: hexToBigInt(block.size),
    stateRoot: block.stateRoot,
    totalDifficulty: block.totalDifficulty ? hexToBigInt(block.totalDifficulty) : null,
    transactionsRoot: block.transactionsRoot
  };
};
var encodeLog = ({
  log,
  block,
  chainId
}) => {
  return {
    id: `${log.blockHash}-${log.logIndex}`,
    chainId,
    checkpoint: block === void 0 ? null : encodeCheckpoint({
      blockTimestamp: hexToNumber(block.timestamp),
      chainId: BigInt(chainId),
      blockNumber: hexToBigInt(log.blockNumber),
      transactionIndex: hexToBigInt(log.transactionIndex),
      eventType: EVENT_TYPES.logs,
      eventIndex: hexToBigInt(log.logIndex)
    }),
    blockHash: log.blockHash,
    blockNumber: hexToBigInt(log.blockNumber),
    logIndex: hexToNumber(log.logIndex),
    transactionHash: log.transactionHash,
    transactionIndex: hexToNumber(log.transactionIndex),
    address: toLowerCase(log.address),
    topic0: log.topics[0] ? log.topics[0] : null,
    topic1: log.topics[1] ? log.topics[1] : null,
    topic2: log.topics[2] ? log.topics[2] : null,
    topic3: log.topics[3] ? log.topics[3] : null,
    data: log.data
  };
};
var encodeTransaction = ({
  transaction,
  chainId
}) => {
  return {
    hash: transaction.hash,
    chainId,
    blockHash: transaction.blockHash,
    blockNumber: hexToBigInt(transaction.blockNumber),
    accessList: transaction.accessList ? JSON.stringify(transaction.accessList) : void 0,
    from: toLowerCase(transaction.from),
    gas: hexToBigInt(transaction.gas),
    gasPrice: transaction.gasPrice ? hexToBigInt(transaction.gasPrice) : null,
    input: transaction.input,
    maxFeePerGas: transaction.maxFeePerGas ? hexToBigInt(transaction.maxFeePerGas) : null,
    maxPriorityFeePerGas: transaction.maxPriorityFeePerGas ? hexToBigInt(transaction.maxPriorityFeePerGas) : null,
    nonce: hexToNumber(transaction.nonce),
    r: transaction.r ?? null,
    s: transaction.s ?? null,
    to: transaction.to ? toLowerCase(transaction.to) : null,
    transactionIndex: hexToNumber(transaction.transactionIndex),
    type: transaction.type ?? "0x0",
    value: hexToBigInt(transaction.value),
    v: transaction.v ? hexToBigInt(transaction.v) : null
  };
};
var encodeTransactionReceipt = ({
  transactionReceipt,
  chainId
}) => {
  return {
    transactionHash: transactionReceipt.transactionHash,
    chainId,
    blockHash: transactionReceipt.blockHash,
    blockNumber: hexToBigInt(transactionReceipt.blockNumber),
    contractAddress: transactionReceipt.contractAddress ? toLowerCase(transactionReceipt.contractAddress) : null,
    cumulativeGasUsed: hexToBigInt(transactionReceipt.cumulativeGasUsed),
    effectiveGasPrice: hexToBigInt(transactionReceipt.effectiveGasPrice),
    from: toLowerCase(transactionReceipt.from),
    gasUsed: hexToBigInt(transactionReceipt.gasUsed),
    logs: JSON.stringify(transactionReceipt.logs),
    logsBloom: transactionReceipt.logsBloom,
    status: transactionReceipt.status,
    to: transactionReceipt.to ? toLowerCase(transactionReceipt.to) : null,
    transactionIndex: hexToNumber(transactionReceipt.transactionIndex),
    type: transactionReceipt.type
  };
};
function encodeCallTrace({
  trace,
  chainId
}) {
  return {
    id: `${trace.transactionHash}-${JSON.stringify(trace.traceAddress)}`,
    chainId,
    callType: trace.action.callType,
    from: toLowerCase(trace.action.from),
    gas: hexToBigInt(trace.action.gas),
    input: trace.action.input,
    to: toLowerCase(trace.action.to),
    value: hexToBigInt(trace.action.value),
    blockHash: trace.blockHash,
    blockNumber: hexToBigInt(trace.blockNumber),
    error: trace.error ?? null,
    gasUsed: trace.result ? hexToBigInt(trace.result.gasUsed) : null,
    output: trace.result ? trace.result.output : null,
    subtraces: trace.subtraces,
    traceAddress: JSON.stringify(trace.traceAddress),
    transactionHash: trace.transactionHash,
    transactionPosition: trace.transactionPosition,
    functionSelector: trace.action.input.slice(0, 10).toLowerCase()
  };
}

// src/sync-store/index.ts
var logFactorySQL = (qb, factory) => qb.select(
  (() => {
    if (factory.childAddressLocation.startsWith("offset")) {
      const childAddressOffset = Number(
        factory.childAddressLocation.substring(6)
      );
      const start3 = 2 + 12 * 2 + childAddressOffset * 2 + 1;
      const length = 20 * 2;
      return ksql`'0x' || substring(data from ${start3}::int for ${length}::int)`;
    } else {
      const start3 = 2 + 12 * 2 + 1;
      const length = 20 * 2;
      return ksql`'0x' || substring(${ksql.ref(
        factory.childAddressLocation
      )} from ${start3}::integer for ${length}::integer)`;
    }
  })().as("childAddress")
).$call((qb2) => {
  if (Array.isArray(factory.address)) {
    return qb2.where("address", "in", factory.address);
  }
  return qb2.where("address", "=", factory.address);
}).where("topic0", "=", factory.eventSelector).where("chainId", "=", factory.chainId);
var createSyncStore = ({
  common,
  db
}) => ({
  insertInterval: async ({ filter, interval }) => db.wrap({ method: "insertInterval" }, async () => {
    const startBlock = BigInt(interval[0]);
    const endBlock = BigInt(interval[1]);
    switch (filter.type) {
      case "log": {
        for (const fragment of buildLogFilterFragments(filter)) {
          if (isAddressFactory(filter.address)) {
            await db.insertInto("factoryLogFilterIntervals").values({
              factoryId: fragment.id,
              startBlock,
              endBlock
            }).execute();
          } else {
            await db.insertInto("logFilterIntervals").values({
              logFilterId: fragment.id,
              startBlock,
              endBlock
            }).execute();
          }
        }
        break;
      }
      case "block": {
        const fragment = buildBlockFilterFragment(filter);
        await db.insertInto("blockFilterIntervals").values({
          blockFilterId: fragment.id,
          startBlock,
          endBlock
        }).execute();
        break;
      }
      case "callTrace": {
        for (const fragment of buildTraceFilterFragments(filter)) {
          if (isAddressFactory(filter.toAddress)) {
            await db.insertInto("factoryTraceFilterIntervals").values({
              factoryId: fragment.id,
              startBlock,
              endBlock
            }).execute();
          } else {
            await db.insertInto("traceFilterIntervals").values({
              traceFilterId: fragment.id,
              startBlock,
              endBlock
            }).execute();
          }
        }
        break;
      }
      default:
        never(filter);
    }
  }),
  getIntervals: async ({ filter }) => db.wrap({ method: "getIntervals" }, async () => {
    const topicSQL = (qb, fragment) => qb.where(
      (eb) => eb.or([
        eb("topic0", "is", null),
        eb("topic0", "=", fragment.topic0)
      ])
    ).where(
      (eb) => eb.or([
        eb("topic1", "is", null),
        eb("topic1", "=", fragment.topic1)
      ])
    ).where(
      (eb) => eb.or([
        eb("topic2", "is", null),
        eb("topic2", "=", fragment.topic2)
      ])
    ).where(
      (eb) => eb.or([
        eb("topic3", "is", null),
        eb("topic3", "=", fragment.topic3)
      ])
    );
    let fragments;
    let table;
    let idCol;
    let fragmentSelect;
    switch (filter.type) {
      case "log":
        {
          if (isAddressFactory(filter.address)) {
            fragments = buildLogFilterFragments(filter);
            table = "factoryLogFilter";
            idCol = "factoryId";
            fragmentSelect = (fragment, qb) => qb.where("address", "=", fragment.address).where("eventSelector", "=", fragment.eventSelector).where(
              "childAddressLocation",
              "=",
              fragment.childAddressLocation
            ).where(
              "includeTransactionReceipts",
              ">=",
              fragment.includeTransactionReceipts
            ).$call((qb2) => topicSQL(qb2, fragment));
          } else {
            fragments = buildLogFilterFragments(filter);
            table = "logFilter";
            idCol = "logFilterId";
            fragmentSelect = (fragment, qb) => qb.where(
              (eb) => eb.or([
                eb("address", "is", null),
                eb("address", "=", fragment.address)
              ])
            ).where(
              "includeTransactionReceipts",
              ">=",
              fragment.includeTransactionReceipts
            ).$call((qb2) => topicSQL(qb2, fragment));
          }
        }
        break;
      case "block":
        {
          fragments = [buildBlockFilterFragment(filter)];
          table = "blockFilter";
          idCol = "blockFilterId";
          fragmentSelect = (fragment, qb) => qb.where("blockFilterId", "=", fragment.id);
        }
        break;
      case "callTrace":
        {
          if (isAddressFactory(filter.toAddress)) {
            fragments = buildTraceFilterFragments(filter);
            table = "factoryTraceFilter";
            idCol = "factoryId";
            fragmentSelect = (fragment, qb) => qb.where("address", "=", fragment.address).where("eventSelector", "=", fragment.eventSelector).where(
              "childAddressLocation",
              "=",
              fragment.childAddressLocation
            ).where(
              (eb) => eb.or([
                eb("fromAddress", "is", null),
                eb("fromAddress", "=", fragment.fromAddress)
              ])
            );
          } else {
            fragments = buildTraceFilterFragments(filter);
            table = "traceFilter";
            idCol = "traceFilterId";
            fragmentSelect = (fragment, qb) => qb.where(
              (eb) => eb.or([
                eb("fromAddress", "is", null),
                eb("fromAddress", "=", fragment.fromAddress)
              ])
            ).where(
              (eb) => eb.or([
                eb("toAddress", "is", null),
                eb("toAddress", "=", fragment.toAddress)
              ])
            );
          }
        }
        break;
      default:
        never(filter);
    }
    for (const fragment of fragments) {
      await db.insertInto(`${table}s`).values(fragment).onConflict((oc) => oc.column("id").doNothing()).execute();
      let mergeComplete = false;
      while (mergeComplete === false) {
        await db.transaction().execute(async (tx) => {
          const existingIntervals = await tx.deleteFrom(`${table}Intervals`).where(
            "id",
            "in",
            tx.selectFrom(`${table}Intervals`).where(idCol, "=", fragment.id).select("id").orderBy("startBlock asc").limit(common.options.syncStoreMaxIntervals)
          ).returning(["startBlock", "endBlock"]).execute();
          const mergedIntervals = intervalUnion(
            existingIntervals.map((i) => [
              Number(i.startBlock),
              Number(i.endBlock)
            ])
          );
          const mergedIntervalRows = mergedIntervals.map(
            ([startBlock, endBlock]) => ({
              [idCol]: fragment.id,
              startBlock: BigInt(startBlock),
              endBlock: BigInt(endBlock)
            })
          );
          if (mergedIntervalRows.length > 0) {
            await tx.insertInto(`${table}Intervals`).values(mergedIntervalRows).execute();
          }
          if (mergedIntervalRows.length === common.options.syncStoreMaxIntervals) {
            throw new NonRetryableError(
              `'${table}Intervals' table for chain '${fragment.chainId}' has reached an unrecoverable level of fragmentation.`
            );
          }
          if (existingIntervals.length !== common.options.syncStoreMaxIntervals) {
            mergeComplete = true;
          }
        });
      }
    }
    const intervals = [];
    for (const fragment of fragments) {
      const _intervals = await db.selectFrom(`${table}Intervals`).innerJoin(`${table}s`, idCol, `${table}s.id`).$call((qb) => fragmentSelect(fragment, qb)).where("chainId", "=", fragment.chainId).select(["startBlock", "endBlock"]).execute();
      const union = intervalUnion(
        _intervals.map(({ startBlock, endBlock }) => [
          Number(startBlock),
          Number(endBlock)
        ])
      );
      intervals.push(union);
    }
    return intervalIntersectionMany(intervals);
  }),
  getChildAddresses: ({ filter, limit }) => db.wrap({ method: "getChildAddresses" }, async () => {
    return await db.selectFrom("logs").$call((qb) => logFactorySQL(qb, filter)).orderBy("id asc").$if(limit !== void 0, (qb) => qb.limit(limit)).execute().then((addresses) => addresses.map(({ childAddress }) => childAddress));
  }),
  filterChildAddresses: ({ filter, addresses }) => db.wrap({ method: "filterChildAddresses" }, async () => {
    const result = await db.with(
      "addresses(address)",
      () => ksql`( values ${ksql.join(addresses.map((a) => ksql`( ${ksql.val(a)} )`))} )`
    ).with(
      "childAddresses",
      (db2) => db2.selectFrom("logs").$call((qb) => logFactorySQL(qb, filter))
    ).selectFrom("addresses").where(
      "addresses.address",
      "in",
      ksql`(SELECT "childAddress" FROM "childAddresses")`
    ).selectAll().execute();
    return /* @__PURE__ */ new Set([...result.map(({ address }) => address)]);
  }),
  insertLogs: async ({ logs, shouldUpdateCheckpoint, chainId }) => {
    if (logs.length === 0)
      return;
    await db.wrap({ method: "insertLogs" }, async () => {
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters / Object.keys(encodeLog({ log: logs[0].log, chainId })).length
      );
      for (let i = 0; i < logs.length; i += batchSize) {
        await db.insertInto("logs").values(
          logs.slice(i, i + batchSize).map(({ log, block }) => encodeLog({ log, block, chainId }))
        ).onConflict(
          (oc) => oc.column("id").$call(
            (qb) => shouldUpdateCheckpoint ? qb.doUpdateSet((eb) => ({
              checkpoint: eb.ref("excluded.checkpoint")
            })) : qb.doNothing()
          )
        ).execute();
      }
    });
  },
  insertBlocks: async ({ blocks, chainId }) => {
    if (blocks.length === 0)
      return;
    await db.wrap({ method: "insertBlocks" }, async () => {
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters / Object.keys(encodeBlock({ block: blocks[0], chainId })).length
      );
      for (let i = 0; i < blocks.length; i += batchSize) {
        await db.insertInto("blocks").values(
          blocks.slice(i, i + batchSize).map((block) => encodeBlock({ block, chainId }))
        ).onConflict((oc) => oc.column("hash").doNothing()).execute();
      }
    });
  },
  hasBlock: async ({ hash }) => db.wrap({ method: "hasBlock" }, async () => {
    return await db.selectFrom("blocks").select("hash").where("hash", "=", hash).executeTakeFirst().then((result) => result !== void 0);
  }),
  insertTransactions: async ({ transactions, chainId }) => {
    if (transactions.length === 0)
      return;
    await db.wrap({ method: "insertTransactions" }, async () => {
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters / Object.keys(
          encodeTransaction({
            transaction: transactions[0],
            chainId
          })
        ).length
      );
      for (let i = 0; i < transactions.length; i += batchSize) {
        await db.insertInto("transactions").values(
          transactions.slice(i, i + batchSize).map(
            (transaction) => encodeTransaction({ transaction, chainId })
          )
        ).onConflict((oc) => oc.column("hash").doNothing()).execute();
      }
    });
  },
  hasTransaction: async ({ hash }) => db.wrap({ method: "hasTransaction" }, async () => {
    return await db.selectFrom("transactions").select("hash").where("hash", "=", hash).executeTakeFirst().then((result) => result !== void 0);
  }),
  insertTransactionReceipts: async ({ transactionReceipts, chainId }) => {
    if (transactionReceipts.length === 0)
      return;
    await db.wrap({ method: "insertTransactionReceipts" }, async () => {
      const batchSize = Math.floor(
        common.options.databaseMaxQueryParameters / Object.keys(
          encodeTransactionReceipt({
            transactionReceipt: transactionReceipts[0],
            chainId
          })
        ).length
      );
      for (let i = 0; i < transactionReceipts.length; i += batchSize) {
        await db.insertInto("transactionReceipts").values(
          transactionReceipts.slice(i, i + batchSize).map(
            (transactionReceipt) => encodeTransactionReceipt({
              transactionReceipt,
              chainId
            })
          )
        ).onConflict((oc) => oc.column("transactionHash").doNothing()).execute();
      }
    });
  },
  hasTransactionReceipt: async ({ hash }) => db.wrap({ method: "hasTransactionReceipt" }, async () => {
    return await db.selectFrom("transactionReceipts").select("transactionHash").where("transactionHash", "=", hash).executeTakeFirst().then((result) => result !== void 0);
  }),
  insertCallTraces: async ({ callTraces, chainId }) => {
    if (callTraces.length === 0)
      return;
    await db.wrap({ method: "insertCallTrace" }, async () => {
      const traceByTransactionHash = {};
      for (const { callTrace, block } of callTraces) {
        if (traceByTransactionHash[callTrace.transactionHash] === void 0) {
          traceByTransactionHash[callTrace.transactionHash] = {
            traces: [],
            block
          };
        }
        traceByTransactionHash[callTrace.transactionHash].traces.push(
          callTrace
        );
      }
      const values = [];
      await db.transaction().execute(async (tx) => {
        for (const transactionHash of Object.keys(traceByTransactionHash)) {
          const block = traceByTransactionHash[transactionHash].block;
          const traces = await tx.deleteFrom("callTraces").returningAll().where("transactionHash", "=", transactionHash).where("chainId", "=", chainId).execute();
          traces.push(
            ...traceByTransactionHash[transactionHash].traces.map(
              (trace) => encodeCallTrace({ trace, chainId })
            )
          );
          traces.sort((a, b) => {
            return a.traceAddress < b.traceAddress ? -1 : 1;
          });
          for (let i = 0; i < traces.length; i++) {
            const trace = traces[i];
            const checkpoint = encodeCheckpoint({
              blockTimestamp: hexToNumber2(block.timestamp),
              chainId: BigInt(chainId),
              blockNumber: hexToBigInt2(block.number),
              transactionIndex: BigInt(trace.transactionPosition),
              eventType: EVENT_TYPES.callTraces,
              eventIndex: BigInt(i)
            });
            trace.checkpoint = checkpoint;
            values.push(trace);
          }
        }
        const batchSize = Math.floor(
          common.options.databaseMaxQueryParameters / Object.keys(values[0]).length
        );
        for (let i = 0; i < values.length; i += batchSize) {
          await tx.insertInto("callTraces").values(values.slice(i, i + batchSize)).onConflict((oc) => oc.column("id").doNothing()).execute();
        }
      });
    });
  },
  getEvents: async ({ filters, from, to, limit }) => {
    const addressSQL = (qb, address, column) => {
      if (typeof address === "string")
        return qb.where(column, "=", address);
      if (isAddressFactory(address)) {
        return qb.where(
          column,
          "in",
          db.selectFrom("logs").$call((qb2) => logFactorySQL(qb2, address))
        );
      }
      if (Array.isArray(address))
        return qb.where(column, "in", address);
      return qb;
    };
    const logSQL = (filter, db2, index) => db2.selectFrom("logs").select([
      ksql.raw(`'${index}'`).as("filterIndex"),
      "checkpoint",
      "chainId",
      "blockHash",
      "transactionHash",
      "id as logId",
      ksql`null`.as("callTraceId")
    ]).where("chainId", "=", filter.chainId).$if(filter.topics !== void 0, (qb) => {
      for (const idx_ of [0, 1, 2, 3]) {
        const idx = idx_;
        const raw = filter.topics[idx] ?? null;
        if (raw === null)
          continue;
        const topic = Array.isArray(raw) && raw.length === 1 ? raw[0] : raw;
        if (Array.isArray(topic)) {
          qb = qb.where(
            (eb) => eb.or(topic.map((t) => eb(`logs.topic${idx}`, "=", t)))
          );
        } else {
          qb = qb.where(`logs.topic${idx}`, "=", topic);
        }
      }
      return qb;
    }).$call((qb) => addressSQL(qb, filter.address, "address")).where("blockNumber", ">=", filter.fromBlock.toString()).$if(
      filter.toBlock !== void 0,
      (qb) => qb.where("blockNumber", "<=", filter.toBlock.toString())
    );
    const callTraceSQL = (filter, db2, index) => db2.selectFrom("callTraces").select([
      ksql.raw(`'${index}'`).as("filterIndex"),
      "checkpoint",
      "chainId",
      "blockHash",
      "transactionHash",
      ksql`null`.as("logId"),
      "id as callTraceId"
    ]).where("chainId", "=", filter.chainId).where(
      (eb) => eb.or(
        filter.functionSelectors.map(
          (fs2) => eb("callTraces.functionSelector", "=", fs2)
        )
      )
    ).where(ksql`${ksql.ref("callTraces.error")} IS NULL`).$call((qb) => addressSQL(qb, filter.fromAddress, "from")).$call((qb) => addressSQL(qb, filter.toAddress, "to")).where("blockNumber", ">=", filter.fromBlock.toString()).$if(
      filter.toBlock !== void 0,
      (qb) => qb.where("blockNumber", "<=", filter.toBlock.toString())
    );
    const blockSQL = (filter, db2, index) => db2.selectFrom("blocks").select([
      ksql.raw(`'${index}'`).as("filterIndex"),
      "checkpoint",
      "chainId",
      "hash as blockHash",
      ksql`null`.as("transactionHash"),
      ksql`null`.as("logId"),
      ksql`null`.as("callTraceId")
    ]).where("chainId", "=", filter.chainId).$if(
      filter !== void 0 && filter.interval !== void 0,
      (qb) => qb.where(ksql`(number - ${filter.offset}) % ${filter.interval} = 0`)
    ).where("number", ">=", filter.fromBlock.toString()).$if(
      filter.toBlock !== void 0,
      (qb) => qb.where("number", "<=", filter.toBlock.toString())
    );
    const rows = await db.wrap(
      {
        method: "getEvents",
        shouldRetry(error) {
          return error.message.includes("statement timeout") === false;
        }
      },
      async () => {
        let query2;
        for (let i = 0; i < filters.length; i++) {
          const filter = filters[i];
          const _query = filter.type === "log" ? logSQL(filter, db, i) : filter.type === "callTrace" ? callTraceSQL(filter, db, i) : blockSQL(filter, db, i);
          query2 = query2 === void 0 ? _query : query2.unionAll(_query);
        }
        return await db.with("event", () => query2).selectFrom("event").select([
          "event.filterIndex as event_filterIndex",
          "event.checkpoint as event_checkpoint"
        ]).innerJoin("blocks", "blocks.hash", "event.blockHash").select([
          "blocks.baseFeePerGas as block_baseFeePerGas",
          "blocks.difficulty as block_difficulty",
          "blocks.extraData as block_extraData",
          "blocks.gasLimit as block_gasLimit",
          "blocks.gasUsed as block_gasUsed",
          "blocks.hash as block_hash",
          "blocks.logsBloom as block_logsBloom",
          "blocks.miner as block_miner",
          "blocks.mixHash as block_mixHash",
          "blocks.nonce as block_nonce",
          "blocks.number as block_number",
          "blocks.parentHash as block_parentHash",
          "blocks.receiptsRoot as block_receiptsRoot",
          "blocks.sha3Uncles as block_sha3Uncles",
          "blocks.size as block_size",
          "blocks.stateRoot as block_stateRoot",
          "blocks.timestamp as block_timestamp",
          "blocks.totalDifficulty as block_totalDifficulty",
          "blocks.transactionsRoot as block_transactionsRoot"
        ]).leftJoin("logs", "logs.id", "event.logId").select([
          "logs.address as log_address",
          "logs.blockHash as log_blockHash",
          "logs.blockNumber as log_blockNumber",
          "logs.chainId as log_chainId",
          "logs.data as log_data",
          "logs.id as log_id",
          "logs.logIndex as log_logIndex",
          "logs.topic0 as log_topic0",
          "logs.topic1 as log_topic1",
          "logs.topic2 as log_topic2",
          "logs.topic3 as log_topic3",
          "logs.transactionHash as log_transactionHash",
          "logs.transactionIndex as log_transactionIndex"
        ]).leftJoin(
          "transactions",
          "transactions.hash",
          "event.transactionHash"
        ).select([
          "transactions.accessList as tx_accessList",
          "transactions.blockHash as tx_blockHash",
          "transactions.blockNumber as tx_blockNumber",
          "transactions.from as tx_from",
          "transactions.gas as tx_gas",
          "transactions.gasPrice as tx_gasPrice",
          "transactions.hash as tx_hash",
          "transactions.input as tx_input",
          "transactions.maxFeePerGas as tx_maxFeePerGas",
          "transactions.maxPriorityFeePerGas as tx_maxPriorityFeePerGas",
          "transactions.nonce as tx_nonce",
          "transactions.r as tx_r",
          "transactions.s as tx_s",
          "transactions.to as tx_to",
          "transactions.transactionIndex as tx_transactionIndex",
          "transactions.type as tx_type",
          "transactions.value as tx_value",
          "transactions.v as tx_v"
        ]).leftJoin("callTraces", "callTraces.id", "event.callTraceId").select([
          "callTraces.id as callTrace_id",
          "callTraces.callType as callTrace_callType",
          "callTraces.from as callTrace_from",
          "callTraces.gas as callTrace_gas",
          "callTraces.input as callTrace_input",
          "callTraces.to as callTrace_to",
          "callTraces.value as callTrace_value",
          "callTraces.blockHash as callTrace_blockHash",
          "callTraces.blockNumber as callTrace_blockNumber",
          "callTraces.gasUsed as callTrace_gasUsed",
          "callTraces.output as callTrace_output",
          "callTraces.subtraces as callTrace_subtraces",
          "callTraces.traceAddress as callTrace_traceAddress",
          "callTraces.transactionHash as callTrace_transactionHash",
          "callTraces.transactionPosition as callTrace_transactionPosition"
        ]).leftJoin(
          "transactionReceipts",
          "transactionReceipts.transactionHash",
          "event.transactionHash"
        ).select([
          "transactionReceipts.blockHash as txr_blockHash",
          "transactionReceipts.blockNumber as txr_blockNumber",
          "transactionReceipts.contractAddress as txr_contractAddress",
          "transactionReceipts.cumulativeGasUsed as txr_cumulativeGasUsed",
          "transactionReceipts.effectiveGasPrice as txr_effectiveGasPrice",
          "transactionReceipts.from as txr_from",
          "transactionReceipts.gasUsed as txr_gasUsed",
          "transactionReceipts.logs as txr_logs",
          "transactionReceipts.logsBloom as txr_logsBloom",
          "transactionReceipts.status as txr_status",
          "transactionReceipts.to as txr_to",
          "transactionReceipts.transactionHash as txr_transactionHash",
          "transactionReceipts.transactionIndex as txr_transactionIndex",
          "transactionReceipts.type as txr_type"
        ]).where("event.checkpoint", ">", from).where("event.checkpoint", "<=", to).orderBy("event.checkpoint", "asc").orderBy("event.filterIndex", "asc").limit(limit).execute();
      }
    );
    const events = rows.map((_row) => {
      const row = _row;
      const filter = filters[row.event_filterIndex];
      const hasLog = row.log_id !== null;
      const hasTransaction = row.tx_hash !== null;
      const hasCallTrace = row.callTrace_id !== null;
      const hasTransactionReceipt = (filter.type === "log" || filter.type === "callTrace") && filter.includeTransactionReceipts;
      return {
        chainId: filter.chainId,
        sourceIndex: Number(row.event_filterIndex),
        checkpoint: row.event_checkpoint,
        block: {
          baseFeePerGas: row.block_baseFeePerGas !== null ? BigInt(row.block_baseFeePerGas) : null,
          difficulty: BigInt(row.block_difficulty),
          extraData: row.block_extraData,
          gasLimit: BigInt(row.block_gasLimit),
          gasUsed: BigInt(row.block_gasUsed),
          hash: row.block_hash,
          logsBloom: row.block_logsBloom,
          miner: checksumAddress2(row.block_miner),
          mixHash: row.block_mixHash,
          nonce: row.block_nonce,
          number: BigInt(row.block_number),
          parentHash: row.block_parentHash,
          receiptsRoot: row.block_receiptsRoot,
          sha3Uncles: row.block_sha3Uncles,
          size: BigInt(row.block_size),
          stateRoot: row.block_stateRoot,
          timestamp: BigInt(row.block_timestamp),
          totalDifficulty: row.block_totalDifficulty !== null ? BigInt(row.block_totalDifficulty) : null,
          transactionsRoot: row.block_transactionsRoot
        },
        log: hasLog ? {
          address: checksumAddress2(row.log_address),
          blockHash: row.log_blockHash,
          blockNumber: BigInt(row.log_blockNumber),
          data: row.log_data,
          id: row.log_id,
          logIndex: Number(row.log_logIndex),
          removed: false,
          topics: [
            row.log_topic0,
            row.log_topic1,
            row.log_topic2,
            row.log_topic3
          ].filter((t) => t !== null),
          transactionHash: row.log_transactionHash,
          transactionIndex: Number(row.log_transactionIndex)
        } : void 0,
        transaction: hasTransaction ? {
          blockHash: row.tx_blockHash,
          blockNumber: BigInt(row.tx_blockNumber),
          from: checksumAddress2(row.tx_from),
          gas: BigInt(row.tx_gas),
          hash: row.tx_hash,
          input: row.tx_input,
          nonce: Number(row.tx_nonce),
          r: row.tx_r,
          s: row.tx_s,
          to: row.tx_to ? checksumAddress2(row.tx_to) : row.tx_to,
          transactionIndex: Number(row.tx_transactionIndex),
          value: BigInt(row.tx_value),
          v: row.tx_v !== null ? BigInt(row.tx_v) : null,
          ...row.tx_type === "0x0" ? {
            type: "legacy",
            gasPrice: BigInt(row.tx_gasPrice)
          } : row.tx_type === "0x1" ? {
            type: "eip2930",
            gasPrice: BigInt(row.tx_gasPrice),
            accessList: JSON.parse(row.tx_accessList)
          } : row.tx_type === "0x2" ? {
            type: "eip1559",
            maxFeePerGas: BigInt(row.tx_maxFeePerGas),
            maxPriorityFeePerGas: BigInt(
              row.tx_maxPriorityFeePerGas
            )
          } : row.tx_type === "0x7e" ? {
            type: "deposit",
            maxFeePerGas: row.tx_maxFeePerGas !== null ? BigInt(row.tx_maxFeePerGas) : void 0,
            maxPriorityFeePerGas: row.tx_maxPriorityFeePerGas !== null ? BigInt(row.tx_maxPriorityFeePerGas) : void 0
          } : {
            type: row.tx_type
          }
        } : void 0,
        trace: hasCallTrace ? {
          id: row.callTrace_id,
          from: checksumAddress2(row.callTrace_from),
          to: checksumAddress2(row.callTrace_to),
          gas: BigInt(row.callTrace_gas),
          value: BigInt(row.callTrace_value),
          input: row.callTrace_input,
          output: row.callTrace_output,
          gasUsed: BigInt(row.callTrace_gasUsed),
          subtraces: row.callTrace_subtraces,
          traceAddress: JSON.parse(row.callTrace_traceAddress),
          blockHash: row.callTrace_blockHash,
          blockNumber: BigInt(row.callTrace_blockNumber),
          transactionHash: row.callTrace_transactionHash,
          transactionIndex: row.callTrace_transactionPosition,
          callType: row.callTrace_callType
        } : void 0,
        transactionReceipt: hasTransactionReceipt ? {
          blockHash: row.txr_blockHash,
          blockNumber: BigInt(row.txr_blockNumber),
          contractAddress: row.txr_contractAddress ? checksumAddress2(row.txr_contractAddress) : null,
          cumulativeGasUsed: BigInt(row.txr_cumulativeGasUsed),
          effectiveGasPrice: BigInt(row.txr_effectiveGasPrice),
          from: checksumAddress2(row.txr_from),
          gasUsed: BigInt(row.txr_gasUsed),
          logs: JSON.parse(row.txr_logs).map((log) => ({
            id: `${log.blockHash}-${log.logIndex}`,
            address: checksumAddress2(log.address),
            blockHash: log.blockHash,
            blockNumber: hexToBigInt2(log.blockNumber),
            data: log.data,
            logIndex: hexToNumber2(log.logIndex),
            removed: false,
            topics: [
              log.topics[0] ?? null,
              log.topics[1] ?? null,
              log.topics[2] ?? null,
              log.topics[3] ?? null
            ].filter((t) => t !== null),
            transactionHash: log.transactionHash,
            transactionIndex: hexToNumber2(log.transactionIndex)
          })),
          logsBloom: row.txr_logsBloom,
          status: row.txr_status === "0x1" ? "success" : row.txr_status === "0x0" ? "reverted" : row.txr_status,
          to: row.txr_to ? checksumAddress2(row.txr_to) : null,
          transactionHash: row.txr_transactionHash,
          transactionIndex: Number(row.txr_transactionIndex),
          type: row.txr_type === "0x0" ? "legacy" : row.txr_type === "0x1" ? "eip2930" : row.tx_type === "0x2" ? "eip1559" : row.tx_type === "0x7e" ? "deposit" : row.tx_type
        } : void 0
      };
    });
    let cursor;
    if (events.length !== limit) {
      cursor = to;
    } else {
      cursor = events[events.length - 1].checkpoint;
    }
    return { events, cursor };
  },
  insertRpcRequestResult: async ({ request, blockNumber, chainId, result }) => db.wrap({ method: "insertRpcRequestResult" }, async () => {
    await db.insertInto("rpcRequestResults").values({
      request,
      blockNumber,
      chainId,
      result
    }).onConflict(
      (oc) => oc.columns(["request", "chainId", "blockNumber"]).doUpdateSet({ result })
    ).execute();
  }),
  getRpcRequestResult: async ({ request, blockNumber, chainId }) => db.wrap({ method: "getRpcRequestResult" }, async () => {
    const result = await db.selectFrom("rpcRequestResults").select("result").where("request", "=", request).where("chainId", "=", chainId).where("blockNumber", "=", blockNumber.toString()).executeTakeFirst();
    return result?.result ?? null;
  }),
  pruneRpcRequestResult: async ({ blocks, chainId }) => db.wrap({ method: "pruneRpcRequestResult" }, async () => {
    if (blocks.length === 0)
      return;
    const numbers = blocks.map(
      ({ number }) => hexToBigInt2(number).toString()
    );
    await db.deleteFrom("rpcRequestResults").where("chainId", "=", chainId).where("blockNumber", "in", numbers).execute();
  }),
  pruneByChain: async ({ fromBlock, chainId }) => db.wrap(
    { method: "pruneByChain" },
    () => db.transaction().execute(async (tx) => {
      await tx.with(
        "deleteLogFilter(logFilterId)",
        (qb) => qb.selectFrom("logFilterIntervals").innerJoin("logFilters", "logFilterId", "logFilters.id").select("logFilterId").where("chainId", "=", chainId).where("startBlock", ">=", fromBlock.toString())
      ).deleteFrom("logFilterIntervals").where(
        "logFilterId",
        "in",
        ksql`(SELECT "logFilterId" FROM ${ksql.table("deleteLogFilter")})`
      ).execute();
      await tx.with(
        "updateLogFilter(logFilterId)",
        (qb) => qb.selectFrom("logFilterIntervals").innerJoin("logFilters", "logFilterId", "logFilters.id").select("logFilterId").where("chainId", "=", chainId).where("startBlock", "<", fromBlock.toString()).where("endBlock", ">", fromBlock.toString())
      ).updateTable("logFilterIntervals").set({
        endBlock: fromBlock.toString()
      }).where(
        "logFilterId",
        "in",
        ksql`(SELECT "logFilterId" FROM ${ksql.table("updateLogFilter")})`
      ).execute();
      await tx.with(
        "deleteFactoryLogFilter(factoryId)",
        (qb) => qb.selectFrom("factoryLogFilterIntervals").innerJoin(
          "factoryLogFilters",
          "factoryId",
          "factoryLogFilters.id"
        ).select("factoryId").where("chainId", "=", chainId).where("startBlock", ">=", fromBlock.toString())
      ).deleteFrom("factoryLogFilterIntervals").where(
        "factoryId",
        "in",
        ksql`(SELECT "factoryId" FROM ${ksql.table("deleteFactoryLogFilter")})`
      ).execute();
      await tx.with(
        "updateFactoryLogFilter(factoryId)",
        (qb) => qb.selectFrom("factoryLogFilterIntervals").innerJoin(
          "factoryLogFilters",
          "factoryId",
          "factoryLogFilters.id"
        ).select("factoryId").where("chainId", "=", chainId).where("startBlock", "<", fromBlock.toString()).where("endBlock", ">", fromBlock.toString())
      ).updateTable("factoryLogFilterIntervals").set({
        endBlock: BigInt(fromBlock)
      }).where(
        "factoryId",
        "in",
        ksql`(SELECT "factoryId" FROM ${ksql.table("updateFactoryLogFilter")})`
      ).execute();
      await tx.with(
        "deleteTraceFilter(traceFilterId)",
        (qb) => qb.selectFrom("traceFilterIntervals").innerJoin("traceFilters", "traceFilterId", "traceFilters.id").select("traceFilterId").where("chainId", "=", chainId).where("startBlock", ">=", fromBlock.toString())
      ).deleteFrom("traceFilterIntervals").where(
        "traceFilterId",
        "in",
        ksql`(SELECT "traceFilterId" FROM ${ksql.table("deleteTraceFilter")})`
      ).execute();
      await tx.with(
        "updateTraceFilter(traceFilterId)",
        (qb) => qb.selectFrom("traceFilterIntervals").innerJoin("traceFilters", "traceFilterId", "traceFilters.id").select("traceFilterId").where("chainId", "=", chainId).where("startBlock", "<", fromBlock.toString()).where("endBlock", ">", fromBlock.toString())
      ).updateTable("traceFilterIntervals").set({
        endBlock: BigInt(fromBlock)
      }).where(
        "traceFilterId",
        "in",
        ksql`(SELECT "traceFilterId" FROM ${ksql.table("updateTraceFilter")})`
      ).execute();
      await tx.with(
        "deleteFactoryTraceFilter(factoryId)",
        (qb) => qb.selectFrom("factoryTraceFilterIntervals").innerJoin(
          "factoryTraceFilters",
          "factoryId",
          "factoryTraceFilters.id"
        ).select("factoryId").where("chainId", "=", chainId).where("startBlock", ">=", fromBlock.toString())
      ).deleteFrom("factoryTraceFilterIntervals").where(
        "factoryId",
        "in",
        ksql`(SELECT "factoryId" FROM ${ksql.table("deleteFactoryTraceFilter")})`
      ).execute();
      await tx.with(
        "updateFactoryTraceFilter(factoryId)",
        (qb) => qb.selectFrom("factoryTraceFilterIntervals").innerJoin(
          "factoryTraceFilters",
          "factoryId",
          "factoryTraceFilters.id"
        ).select("factoryId").where("chainId", "=", chainId).where("startBlock", "<", fromBlock.toString()).where("endBlock", ">", fromBlock.toString())
      ).updateTable("factoryTraceFilterIntervals").set({
        endBlock: BigInt(fromBlock)
      }).where(
        "factoryId",
        "in",
        ksql`(SELECT "factoryId" FROM ${ksql.table("updateFactoryTraceFilter")})`
      ).execute();
      await tx.with(
        "deleteBlockFilter(blockFilterId)",
        (qb) => qb.selectFrom("blockFilterIntervals").innerJoin("blockFilters", "blockFilterId", "blockFilters.id").select("blockFilterId").where("chainId", "=", chainId).where("startBlock", ">=", fromBlock.toString())
      ).deleteFrom("blockFilterIntervals").where(
        "blockFilterId",
        "in",
        ksql`(SELECT "blockFilterId" FROM ${ksql.table("deleteBlockFilter")})`
      ).execute();
      await tx.with(
        "updateBlockFilter(blockFilterId)",
        (qb) => qb.selectFrom("blockFilterIntervals").innerJoin("blockFilters", "blockFilterId", "blockFilters.id").select("blockFilterId").where("chainId", "=", chainId).where("startBlock", "<", fromBlock.toString()).where("endBlock", ">", fromBlock.toString())
      ).updateTable("blockFilterIntervals").set({
        endBlock: BigInt(fromBlock)
      }).where(
        "blockFilterId",
        "in",
        ksql`(SELECT "blockFilterId" FROM ${ksql.table("updateBlockFilter")})`
      ).execute();
      await tx.deleteFrom("logs").where("chainId", "=", chainId).where("blockNumber", ">=", fromBlock.toString()).execute();
      await tx.deleteFrom("blocks").where("chainId", "=", chainId).where("number", ">=", fromBlock.toString()).execute();
      await tx.deleteFrom("rpcRequestResults").where("chainId", "=", chainId).where("blockNumber", ">=", fromBlock.toString()).execute();
      await tx.deleteFrom("callTraces").where("chainId", "=", chainId).where("blockNumber", ">=", fromBlock.toString()).execute();
      await tx.deleteFrom("transactions").where("chainId", "=", chainId).where("blockNumber", ">=", fromBlock.toString()).execute();
      await tx.deleteFrom("transactionReceipts").where("chainId", "=", chainId).where("blockNumber", ">=", fromBlock.toString()).execute();
    })
  )
});

// src/sync-realtime/filter.ts
import { hexToNumber as hexToNumber3 } from "viem";
var isLogFactoryMatched = ({
  filter,
  log
}) => {
  const addresses = Array.isArray(filter.address) ? filter.address : [filter.address];
  if (addresses.every((address) => address !== toLowerCase(log.address))) {
    return false;
  }
  if (log.topics.length === 0)
    return false;
  if (filter.eventSelector !== toLowerCase(log.topics[0]))
    return false;
  return true;
};
var isLogFilterMatched = ({
  filter,
  block,
  log
}) => {
  if (hexToNumber3(block.number) < filter.fromBlock || hexToNumber3(block.number) > (filter.toBlock ?? Number.POSITIVE_INFINITY)) {
    return false;
  }
  return buildLogFilterFragments(filter).some((fragment) => {
    if (fragment.topic0 !== null && fragment.topic0 !== log.topics[0]?.toLowerCase())
      return false;
    if (fragment.topic1 !== null && fragment.topic1 !== log.topics[1]?.toLowerCase())
      return false;
    if (fragment.topic2 !== null && fragment.topic2 !== log.topics[2]?.toLowerCase())
      return false;
    if (fragment.topic3 !== null && fragment.topic3 !== log.topics[3]?.toLowerCase())
      return false;
    if (isAddressFactory(filter.address) === false && fragment.address !== null && fragment.address !== log.address.toLowerCase())
      return false;
    return true;
  });
};
var isCallTraceFilterMatched = ({
  filter,
  block,
  callTrace
}) => {
  if (hexToNumber3(block.number) < filter.fromBlock || hexToNumber3(block.number) > (filter.toBlock ?? Number.POSITIVE_INFINITY)) {
    return false;
  }
  return buildTraceFilterFragments(filter).some((fragment) => {
    if (fragment.fromAddress !== null && fragment.fromAddress !== callTrace.action.from.toLowerCase()) {
      return false;
    }
    if (isAddressFactory(filter.toAddress) === false && fragment.toAddress !== null && fragment.toAddress !== callTrace.action.to.toLowerCase()) {
      return false;
    }
    return true;
  });
};
var isBlockFilterMatched = ({
  filter,
  block
}) => {
  if (hexToNumber3(block.number) < filter.fromBlock || hexToNumber3(block.number) > (filter.toBlock ?? Number.POSITIVE_INFINITY)) {
    return false;
  }
  return (hexToNumber3(block.number) - filter.offset) % filter.interval === 0;
};

// src/sync/events.ts
import {
  DecodeLogDataMismatch,
  DecodeLogTopicsMismatch,
  checksumAddress as checksumAddress3,
  decodeAbiParameters,
  decodeFunctionData,
  decodeFunctionResult,
  hexToBigInt as hexToBigInt3,
  hexToNumber as hexToNumber4
} from "viem";
var buildEvents = ({
  sources,
  blockWithEventData: {
    block,
    logs,
    transactions,
    transactionReceipts,
    callTraces
  },
  finalizedChildAddresses,
  unfinalizedChildAddresses,
  chainId
}) => {
  const events = [];
  const transactionCache = /* @__PURE__ */ new Map();
  const transactionReceiptCache = /* @__PURE__ */ new Map();
  const traceByTransactionHash = /* @__PURE__ */ new Map();
  for (const transaction of transactions) {
    transactionCache.set(transaction.hash, transaction);
  }
  for (const transactionReceipt of transactionReceipts) {
    transactionReceiptCache.set(
      transactionReceipt.transactionHash,
      transactionReceipt
    );
  }
  for (const callTrace of callTraces) {
    if (traceByTransactionHash.has(callTrace.transactionHash) === false) {
      traceByTransactionHash.set(callTrace.transactionHash, []);
    }
    traceByTransactionHash.get(callTrace.transactionHash).push(callTrace);
  }
  for (let i = 0; i < sources.length; i++) {
    const filter = sources[i].filter;
    if (chainId !== filter.chainId)
      continue;
    switch (filter.type) {
      case "log": {
        for (const log of logs) {
          if (isLogFilterMatched({ filter, block, log }) && (isAddressFactory(filter.address) ? finalizedChildAddresses.get(filter.address).has(log.address) || unfinalizedChildAddresses.get(filter.address).has(log.address) : true)) {
            events.push({
              chainId: filter.chainId,
              sourceIndex: i,
              checkpoint: encodeCheckpoint({
                blockTimestamp: hexToNumber4(block.timestamp),
                chainId: BigInt(filter.chainId),
                blockNumber: hexToBigInt3(log.blockNumber),
                transactionIndex: hexToBigInt3(log.transactionIndex),
                eventType: EVENT_TYPES.logs,
                eventIndex: hexToBigInt3(log.logIndex)
              }),
              log: convertLog(log),
              block: convertBlock(block),
              transaction: convertTransaction(
                transactionCache.get(log.transactionHash)
              ),
              transactionReceipt: filter.includeTransactionReceipts ? convertTransactionReceipt(
                transactionReceiptCache.get(log.transactionHash)
              ) : void 0,
              trace: void 0
            });
          }
        }
        break;
      }
      case "block": {
        if (isBlockFilterMatched({ filter, block })) {
          events.push({
            chainId: filter.chainId,
            sourceIndex: i,
            checkpoint: encodeCheckpoint({
              blockTimestamp: hexToNumber4(block.timestamp),
              chainId: BigInt(filter.chainId),
              blockNumber: hexToBigInt3(block.number),
              transactionIndex: maxCheckpoint.transactionIndex,
              eventType: EVENT_TYPES.blocks,
              eventIndex: zeroCheckpoint.eventIndex
            }),
            block: convertBlock(block),
            log: void 0,
            trace: void 0,
            transaction: void 0,
            transactionReceipt: void 0
          });
        }
        break;
      }
      case "callTrace": {
        for (const callTraces2 of Array.from(traceByTransactionHash.values())) {
          callTraces2.sort((a, b) => {
            return a.traceAddress < b.traceAddress ? -1 : 1;
          });
          let eventIndex = 0n;
          for (const callTrace of callTraces2) {
            if (isCallTraceFilterMatched({ filter, block, callTrace }) && (isAddressFactory(filter.toAddress) ? finalizedChildAddresses.get(filter.toAddress).has(callTrace.action.to) || unfinalizedChildAddresses.get(filter.toAddress).has(callTrace.action.to) : true) && callTrace.result !== null && filter.functionSelectors.includes(
              callTrace.action.input.slice(0, 10).toLowerCase()
            )) {
              events.push({
                chainId: filter.chainId,
                sourceIndex: i,
                checkpoint: encodeCheckpoint({
                  blockTimestamp: hexToNumber4(block.timestamp),
                  chainId: BigInt(filter.chainId),
                  blockNumber: hexToBigInt3(callTrace.blockNumber),
                  transactionIndex: BigInt(callTrace.transactionPosition),
                  eventType: EVENT_TYPES.callTraces,
                  eventIndex: eventIndex++
                }),
                log: void 0,
                trace: convertCallTrace(callTrace),
                block: convertBlock(block),
                transaction: convertTransaction(
                  transactionCache.get(callTrace.transactionHash)
                ),
                transactionReceipt: filter.includeTransactionReceipts ? convertTransactionReceipt(
                  transactionReceiptCache.get(callTrace.transactionHash)
                ) : void 0
              });
            }
          }
        }
        break;
      }
      default:
        never(filter);
    }
  }
  return events.sort((a, b) => a.checkpoint < b.checkpoint ? -1 : 1);
};
var decodeEvents = (common, sources, rawEvents) => {
  const events = [];
  const endClock = startClock();
  for (const event of rawEvents) {
    const source = sources[event.sourceIndex];
    switch (source.type) {
      case "block": {
        events.push({
          type: "block",
          chainId: event.chainId,
          checkpoint: event.checkpoint,
          name: `${source.name}:block`,
          event: {
            block: event.block
          }
        });
        break;
      }
      case "contract": {
        switch (source.filter.type) {
          case "log": {
            try {
              if (event.log.topics[0] === void 0 || source.abiEvents.bySelector[event.log.topics[0]] === void 0) {
                throw new Error();
              }
              const { safeName, item } = source.abiEvents.bySelector[event.log.topics[0]];
              const args = decodeEventLog({
                abiItem: item,
                data: event.log.data,
                topics: event.log.topics
              });
              events.push({
                type: "log",
                chainId: event.chainId,
                checkpoint: event.checkpoint,
                name: `${source.name}:${safeName}`,
                event: {
                  name: safeName,
                  args,
                  log: event.log,
                  block: event.block,
                  transaction: event.transaction,
                  transactionReceipt: event.transactionReceipt
                }
              });
            } catch (err) {
              if (source.filter.address === void 0) {
                common.logger.debug({
                  service: "app",
                  msg: `Unable to decode log, skipping it. id: ${event.log?.id}, data: ${event.log?.data}, topics: ${event.log?.topics}`
                });
              } else {
                common.logger.warn({
                  service: "app",
                  msg: `Unable to decode log, skipping it. id: ${event.log?.id}, data: ${event.log?.data}, topics: ${event.log?.topics}`
                });
              }
            }
            break;
          }
          case "callTrace": {
            try {
              const selector = event.trace.input.slice(0, 10).toLowerCase();
              if (source.abiFunctions.bySelector[selector] === void 0) {
                throw new Error();
              }
              const { safeName, item } = source.abiFunctions.bySelector[selector];
              const { args, functionName } = decodeFunctionData({
                abi: [item],
                data: event.trace.input
              });
              const result = decodeFunctionResult({
                abi: [item],
                data: event.trace.output,
                functionName
              });
              events.push({
                type: "callTrace",
                chainId: event.chainId,
                checkpoint: event.checkpoint,
                name: `${source.name}.${safeName}`,
                event: {
                  args,
                  result,
                  trace: event.trace,
                  block: event.block,
                  transaction: event.transaction,
                  transactionReceipt: event.transactionReceipt
                }
              });
            } catch (err) {
              if (source.filter.toAddress === void 0) {
                common.logger.debug({
                  service: "app",
                  msg: `Unable to decode trace, skipping it. id: ${event.trace?.id}, input: ${event.trace?.input}, output: ${event.trace?.output}`
                });
              } else {
                common.logger.warn({
                  service: "app",
                  msg: `Unable to decode trace, skipping it. id: ${event.trace?.id}, input: ${event.trace?.input}, output: ${event.trace?.output}`
                });
              }
            }
            break;
          }
          default:
            never(source.filter);
        }
        break;
      }
      default:
        never(source);
    }
  }
  common.metrics.ponder_indexing_abi_decoding_duration.observe(endClock());
  return events;
};
function decodeEventLog({
  abiItem,
  topics,
  data
}) {
  const { inputs } = abiItem;
  const isUnnamed = inputs?.some((x) => !("name" in x && x.name));
  let args = isUnnamed ? [] : {};
  const [, ...argTopics] = topics;
  const indexedInputs = inputs.filter((x) => "indexed" in x && x.indexed);
  for (let i = 0; i < indexedInputs.length; i++) {
    const param = indexedInputs[i];
    const topic = argTopics[i];
    if (!topic)
      throw new DecodeLogTopicsMismatch({
        abiItem,
        param
      });
    args[isUnnamed ? i : param.name || i] = decodeTopic({
      param,
      value: topic
    });
  }
  const nonIndexedInputs = inputs.filter((x) => !("indexed" in x && x.indexed));
  if (nonIndexedInputs.length > 0) {
    if (data && data !== "0x") {
      const decodedData = decodeAbiParameters(nonIndexedInputs, data);
      if (decodedData) {
        if (isUnnamed)
          args = [...args, ...decodedData];
        else {
          for (let i = 0; i < nonIndexedInputs.length; i++) {
            args[nonIndexedInputs[i].name] = decodedData[i];
          }
        }
      }
    } else {
      throw new DecodeLogDataMismatch({
        abiItem,
        data: "0x",
        params: nonIndexedInputs,
        size: 0
      });
    }
  }
  return Object.values(args).length > 0 ? args : void 0;
}
function decodeTopic({ param, value }) {
  if (param.type === "string" || param.type === "bytes" || param.type === "tuple" || param.type.match(/^(.*)\[(\d+)?\]$/))
    return value;
  const decodedArg = decodeAbiParameters([param], value) || [];
  return decodedArg[0];
}
var convertBlock = (block) => ({
  baseFeePerGas: block.baseFeePerGas ? hexToBigInt3(block.baseFeePerGas) : null,
  difficulty: hexToBigInt3(block.difficulty),
  extraData: block.extraData,
  gasLimit: hexToBigInt3(block.gasLimit),
  gasUsed: hexToBigInt3(block.gasUsed),
  hash: block.hash,
  logsBloom: block.logsBloom,
  miner: checksumAddress3(block.miner),
  mixHash: block.mixHash,
  nonce: block.nonce,
  number: hexToBigInt3(block.number),
  parentHash: block.parentHash,
  receiptsRoot: block.receiptsRoot,
  sha3Uncles: block.sha3Uncles,
  size: hexToBigInt3(block.size),
  stateRoot: block.stateRoot,
  timestamp: hexToBigInt3(block.timestamp),
  totalDifficulty: block.totalDifficulty ? hexToBigInt3(block.totalDifficulty) : null,
  transactionsRoot: block.transactionsRoot
});
var convertLog = (log) => ({
  id: `${log.blockHash}-${log.logIndex}`,
  address: checksumAddress3(log.address),
  blockHash: log.blockHash,
  blockNumber: hexToBigInt3(log.blockNumber),
  data: log.data,
  logIndex: Number(log.logIndex),
  removed: false,
  topics: log.topics,
  transactionHash: log.transactionHash,
  transactionIndex: Number(log.transactionIndex)
});
var convertTransaction = (transaction) => ({
  blockHash: transaction.blockHash,
  blockNumber: hexToBigInt3(transaction.blockNumber),
  from: checksumAddress3(transaction.from),
  gas: hexToBigInt3(transaction.gas),
  hash: transaction.hash,
  input: transaction.input,
  nonce: Number(transaction.nonce),
  r: transaction.r,
  s: transaction.s,
  to: transaction.to ? checksumAddress3(transaction.to) : transaction.to,
  transactionIndex: Number(transaction.transactionIndex),
  value: hexToBigInt3(transaction.value),
  v: transaction.v ? hexToBigInt3(transaction.v) : null,
  ...transaction.type === "0x0" ? {
    type: "legacy",
    gasPrice: hexToBigInt3(transaction.gasPrice)
  } : transaction.type === "0x1" ? {
    type: "eip2930",
    gasPrice: hexToBigInt3(transaction.gasPrice),
    accessList: transaction.accessList
  } : transaction.type === "0x2" ? {
    type: "eip1559",
    maxFeePerGas: hexToBigInt3(transaction.maxFeePerGas),
    maxPriorityFeePerGas: hexToBigInt3(transaction.maxPriorityFeePerGas)
  } : (
    // @ts-ignore
    transaction.type === "0x7e" ? {
      type: "deposit",
      // @ts-ignore
      maxFeePerGas: transaction.maxFeePerGas ? (
        // @ts-ignore
        hexToBigInt3(transaction.maxFeePerGas)
      ) : void 0,
      // @ts-ignore
      maxPriorityFeePerGas: transaction.maxPriorityFeePerGas ? (
        // @ts-ignore
        hexToBigInt3(transaction.maxPriorityFeePerGas)
      ) : void 0
    } : {
      // @ts-ignore
      type: transaction.type
    }
  )
});
var convertTransactionReceipt = (transactionReceipt) => ({
  blockHash: transactionReceipt.blockHash,
  blockNumber: hexToBigInt3(transactionReceipt.blockNumber),
  contractAddress: transactionReceipt.contractAddress ? checksumAddress3(transactionReceipt.contractAddress) : null,
  cumulativeGasUsed: hexToBigInt3(transactionReceipt.cumulativeGasUsed),
  effectiveGasPrice: hexToBigInt3(transactionReceipt.effectiveGasPrice),
  from: checksumAddress3(transactionReceipt.from),
  gasUsed: hexToBigInt3(transactionReceipt.gasUsed),
  logs: transactionReceipt.logs.map((log) => ({
    id: `${log.blockHash}-${log.logIndex}`,
    address: checksumAddress3(log.address),
    blockHash: log.blockHash,
    blockNumber: hexToBigInt3(log.blockNumber),
    data: log.data,
    logIndex: hexToNumber4(log.logIndex),
    removed: false,
    topics: [
      log.topics[0] ?? null,
      log.topics[1] ?? null,
      log.topics[2] ?? null,
      log.topics[3] ?? null
    ].filter((t) => t !== null),
    transactionHash: log.transactionHash,
    transactionIndex: hexToNumber4(log.transactionIndex)
  })),
  logsBloom: transactionReceipt.logsBloom,
  status: transactionReceipt.status === "0x1" ? "success" : transactionReceipt.status === "0x0" ? "reverted" : transactionReceipt.status,
  to: transactionReceipt.to ? checksumAddress3(transactionReceipt.to) : null,
  transactionHash: transactionReceipt.transactionHash,
  transactionIndex: Number(transactionReceipt.transactionIndex),
  type: transactionReceipt.type === "0x0" ? "legacy" : transactionReceipt.type === "0x1" ? "eip2930" : transactionReceipt.type === "0x2" ? "eip1559" : transactionReceipt.type === "0x7e" ? "deposit" : transactionReceipt.type
});
var convertCallTrace = (callTrace) => ({
  id: `${callTrace.transactionHash}-${JSON.stringify(callTrace.traceAddress)}`,
  from: checksumAddress3(callTrace.action.from),
  to: checksumAddress3(callTrace.action.to),
  gas: hexToBigInt3(callTrace.action.gas),
  value: hexToBigInt3(callTrace.action.value),
  input: callTrace.action.input,
  output: callTrace.result.output,
  gasUsed: hexToBigInt3(callTrace.result.gasUsed),
  subtraces: callTrace.subtraces,
  traceAddress: callTrace.traceAddress,
  blockHash: callTrace.blockHash,
  blockNumber: hexToBigInt3(callTrace.blockNumber),
  transactionHash: callTrace.transactionHash,
  transactionIndex: callTrace.transactionPosition,
  callType: callTrace.action.callType
});

// src/utils/rpc.ts
import {
  BlockNotFoundError,
  TransactionReceiptNotFoundError,
  numberToHex
} from "viem";
var _eth_getBlockByNumber = (requestQueue, {
  blockNumber,
  blockTag
}) => requestQueue.request({
  method: "eth_getBlockByNumber",
  params: [
    typeof blockNumber === "number" ? numberToHex(blockNumber) : blockNumber ?? blockTag,
    true
  ]
}).then((_block) => {
  if (!_block)
    throw new BlockNotFoundError({
      blockNumber: blockNumber ?? blockTag
    });
  return _block;
});
var _eth_getBlockByHash = (requestQueue, { hash }) => requestQueue.request({
  method: "eth_getBlockByHash",
  params: [hash, true]
}).then((_block) => {
  if (!_block)
    throw new BlockNotFoundError({
      blockHash: hash
    });
  return _block;
});
var _eth_getLogs = async (requestQueue, params) => {
  if ("blockHash" in params) {
    return requestQueue.request({
      method: "eth_getLogs",
      params: [
        {
          blockHash: params.blockHash,
          topics: params.topics,
          address: params.address ? Array.isArray(params.address) ? params.address.map((a) => toLowerCase(a)) : toLowerCase(params.address) : void 0
        }
      ]
    }).then((l) => l);
  }
  return requestQueue.request({
    method: "eth_getLogs",
    params: [
      {
        fromBlock: typeof params.fromBlock === "number" ? numberToHex(params.fromBlock) : params.fromBlock,
        toBlock: typeof params.toBlock === "number" ? numberToHex(params.toBlock) : params.toBlock,
        topics: params.topics,
        address: params.address ? Array.isArray(params.address) ? params.address.map((a) => toLowerCase(a)) : toLowerCase(params.address) : void 0
      }
    ]
  }).then((l) => l);
};
var _eth_getTransactionReceipt = (requestQueue, { hash }) => requestQueue.request({
  method: "eth_getTransactionReceipt",
  params: [hash]
}).then((receipt) => {
  if (!receipt)
    throw new TransactionReceiptNotFoundError({
      hash
    });
  return receipt;
});
var _trace_filter = (requestQueue, params) => requestQueue.request({
  method: "trace_filter",
  params: [
    {
      fromBlock: typeof params.fromBlock === "number" ? numberToHex(params.fromBlock) : params.fromBlock,
      toBlock: typeof params.toBlock === "number" ? numberToHex(params.toBlock) : params.toBlock,
      fromAddress: params.fromAddress ? params.fromAddress.map((a) => toLowerCase(a)) : void 0,
      toAddress: params.toAddress ? params.toAddress.map((a) => toLowerCase(a)) : void 0
    }
  ]
}).then((traces) => traces);
var _trace_block = (requestQueue, params) => requestQueue.request({
  method: "trace_block",
  params: [
    typeof params.blockNumber === "number" ? numberToHex(params.blockNumber) : params.blockNumber
  ]
}).then((traces) => traces);

// src/sync-historical/index.ts
import { getLogsRetryHelper } from "@ponder/utils";
import {
  hexToBigInt as hexToBigInt4,
  hexToNumber as hexToNumber5,
  toHex
} from "viem";
var createHistoricalSync = async (args) => {
  let isKilled = false;
  const blockCache = /* @__PURE__ */ new Map();
  const transactionsCache = /* @__PURE__ */ new Set();
  const getLogsRequestMetadata = /* @__PURE__ */ new Map();
  const intervalsCache = /* @__PURE__ */ new Map();
  for (const { filter } of args.sources) {
    const intervals = await args.syncStore.getIntervals({ filter });
    intervalsCache.set(filter, intervals);
  }
  let latestBlock;
  const getLogsDynamic = async ({
    filter,
    address,
    interval
  }) => {
    const metadata = getLogsRequestMetadata.get(filter);
    const intervals = metadata ? getChunks({
      interval,
      maxChunkSize: metadata.confirmedRange ?? metadata.estimatedRange
    }) : [interval];
    const topics = "eventSelector" in filter ? [filter.eventSelector] : filter.topics;
    let addressBatches;
    if (address === void 0 || typeof address === "string") {
      addressBatches = [address];
    } else if (address.length === 0) {
      return [];
    } else if (address.length >= args.common.options.factoryAddressCountThreshold) {
      addressBatches = [void 0];
    } else {
      addressBatches = [];
      for (let i = 0; i < address.length; i += 50) {
        addressBatches.push(address.slice(i, i + 50));
      }
    }
    const logs = await Promise.all(
      intervals.flatMap(
        (interval2) => addressBatches.map(
          (address2) => _eth_getLogs(args.requestQueue, {
            address: address2,
            topics,
            fromBlock: interval2[0],
            toBlock: interval2[1]
          }).catch((error) => {
            const getLogsErrorResponse = getLogsRetryHelper({
              params: [
                {
                  address: address2,
                  topics,
                  fromBlock: toHex(interval2[0]),
                  toBlock: toHex(interval2[1])
                }
              ],
              error
            });
            if (getLogsErrorResponse.shouldRetry === false)
              throw error;
            const range2 = hexToNumber5(getLogsErrorResponse.ranges[0].toBlock) - hexToNumber5(getLogsErrorResponse.ranges[0].fromBlock);
            args.common.logger.debug({
              service: "sync",
              msg: `Caught eth_getLogs error on '${args.network.name}', updating recommended range to ${range2}.`
            });
            getLogsRequestMetadata.set(filter, {
              estimatedRange: range2,
              confirmedRange: getLogsErrorResponse.isSuggestedRange ? range2 : void 0
            });
            return getLogsDynamic({ address: address2, interval: interval2, filter });
          })
        )
      )
    ).then((logs2) => logs2.flat());
    if (getLogsRequestMetadata.has(filter) && getLogsRequestMetadata.get(filter).confirmedRange === void 0) {
      getLogsRequestMetadata.get(filter).estimatedRange = Math.round(
        getLogsRequestMetadata.get(filter).estimatedRange * 1.05
      );
    }
    return logs;
  };
  const syncLogFilter = async (filter, interval) => {
    const address = isAddressFactory(filter.address) ? await syncAddress(filter.address, interval) : filter.address;
    if (isKilled)
      return;
    const logs = await getLogsDynamic({ filter, interval, address });
    if (isKilled)
      return;
    const blocks = await Promise.all(
      logs.map((log) => syncBlock(hexToBigInt4(log.blockNumber)))
    );
    for (let i = 0; i < logs.length; i++) {
      const log = logs[i];
      const block = blocks[i];
      if (block.hash !== log.blockHash) {
        throw new Error(
          `Detected inconsistent RPC responses. 'log.blockHash' ${log.blockHash} does not match 'block.hash' ${block.hash}`
        );
      }
      if (block.transactions.find((t) => t.hash === log.transactionHash) === void 0) {
        throw new Error(
          `Detected inconsistent RPC responses. 'log.transactionHash' ${log.transactionHash} not found in 'block.transactions' ${block.hash}`
        );
      }
    }
    const transactionHashes = new Set(logs.map((l) => l.transactionHash));
    for (const hash of transactionHashes) {
      transactionsCache.add(hash);
    }
    if (isKilled)
      return;
    await args.syncStore.insertLogs({
      logs: logs.map((log, i) => ({ log, block: blocks[i] })),
      shouldUpdateCheckpoint: true,
      chainId: args.network.chainId
    });
    if (isKilled)
      return;
    if (filter.includeTransactionReceipts) {
      const transactionReceipts = await Promise.all(
        Array.from(transactionHashes).map(
          (hash) => _eth_getTransactionReceipt(args.requestQueue, { hash })
        )
      );
      if (isKilled)
        return;
      await args.syncStore.insertTransactionReceipts({
        transactionReceipts,
        chainId: args.network.chainId
      });
    }
  };
  const syncBlockFilter = async (filter, interval) => {
    const baseOffset = (interval[0] - filter.offset) % filter.interval;
    const offset = baseOffset === 0 ? 0 : filter.interval - baseOffset;
    const requiredBlocks = [];
    for (let b = interval[0] + offset; b <= interval[1]; b += filter.interval) {
      requiredBlocks.push(b);
    }
    await Promise.all(requiredBlocks.map((b) => syncBlock(BigInt(b))));
  };
  const syncTraceFilter = async (filter, interval) => {
    let toAddress;
    if (isAddressFactory(filter.toAddress)) {
      const childAddresses = await syncAddress(filter.toAddress, interval);
      if (childAddresses.length < args.common.options.factoryAddressCountThreshold) {
        toAddress = childAddresses;
      } else {
        toAddress = void 0;
      }
    } else {
      toAddress = filter.toAddress;
    }
    if (isKilled)
      return;
    let callTraces = await _trace_filter(args.requestQueue, {
      fromAddress: filter.fromAddress,
      toAddress,
      fromBlock: interval[0],
      toBlock: interval[1]
    }).then(
      (traces) => traces.flat().filter((t) => t.type === "call")
    );
    if (isKilled)
      return;
    const blocks = await Promise.all(
      callTraces.map((trace) => syncBlock(hexToBigInt4(trace.blockNumber)))
    );
    const transactionHashes = new Set(callTraces.map((t) => t.transactionHash));
    for (let i = 0; i < callTraces.length; i++) {
      const callTrace = callTraces[i];
      const block = blocks[i];
      if (block.hash !== callTrace.blockHash) {
        throw new Error(
          `Detected inconsistent RPC responses. 'trace.blockHash' ${callTrace.blockHash} does not match 'block.hash' ${block.hash}`
        );
      }
      if (block.transactions.find((t) => t.hash === callTrace.transactionHash) === void 0) {
        throw new Error(
          `Detected inconsistent RPC responses. 'trace.transactionHash' ${callTrace.transactionHash} not found in 'block.transactions' ${block.hash}`
        );
      }
    }
    const transactionReceipts = await Promise.all(
      Array.from(transactionHashes).map(
        (hash) => _eth_getTransactionReceipt(args.requestQueue, {
          hash
        })
      )
    );
    const revertedTransactions = /* @__PURE__ */ new Set();
    for (const receipt of transactionReceipts) {
      if (receipt.status === "0x0") {
        revertedTransactions.add(receipt.transactionHash);
      }
    }
    callTraces = callTraces.filter(
      (trace) => revertedTransactions.has(trace.transactionHash) === false
    );
    if (isKilled)
      return;
    for (const hash of transactionHashes) {
      if (revertedTransactions.has(hash) === false) {
        transactionsCache.add(hash);
      }
    }
    if (isKilled)
      return;
    await args.syncStore.insertCallTraces({
      callTraces: callTraces.map((callTrace, i) => ({
        callTrace,
        block: blocks[i]
      })),
      chainId: args.network.chainId
    });
  };
  const syncLogFactory = async (filter, interval) => {
    const logs = await getLogsDynamic({
      filter,
      interval,
      address: filter.address
    });
    if (isKilled)
      return;
    await args.syncStore.insertLogs({
      logs: logs.map((log) => ({ log })),
      shouldUpdateCheckpoint: false,
      chainId: args.network.chainId
    });
  };
  const syncBlock = async (number) => {
    let block;
    if (blockCache.has(number)) {
      block = await blockCache.get(number);
    } else {
      const _block = _eth_getBlockByNumber(args.requestQueue, {
        blockNumber: toHex(number)
      });
      blockCache.set(number, _block);
      block = await _block;
      if (hexToBigInt4(block.number) >= hexToBigInt4(latestBlock?.number ?? "0x0")) {
        latestBlock = block;
      }
    }
    return block;
  };
  const syncAddress = async (filter, interval) => {
    await syncLogFactory(filter, interval);
    return await args.syncStore.getChildAddresses({
      filter,
      limit: args.common.options.factoryAddressCountThreshold
    });
  };
  return {
    intervalsCache,
    async sync(_interval) {
      const syncedIntervals = [];
      await Promise.all(
        args.sources.map(async (source) => {
          if (source.filter.fromBlock > _interval[1] || source.filter.toBlock && source.filter.toBlock < _interval[0]) {
            return;
          }
          const interval = [
            Math.max(source.filter.fromBlock, _interval[0]),
            Math.min(
              source.filter.toBlock ?? Number.POSITIVE_INFINITY,
              _interval[1]
            )
          ];
          const completedIntervals = intervalsCache.get(source.filter);
          const requiredIntervals = intervalDifference(
            [interval],
            completedIntervals
          );
          if (requiredIntervals.length === 0)
            return;
          const blockPromise = syncBlock(BigInt(interval[1]));
          try {
            await Promise.all(
              requiredIntervals.map(async (interval2) => {
                if (source.type === "contract") {
                  const filter = source.filter;
                  switch (filter.type) {
                    case "log": {
                      await syncLogFilter(filter, interval2);
                      break;
                    }
                    case "callTrace":
                      await Promise.all(
                        getChunks({ interval: interval2, maxChunkSize: 10 }).map(
                          async (interval3) => {
                            await syncTraceFilter(filter, interval3);
                          }
                        )
                      );
                      break;
                    default:
                      never(filter);
                  }
                } else {
                  await syncBlockFilter(source.filter, interval2);
                }
              })
            );
          } catch (_error) {
            const error = _error;
            args.common.logger.error({
              service: "sync",
              msg: `Fatal error: Unable to sync '${args.network.name}' from ${interval[0]} to ${interval[1]}.`,
              error
            });
            args.onFatalError(error);
            return;
          }
          if (isKilled)
            return;
          await blockPromise;
          syncedIntervals.push({ filter: source.filter, interval });
        })
      );
      const blocks = await Promise.all(blockCache.values());
      await Promise.all([
        args.syncStore.insertBlocks({ blocks, chainId: args.network.chainId }),
        args.syncStore.insertTransactions({
          transactions: blocks.flatMap(
            ({ transactions }) => transactions.filter(({ hash }) => transactionsCache.has(hash))
          ),
          chainId: args.network.chainId
        })
      ]);
      await Promise.all(
        syncedIntervals.map(
          ({ filter, interval }) => args.syncStore.insertInterval({
            filter,
            interval
          })
        )
      );
      blockCache.clear();
      transactionsCache.clear();
      return latestBlock;
    },
    kill() {
      isKilled = true;
    }
  };
};

// src/utils/range.ts
var range = (start3, stop) => Array.from({ length: stop - start3 }, (_, i) => start3 + i);

// src/sync-realtime/index.ts
import { hexToNumber as hexToNumber7 } from "viem";

// src/sync-realtime/bloom.ts
import { hexToBytes, hexToNumber as hexToNumber6, keccak256 } from "viem";
var zeroLogsBloom = "0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000";
var BLOOM_SIZE_BYTES = 256;
var isInBloom = (_bloom, input) => {
  const bloom = hexToBytes(_bloom);
  const hash = hexToBytes(keccak256(input));
  for (const i of [0, 2, 4]) {
    const bit = hash[i + 1] + (hash[i] << 8) & 2047;
    if ((bloom[BLOOM_SIZE_BYTES - 1 - Math.floor(bit / 8)] & 1 << bit % 8) === 0)
      return false;
  }
  return true;
};
function isFilterInBloom({
  block,
  filter
}) {
  if (hexToNumber6(block.number) < filter.fromBlock || hexToNumber6(block.number) > (filter.toBlock ?? Number.POSITIVE_INFINITY)) {
    return false;
  }
  let isTopicsInBloom;
  let isAddressInBloom;
  if (filter.topics === void 0 || filter.topics.length === 0) {
    isTopicsInBloom = true;
  } else {
    isTopicsInBloom = filter.topics.some((topic) => {
      if (topic === null || topic === void 0) {
        return true;
      } else if (Array.isArray(topic)) {
        return topic.some((t) => isInBloom(block.logsBloom, t));
      } else {
        return isInBloom(block.logsBloom, topic);
      }
    });
  }
  if (filter.address === void 0)
    isAddressInBloom = true;
  else if (isAddressFactory(filter.address)) {
    if ((Array.isArray(filter.address.address) ? filter.address.address.some(
      (address) => isInBloom(block.logsBloom, address)
    ) : isInBloom(block.logsBloom, filter.address.address)) && isInBloom(block.logsBloom, filter.address.eventSelector)) {
      return true;
    }
    isAddressInBloom = true;
  } else if (Array.isArray(filter.address)) {
    if (filter.address.length === 0) {
      isAddressInBloom = true;
    } else {
      isAddressInBloom = filter.address.some(
        (address) => isInBloom(block.logsBloom, address)
      );
    }
  } else {
    isAddressInBloom = isInBloom(block.logsBloom, filter.address);
  }
  return isAddressInBloom && isTopicsInBloom;
}

// src/sync-realtime/index.ts
var ERROR_TIMEOUT = [
  1,
  2,
  5,
  10,
  30,
  60,
  60,
  60,
  60,
  60,
  60,
  60,
  60,
  60
];
var MAX_QUEUED_BLOCKS = 25;
var createRealtimeSync = (args) => {
  let isKilled = false;
  let finalizedBlock;
  let finalizedChildAddresses;
  const unfinalizedChildAddresses = /* @__PURE__ */ new Map();
  const factoryLogsPerBlock = /* @__PURE__ */ new Map();
  let unfinalizedBlocks = [];
  let queue;
  let consecutiveErrors = 0;
  let interval;
  const factories = [];
  const logFilters = [];
  const callTraceFilters = [];
  const blockFilters = [];
  for (const source of args.sources) {
    if (source.type === "contract") {
      if (source.filter.type === "log") {
        logFilters.push(source.filter);
      } else if (source.filter.type === "callTrace") {
        callTraceFilters.push(source.filter);
      }
      const _address = source.filter.type === "log" ? source.filter.address : source.filter.toAddress;
      if (isAddressFactory(_address)) {
        factories.push(_address);
      }
    } else if (source.type === "block") {
      blockFilters.push(source.filter);
    }
  }
  for (const factory of factories) {
    unfinalizedChildAddresses.set(factory, /* @__PURE__ */ new Set());
  }
  const handleBlock = async ({
    block,
    logs,
    factoryLogs,
    callTraces,
    transactions,
    transactionReceipts
  }) => {
    args.common.logger.debug({
      service: "realtime",
      msg: `Started syncing '${args.network.name}' block ${hexToNumber7(block.number)}`
    });
    for (const log of factoryLogs) {
      for (const filter of factories) {
        if (isLogFactoryMatched({ filter, log })) {
          unfinalizedChildAddresses.get(filter).add(getChildAddress({ log, factory: filter }));
        }
      }
    }
    const matchedFilters = /* @__PURE__ */ new Set();
    logs = logs.filter((log) => {
      let isMatched = false;
      for (const filter of logFilters) {
        if (isLogFilterMatched({ filter, block, log }) && (isAddressFactory(filter.address) ? finalizedChildAddresses.get(filter.address).has(log.address.toLowerCase()) || unfinalizedChildAddresses.get(filter.address).has(log.address.toLowerCase()) : true)) {
          matchedFilters.add(filter);
          isMatched = true;
        }
      }
      return isMatched;
    });
    callTraces = callTraces.filter((callTrace) => {
      let isMatched = false;
      for (const filter of callTraceFilters) {
        if (isCallTraceFilterMatched({ filter, block, callTrace }) && (isAddressFactory(filter.toAddress) ? finalizedChildAddresses.get(filter.toAddress).has(callTrace.action.to.toLowerCase()) || unfinalizedChildAddresses.get(filter.toAddress).has(callTrace.action.to.toLowerCase()) : true)) {
          matchedFilters.add(filter);
          isMatched = true;
        }
      }
      return isMatched;
    });
    const transactionHashes = /* @__PURE__ */ new Set();
    for (const log of logs) {
      transactionHashes.add(log.transactionHash);
    }
    for (const trace of callTraces) {
      transactionHashes.add(trace.transactionHash);
    }
    transactions = transactions.filter((t) => transactionHashes.has(t.hash));
    transactionReceipts = transactionReceipts.filter(
      (t) => transactionHashes.has(t.transactionHash)
    );
    for (const filter of blockFilters) {
      if (isBlockFilterMatched({ filter, block })) {
        matchedFilters.add(filter);
      }
    }
    if (logs.length > 0 || callTraces.length > 0) {
      const _text = [];
      if (logs.length === 1) {
        _text.push("1 log");
      } else if (logs.length > 1) {
        _text.push(`${logs.length} logs`);
      }
      if (callTraces.length === 1) {
        _text.push("1 call trace");
      } else if (callTraces.length > 1) {
        _text.push(`${callTraces.length} call traces`);
      }
      const text = _text.filter((t) => t !== void 0).join(" and ");
      args.common.logger.info({
        service: "realtime",
        msg: `Synced ${text} from '${args.network.name}' block ${hexToNumber7(block.number)}`
      });
    } else {
      args.common.logger.info({
        service: "realtime",
        msg: `Synced block ${hexToNumber7(block.number)} from '${args.network.name}' `
      });
    }
    unfinalizedBlocks.push(syncBlockToLightBlock(block));
    block.transactions = void 0;
    await args.onEvent({
      type: "block",
      filters: matchedFilters,
      block,
      factoryLogs,
      logs,
      callTraces,
      transactions,
      transactionReceipts
    });
    const blockMovesFinality = hexToNumber7(block.number) >= hexToNumber7(finalizedBlock.number) + 2 * args.network.finalityBlockCount;
    if (blockMovesFinality) {
      const pendingFinalizedBlock = unfinalizedBlocks.find(
        (lb) => hexToNumber7(lb.number) === hexToNumber7(block.number) - args.network.finalityBlockCount
      );
      args.common.logger.debug({
        service: "realtime",
        msg: `Finalized ${hexToNumber7(pendingFinalizedBlock.number) - hexToNumber7(finalizedBlock.number) + 1} '${args.network.name}' blocks from ${hexToNumber7(finalizedBlock.number) + 1} to ${hexToNumber7(pendingFinalizedBlock.number)}`
      });
      const finalizedBlocks = unfinalizedBlocks.filter(
        (lb) => hexToNumber7(lb.number) <= hexToNumber7(pendingFinalizedBlock.number)
      );
      unfinalizedBlocks = unfinalizedBlocks.filter(
        (lb) => hexToNumber7(lb.number) > hexToNumber7(pendingFinalizedBlock.number)
      );
      for (const filter of factories) {
        for (const { hash } of finalizedBlocks) {
          const factoryLogs2 = factoryLogsPerBlock.get(hash);
          if (factoryLogs2 !== void 0) {
            for (const log of factoryLogs2) {
              if (isLogFactoryMatched({ filter, log })) {
                finalizedChildAddresses.get(filter).add(getChildAddress({ log, factory: filter }));
              }
            }
          }
        }
      }
      unfinalizedChildAddresses.clear();
      for (const filter of factories) {
        unfinalizedChildAddresses.set(filter, /* @__PURE__ */ new Set());
        for (const { hash } of unfinalizedBlocks) {
          const factoryLogs2 = factoryLogsPerBlock.get(hash);
          if (factoryLogs2 !== void 0) {
            for (const log of factoryLogs2) {
              if (isLogFactoryMatched({ filter, log })) {
                unfinalizedChildAddresses.get(filter).add(getChildAddress({ log, factory: filter }));
              }
            }
          }
        }
      }
      for (const { hash } of finalizedBlocks) {
        factoryLogsPerBlock.delete(hash);
      }
      finalizedBlock = pendingFinalizedBlock;
      await args.onEvent({ type: "finalize", block: pendingFinalizedBlock });
    }
    args.common.logger.debug({
      service: "realtime",
      msg: `Finished syncing '${args.network.name}' block ${hexToNumber7(block.number)}`
    });
  };
  const handleReorg = async (block) => {
    args.common.logger.warn({
      service: "realtime",
      msg: `Detected forked '${args.network.name}' block at height ${hexToNumber7(block.number)}`
    });
    const reorgedBlocks = unfinalizedBlocks.filter(
      (lb) => hexToNumber7(lb.number) >= hexToNumber7(block.number)
    );
    unfinalizedBlocks = unfinalizedBlocks.filter(
      (lb) => hexToNumber7(lb.number) < hexToNumber7(block.number)
    );
    let remoteBlock = block;
    while (true) {
      const parentBlock = getLatestUnfinalizedBlock();
      if (parentBlock.hash === remoteBlock.parentHash)
        break;
      if (unfinalizedBlocks.length === 0) {
        const msg = `Encountered unrecoverable '${args.network.name}' reorg beyond finalized block ${hexToNumber7(finalizedBlock.number)}`;
        args.common.logger.warn({ service: "realtime", msg });
        throw new Error(msg);
      } else {
        remoteBlock = await _eth_getBlockByHash(args.requestQueue, {
          hash: remoteBlock.parentHash
        });
        reorgedBlocks.push(unfinalizedBlocks.pop());
      }
    }
    const commonAncestor = getLatestUnfinalizedBlock();
    await args.onEvent({ type: "reorg", block: commonAncestor, reorgedBlocks });
    args.common.logger.warn({
      service: "realtime",
      msg: `Reconciled ${reorgedBlocks.length}-block reorg on '${args.network.name}' with common ancestor block ${hexToNumber7(commonAncestor.number)}`
    });
    unfinalizedChildAddresses.clear();
    for (const filter of factories) {
      unfinalizedChildAddresses.set(filter, /* @__PURE__ */ new Set());
      for (const { hash } of unfinalizedBlocks) {
        const factoryLogs = factoryLogsPerBlock.get(hash);
        if (factoryLogs !== void 0) {
          for (const log of factoryLogs) {
            if (isLogFactoryMatched({ filter, log })) {
              unfinalizedChildAddresses.get(filter).add(getChildAddress({ log, factory: filter }));
            }
          }
        }
      }
    }
    for (const { hash } of reorgedBlocks) {
      factoryLogsPerBlock.delete(hash);
    }
  };
  const fetchBlockEventData = async (block) => {
    const shouldRequestLogs = block.logsBloom === zeroLogsBloom || logFilters.some((filter) => isFilterInBloom({ block, filter }));
    let logs = [];
    if (shouldRequestLogs) {
      logs = await _eth_getLogs(args.requestQueue, { blockHash: block.hash });
      if (block.logsBloom !== zeroLogsBloom && logs.length === 0) {
        throw new Error(
          "Detected invalid eth_getLogs response. `block.logsBloom` is not empty but zero logs were returned."
        );
      }
      for (const log of logs) {
        if (log.blockHash !== block.hash) {
          throw new Error(
            `Detected invalid eth_getLogs response. 'log.blockHash' ${log.blockHash} does not match requested block hash ${block.hash}`
          );
        }
      }
    }
    if (shouldRequestLogs === false && args.sources.some((s) => s.filter.type === "log")) {
      args.common.logger.debug({
        service: "realtime",
        msg: `Skipped fetching logs for '${args.network.name}' block ${hexToNumber7(block.number)} due to bloom filter result`
      });
    }
    const shouldRequestTraces = callTraceFilters.length > 0;
    let callTraces = [];
    if (shouldRequestTraces) {
      const traces = await _trace_block(args.requestQueue, {
        blockNumber: hexToNumber7(block.number)
      });
      if (block.transactions.length !== 0 && traces.length === 0) {
        throw new Error(
          "Detected invalid trace_block response. `block.transactions` is not empty but zero traces were returned."
        );
      }
      callTraces = traces.filter(
        (trace) => trace.type === "call"
      );
    }
    for (const trace of callTraces) {
      if (trace.blockHash !== block.hash) {
        throw new Error(
          `Detected inconsistent RPC responses. 'trace.blockHash' ${trace.blockHash} does not match 'block.hash' ${block.hash}`
        );
      }
    }
    const factoryLogs = logs.filter((log) => {
      let isMatched = false;
      for (const filter of factories) {
        if (isLogFactoryMatched({ filter, log })) {
          if (factoryLogsPerBlock.has(block.hash) === false) {
            factoryLogsPerBlock.set(block.hash, []);
          }
          factoryLogsPerBlock.get(block.hash).push(log);
          isMatched = true;
        }
      }
      return isMatched;
    });
    const requiredTransactions = /* @__PURE__ */ new Set();
    const requiredTransactionReceipts = /* @__PURE__ */ new Set();
    logs = logs.filter((log) => {
      let isLogMatched = false;
      for (const filter of logFilters) {
        if (isLogFilterMatched({ filter, block, log })) {
          isLogMatched = true;
          requiredTransactions.add(log.transactionHash);
          if (filter.includeTransactionReceipts) {
            requiredTransactionReceipts.add(log.transactionHash);
          }
        }
      }
      return isLogMatched;
    });
    callTraces = callTraces.filter((callTrace) => {
      let isCallTraceMatched = false;
      for (const filter of callTraceFilters) {
        if (isCallTraceFilterMatched({ filter, block, callTrace })) {
          isCallTraceMatched = true;
          requiredTransactions.add(callTrace.transactionHash);
          if (filter.includeTransactionReceipts) {
            requiredTransactionReceipts.add(callTrace.transactionHash);
          }
        }
      }
      return isCallTraceMatched;
    });
    const transactions = block.transactions.filter(
      ({ hash }) => requiredTransactions.has(hash)
    );
    const blockTransactionsHashes = new Set(
      block.transactions.map((t) => t.hash)
    );
    for (const hash of Array.from(requiredTransactions)) {
      if (blockTransactionsHashes.has(hash) === false) {
        throw new Error(
          `Detected inconsistent RPC responses. Transaction with hash ${hash} is missing in \`block.transactions\`.`
        );
      }
    }
    const transactionReceipts = await Promise.all(
      block.transactions.filter(({ hash }) => requiredTransactionReceipts.has(hash)).map(
        ({ hash }) => _eth_getTransactionReceipt(args.requestQueue, { hash })
      )
    );
    const revertedTransactions = /* @__PURE__ */ new Set();
    for (const receipt of transactionReceipts) {
      if (receipt.status === "0x0") {
        revertedTransactions.add(receipt.transactionHash);
      }
    }
    callTraces = callTraces.filter(
      (trace) => revertedTransactions.has(trace.transactionHash) === false
    );
    return {
      block,
      logs,
      factoryLogs,
      callTraces,
      transactions,
      transactionReceipts
    };
  };
  const getLatestUnfinalizedBlock = () => {
    if (unfinalizedBlocks.length === 0) {
      return finalizedBlock;
    } else
      return unfinalizedBlocks[unfinalizedBlocks.length - 1];
  };
  return {
    start(startArgs) {
      finalizedBlock = startArgs.syncProgress.finalized;
      finalizedChildAddresses = startArgs.initialChildAddresses;
      queue = createQueue({
        browser: false,
        concurrency: 1,
        initialStart: true,
        worker: async ({ block, ...rest }) => {
          const latestBlock = getLatestUnfinalizedBlock();
          if (latestBlock.hash === block.hash) {
            args.common.logger.trace({
              service: "realtime",
              msg: `Skipped processing '${args.network.name}' block ${hexToNumber7(block.number)}, already synced`
            });
            return;
          }
          try {
            if (hexToNumber7(latestBlock.number) >= hexToNumber7(block.number)) {
              await handleReorg(block);
              queue.clear();
              return;
            }
            if (hexToNumber7(latestBlock.number) + 1 < hexToNumber7(block.number)) {
              const missingBlockRange = range(
                hexToNumber7(latestBlock.number) + 1,
                Math.min(
                  hexToNumber7(block.number),
                  hexToNumber7(latestBlock.number) + MAX_QUEUED_BLOCKS
                )
              );
              const pendingBlocks = await Promise.all(
                missingBlockRange.map(
                  (blockNumber) => _eth_getBlockByNumber(args.requestQueue, {
                    blockNumber
                  }).then((block2) => fetchBlockEventData(block2))
                )
              );
              args.common.logger.debug({
                service: "realtime",
                msg: `Fetched ${missingBlockRange.length} missing '${args.network.name}' blocks from ${hexToNumber7(latestBlock.number) + 1} to ${Math.min(
                  hexToNumber7(block.number),
                  hexToNumber7(latestBlock.number) + MAX_QUEUED_BLOCKS
                )}`
              });
              if (isKilled)
                return;
              queue.clear();
              for (const pendingBlock of pendingBlocks) {
                queue.add(pendingBlock);
              }
              queue.add({ block, ...rest });
              return;
            }
            if (block.parentHash !== latestBlock.hash) {
              await handleReorg(block);
              queue.clear();
              return;
            }
            await handleBlock({ block, ...rest });
            consecutiveErrors = 0;
            return;
          } catch (_error) {
            if (isKilled)
              return;
            const error = _error;
            error.stack = void 0;
            args.common.logger.warn({
              service: "realtime",
              msg: `Failed to process '${args.network.name}' block ${hexToNumber7(block.number)}`,
              error
            });
            const duration = ERROR_TIMEOUT[consecutiveErrors];
            args.common.logger.warn({
              service: "realtime",
              msg: `Retrying '${args.network.name}' sync after ${duration} ${duration === 1 ? "second" : "seconds"}.`
            });
            await wait(duration * 1e3);
            queue.clear();
            if (++consecutiveErrors === ERROR_TIMEOUT.length) {
              args.common.logger.error({
                service: "realtime",
                msg: `Fatal error: Unable to process '${args.network.name}' block ${hexToNumber7(block.number)} after ${ERROR_TIMEOUT.length} attempts.`,
                error
              });
              args.onFatalError(error);
            }
          }
        }
      });
      const enqueue = async () => {
        try {
          const block = await _eth_getBlockByNumber(args.requestQueue, {
            blockTag: "latest"
          });
          const latestBlock = getLatestUnfinalizedBlock();
          if (latestBlock.hash === block.hash) {
            args.common.logger.trace({
              service: "realtime",
              msg: `Skipped processing '${args.network.name}' block ${hexToNumber7(block.number)}, already synced`
            });
            return;
          }
          const blockWithEventData = await fetchBlockEventData(block);
          consecutiveErrors = 0;
          return queue.add(blockWithEventData);
        } catch (_error) {
          if (isKilled)
            return;
          const error = _error;
          args.common.logger.warn({
            service: "realtime",
            msg: `Failed to fetch latest '${args.network.name}' block`,
            error
          });
          if (++consecutiveErrors === ERROR_TIMEOUT.length) {
            args.common.logger.error({
              service: "realtime",
              msg: `Fatal error: Unable to fetch latest '${args.network.name}' block after ${ERROR_TIMEOUT.length} attempts.`,
              error
            });
            args.onFatalError(error);
          }
        }
      };
      interval = setInterval(enqueue, args.network.pollingInterval);
      return enqueue().then(() => queue);
    },
    async kill() {
      clearInterval(interval);
      isKilled = true;
      queue?.pause();
      queue?.clear();
      await queue?.onIdle();
    },
    get unfinalizedBlocks() {
      return unfinalizedBlocks;
    },
    get finalizedChildAddresses() {
      return finalizedChildAddresses;
    },
    get unfinalizedChildAddresses() {
      return unfinalizedChildAddresses;
    }
  };
};

// src/utils/estimate.ts
var estimate = ({
  from,
  to,
  target,
  result,
  min: min2,
  max,
  prev,
  maxIncrease
}) => {
  const density = (to - from) / (result || 1);
  return Math.min(
    Math.max(min2, Math.round(target * density)),
    Math.round(prev * maxIncrease),
    max
  );
};

// src/utils/generators.ts
async function* mergeAsyncGenerators(generators) {
  const results = [];
  let count = generators.length;
  let pwr = promiseWithResolvers();
  generators.map(async (generator) => {
    for await (const result of generator) {
      results.push(result);
      pwr.resolve();
    }
    count--;
    pwr.resolve();
  });
  while (count > 0 || results.length > 0) {
    if (results.length > 0) {
      yield results.shift();
    } else {
      await pwr.promise;
      pwr = promiseWithResolvers();
    }
  }
}

// src/utils/requestQueue.ts
import {
  getLogsRetryHelper as getLogsRetryHelper2
} from "@ponder/utils";
import {
  BlockNotFoundError as BlockNotFoundError2,
  HttpRequestError,
  InternalRpcError,
  InvalidInputRpcError,
  LimitExceededRpcError,
  isHex
} from "viem";
var RETRY_COUNT2 = 9;
var BASE_DURATION2 = 125;
var createRequestQueue = ({
  network,
  common
}) => {
  const fetchRequest = async (request) => {
    for (let i = 0; i <= RETRY_COUNT2; i++) {
      try {
        const stopClock = startClock();
        const response = await network.transport.request(request);
        common.metrics.ponder_rpc_request_duration.observe(
          { method: request.method, network: network.name },
          stopClock()
        );
        return response;
      } catch (_error) {
        const error = _error;
        if (request.method === "eth_getLogs" && isHex(request.params[0].fromBlock) && isHex(request.params[0].toBlock)) {
          const getLogsErrorResponse = getLogsRetryHelper2({
            params: request.params,
            error
          });
          if (getLogsErrorResponse.shouldRetry === true)
            throw error;
        }
        if (shouldRetry(error) === false) {
          common.logger.warn({
            service: "sync",
            msg: `Failed '${request.method}' RPC request`
          });
          throw error;
        }
        if (i === RETRY_COUNT2) {
          common.logger.warn({
            service: "sync",
            msg: `Failed '${request.method}' RPC request after ${i + 1} attempts`,
            error
          });
          throw error;
        }
        const duration = BASE_DURATION2 * 2 ** i;
        common.logger.debug({
          service: "sync",
          msg: `Failed '${request.method}' RPC request, retrying after ${duration} milliseconds`,
          error
        });
        await wait(duration);
      }
    }
  };
  const requestQueue = createQueue({
    frequency: network.maxRequestsPerSecond,
    concurrency: Math.ceil(network.maxRequestsPerSecond / 4),
    initialStart: true,
    browser: false,
    worker: async (task) => {
      common.metrics.ponder_rpc_request_lag.observe(
        { method: task.request.method, network: network.name },
        task.stopClockLag()
      );
      return await fetchRequest(task.request);
    }
  });
  return {
    ...requestQueue,
    request: (params) => {
      const stopClockLag = startClock();
      return requestQueue.add({ request: params, stopClockLag });
    }
  };
};
function shouldRetry(error) {
  if ("code" in error && typeof error.code === "number") {
    if (error.code === -1)
      return true;
    if (error.code === InvalidInputRpcError.code)
      return true;
    if (error.code === LimitExceededRpcError.code)
      return true;
    if (error.code === InternalRpcError.code)
      return true;
    return false;
  }
  if (error instanceof BlockNotFoundError2)
    return true;
  if (error instanceof HttpRequestError && error.status) {
    if (error.status === 403)
      return true;
    if (error.status === 408)
      return true;
    if (error.status === 413)
      return true;
    if (error.status === 429)
      return true;
    if (error.status === 500)
      return true;
    if (error.status === 502)
      return true;
    if (error.status === 503)
      return true;
    if (error.status === 504)
      return true;
    return false;
  }
  return true;
}

// src/sync/index.ts
import {
  hexToBigInt as hexToBigInt6,
  hexToNumber as hexToNumber8
} from "viem";

// src/sync/transport.ts
import { custom, hexToBigInt as hexToBigInt5, maxUint256 } from "viem";
var cachedMethods = [
  "eth_call",
  "eth_getBalance",
  "eth_getCode",
  "eth_getStorageAt"
];
var cachedTransport = ({
  requestQueue,
  syncStore
}) => {
  return ({ chain }) => {
    const c = custom({
      async request({ method, params }) {
        const body = { method, params };
        if (cachedMethods.includes(method)) {
          let request = void 0;
          let blockNumber = void 0;
          if (method === "eth_call") {
            const [{ data, to }, _blockNumber] = params;
            request = `${method}_${toLowerCase(to)}_${toLowerCase(data)}`;
            blockNumber = _blockNumber;
          } else if (method === "eth_getBalance") {
            const [address, _blockNumber] = params;
            request = `${method}_${toLowerCase(address)}`;
            blockNumber = _blockNumber;
          } else if (method === "eth_getCode") {
            const [address, _blockNumber] = params;
            request = `${method}_${toLowerCase(address)}`;
            blockNumber = _blockNumber;
          } else if (method === "eth_getStorageAt") {
            const [address, slot, _blockNumber] = params;
            request = `${method}_${toLowerCase(address)}_${toLowerCase(slot)}`;
            blockNumber = _blockNumber;
          }
          const blockNumberBigInt = blockNumber === "latest" ? maxUint256 : hexToBigInt5(blockNumber);
          const cachedResult = await syncStore.getRpcRequestResult({
            blockNumber: blockNumberBigInt,
            chainId: chain.id,
            request
          });
          if (cachedResult !== null)
            return cachedResult;
          else {
            const response = await requestQueue.request(body);
            await syncStore.insertRpcRequestResult({
              blockNumber: blockNumberBigInt,
              chainId: chain.id,
              request,
              result: response
            });
            return response;
          }
        } else {
          return requestQueue.request(body);
        }
      }
    });
    return c({ chain, retryCount: 0 });
  };
};

// src/sync/index.ts
var syncBlockToLightBlock = ({
  hash,
  parentHash,
  number,
  timestamp
}) => ({
  hash,
  parentHash,
  number,
  timestamp
});
var blockToCheckpoint = (block, chainId, rounding) => {
  return {
    ...rounding === "up" ? maxCheckpoint : zeroCheckpoint,
    blockTimestamp: hexToNumber8(block.timestamp),
    chainId: BigInt(chainId),
    blockNumber: hexToBigInt6(block.number)
  };
};
var isSyncEnd = (syncProgress) => {
  if (syncProgress.end === void 0 || syncProgress.current === void 0) {
    return false;
  }
  return hexToNumber8(syncProgress.current.number) >= hexToNumber8(syncProgress.end.number);
};
var isSyncFinalized = (syncProgress) => {
  if (syncProgress.current === void 0) {
    return false;
  }
  return hexToNumber8(syncProgress.current.number) >= hexToNumber8(syncProgress.finalized.number);
};
var getHistoricalLast = (syncProgress) => {
  return syncProgress.end === void 0 ? syncProgress.finalized : hexToNumber8(syncProgress.end.number) > hexToNumber8(syncProgress.finalized.number) ? syncProgress.finalized : syncProgress.end;
};
var min = (...checkpoints) => {
  return checkpoints.reduce((acc, cur) => {
    if (cur === void 0)
      return acc;
    if (acc === void 0)
      return cur;
    if (acc < cur)
      return acc;
    return cur;
  });
};
var splitEvents = (events) => {
  let prevHash;
  const result = [];
  for (const event of events) {
    if (prevHash === void 0 || prevHash !== event.block.hash) {
      result.push([]);
      prevHash = event.block.hash;
    }
    result[result.length - 1].push(event);
  }
  return result;
};
var getChainCheckpoint = ({
  syncProgress,
  network,
  tag
}) => {
  if (tag === "end" && syncProgress.end === void 0) {
    return void 0;
  }
  if (tag === "current" && isSyncEnd(syncProgress)) {
    return void 0;
  }
  const block = syncProgress[tag];
  return encodeCheckpoint(
    blockToCheckpoint(
      block,
      network.chainId,
      // The checkpoint returned by this function is meant to be used in
      // a closed interval (includes endpoints), so "start" should be inclusive.
      tag === "start" ? "down" : "up"
    )
  );
};
var createSync = async (args) => {
  const localSyncContext = /* @__PURE__ */ new Map();
  const status = {};
  let isKilled = false;
  let pendingEvents = [];
  await Promise.all(
    args.networks.map(async (network) => {
      const requestQueue = createRequestQueue({
        network,
        common: args.common
      });
      const sources = args.sources.filter(
        ({ filter }) => filter.chainId === network.chainId
      );
      const { start: start3, end, finalized } = await syncDiagnostic({
        common: args.common,
        sources,
        requestQueue,
        network
      });
      if (network.disableCache) {
        args.common.logger.warn({
          service: "sync",
          msg: `Deleting cache records for '${network.name}' from block ${hexToNumber8(start3.number)}`
        });
        await args.syncStore.pruneByChain({
          fromBlock: hexToNumber8(start3.number),
          chainId: network.chainId
        });
      }
      const historicalSync = await createHistoricalSync({
        common: args.common,
        sources,
        syncStore: args.syncStore,
        requestQueue,
        network,
        onFatalError: args.onFatalError
      });
      const realtimeSync = createRealtimeSync({
        common: args.common,
        sources,
        requestQueue,
        network,
        onEvent: (event) => onRealtimeSyncEvent({ event, network }).catch((error) => {
          args.common.logger.error({
            service: "sync",
            msg: `Fatal error: Unable to process ${event.type} event`,
            error
          });
          args.onFatalError(error);
        }),
        onFatalError: args.onFatalError
      });
      const cached = await getCachedBlock({
        sources,
        requestQueue,
        historicalSync
      });
      if (cached !== void 0) {
        args.common.metrics.ponder_sync_block.set(
          { network: network.name },
          hexToNumber8(cached.number)
        );
      }
      const syncProgress = {
        start: start3,
        end,
        finalized,
        cached,
        current: cached
      };
      args.common.metrics.ponder_sync_is_realtime.set(
        { network: network.name },
        0
      );
      args.common.metrics.ponder_sync_is_complete.set(
        { network: network.name },
        0
      );
      localSyncContext.set(network, {
        requestQueue,
        syncProgress,
        historicalSync,
        realtimeSync,
        unfinalizedEventData: []
      });
      status[network.name] = { block: null, ready: false };
    })
  );
  const getOmnichainCheckpoint = (tag) => {
    const checkpoints = Array.from(localSyncContext.entries()).map(
      ([network, { syncProgress }]) => getChainCheckpoint({ syncProgress, network, tag })
    );
    if (tag === "end" && checkpoints.some((c) => c === void 0)) {
      return void 0;
    }
    if (tag === "current" && checkpoints.every((c) => c === void 0)) {
      return void 0;
    }
    return min(...checkpoints);
  };
  const updateHistoricalStatus = ({
    events,
    checkpoint,
    network
  }) => {
    if (Number(decodeCheckpoint(checkpoint).chainId) === network.chainId) {
      status[network.name].block = {
        timestamp: decodeCheckpoint(checkpoint).blockTimestamp,
        number: Number(decodeCheckpoint(checkpoint).blockNumber)
      };
    } else {
      let i = events.length - 1;
      while (i >= 0) {
        const event = events[i];
        if (network.chainId === event.chainId) {
          status[network.name].block = {
            timestamp: decodeCheckpoint(event.checkpoint).blockTimestamp,
            number: Number(decodeCheckpoint(event.checkpoint).blockNumber)
          };
        }
        i--;
      }
    }
  };
  const updateRealtimeStatus = ({
    checkpoint,
    network
  }) => {
    const localBlock = localSyncContext.get(network).realtimeSync.unfinalizedBlocks.findLast(
      (block) => encodeCheckpoint(blockToCheckpoint(block, network.chainId, "up")) <= checkpoint
    );
    if (localBlock !== void 0) {
      status[network.name].block = {
        timestamp: hexToNumber8(localBlock.timestamp),
        number: hexToNumber8(localBlock.number)
      };
    }
  };
  let estimateSeconds = 1e3;
  async function* getEvents() {
    let latestFinalizedFetch = Date.now();
    const start3 = args.initialCheckpoint !== encodeCheckpoint(zeroCheckpoint) ? args.initialCheckpoint : getOmnichainCheckpoint("start");
    let from = start3;
    let showLogs = true;
    while (true) {
      const syncGenerator = mergeAsyncGenerators(
        Array.from(localSyncContext.entries()).map(
          ([network, { syncProgress, historicalSync }]) => localHistoricalSyncGenerator({
            common: args.common,
            network,
            syncProgress,
            historicalSync,
            showLogs
          })
        )
      );
      showLogs = false;
      for await (const _ of syncGenerator) {
        if (Array.from(localSyncContext.values()).some(
          ({ syncProgress }) => syncProgress.current === void 0
        )) {
          continue;
        }
        const to = min(
          getOmnichainCheckpoint("end"),
          getOmnichainCheckpoint("finalized"),
          getOmnichainCheckpoint("current")
        );
        while (true) {
          if (isKilled)
            return;
          if (from >= to)
            break;
          const getEventsMaxBatchSize = args.common.options.syncEventsQuerySize;
          let consecutiveErrors = 0;
          const estimatedTo = encodeCheckpoint({
            ...zeroCheckpoint,
            blockTimestamp: Math.min(
              decodeCheckpoint(from).blockTimestamp + estimateSeconds,
              maxCheckpoint.blockTimestamp
            )
          });
          try {
            const { events, cursor } = await args.syncStore.getEvents({
              filters: args.sources.map(({ filter }) => filter),
              from,
              to: to < estimatedTo ? to : estimatedTo,
              limit: getEventsMaxBatchSize
            });
            consecutiveErrors = 0;
            for (const network of args.networks) {
              updateHistoricalStatus({ events, checkpoint: cursor, network });
            }
            estimateSeconds = estimate({
              from: decodeCheckpoint(from).blockTimestamp,
              to: decodeCheckpoint(cursor).blockTimestamp,
              target: getEventsMaxBatchSize,
              result: events.length,
              min: 10,
              max: 86400,
              prev: estimateSeconds,
              maxIncrease: 1.08
            });
            yield { events, checkpoint: to };
            from = cursor;
            const { eta, progress } = await getAppProgress(args.common.metrics);
            if (events.length > 0) {
              if (eta === void 0 || progress === void 0) {
                args.common.logger.info({
                  service: "app",
                  msg: `Indexed ${events.length} events`
                });
              } else {
                args.common.logger.info({
                  service: "app",
                  msg: `Indexed ${events.length} events with ${formatPercentage(progress)} complete and ${formatEta(eta)} remaining`
                });
              }
            }
          } catch (error) {
            estimateSeconds = Math.max(10, Math.round(estimateSeconds / 10));
            if (++consecutiveErrors > 4)
              throw error;
          }
        }
      }
      const allHistoricalSyncExhaustive = Array.from(
        localSyncContext.values()
      ).every(({ syncProgress }) => {
        if (isSyncEnd(syncProgress))
          return true;
        const staleSeconds = (Date.now() - latestFinalizedFetch) / 1e3;
        if (staleSeconds <= args.common.options.syncHandoffStaleSeconds) {
          return true;
        }
        return false;
      });
      if (allHistoricalSyncExhaustive)
        break;
      latestFinalizedFetch = Date.now();
      await Promise.all(
        Array.from(localSyncContext.entries()).map(
          async ([network, { requestQueue, syncProgress }]) => {
            args.common.logger.debug({
              service: "sync",
              msg: `Refetching '${network.name}' finalized block`
            });
            const latestBlock = await _eth_getBlockByNumber(requestQueue, {
              blockTag: "latest"
            });
            const finalizedBlockNumber = Math.max(
              0,
              hexToNumber8(latestBlock.number) - network.finalityBlockCount
            );
            syncProgress.finalized = await _eth_getBlockByNumber(requestQueue, {
              blockNumber: finalizedBlockNumber
            });
            const historicalLast = getHistoricalLast(syncProgress);
            args.common.metrics.ponder_historical_total_blocks.set(
              { network: network.name },
              hexToNumber8(historicalLast.number) - hexToNumber8(syncProgress.start.number) + 1
            );
          }
        )
      );
    }
  }
  const onRealtimeSyncEvent = async ({
    network,
    event
  }) => {
    const { syncProgress, realtimeSync, unfinalizedEventData } = localSyncContext.get(network);
    switch (event.type) {
      case "block": {
        const from = getOmnichainCheckpoint("current");
        syncProgress.current = event.block;
        const to = getOmnichainCheckpoint("current");
        args.common.metrics.ponder_sync_block.set(
          { network: network.name },
          hexToNumber8(syncProgress.current.number)
        );
        const blockWithEventData = {
          block: event.block,
          filters: event.filters,
          logs: event.logs,
          factoryLogs: event.factoryLogs,
          callTraces: event.callTraces,
          transactions: event.transactions,
          transactionReceipts: event.transactionReceipts
        };
        unfinalizedEventData.push(blockWithEventData);
        const events = buildEvents({
          sources: args.sources,
          chainId: network.chainId,
          blockWithEventData,
          finalizedChildAddresses: realtimeSync.finalizedChildAddresses,
          unfinalizedChildAddresses: realtimeSync.unfinalizedChildAddresses
        });
        pendingEvents.push(...events);
        if (to > from) {
          for (const network2 of args.networks) {
            updateRealtimeStatus({ checkpoint: to, network: network2 });
          }
          const events2 = pendingEvents.filter(({ checkpoint }) => checkpoint <= to).sort((a, b) => a.checkpoint < b.checkpoint ? -1 : 1);
          pendingEvents = pendingEvents.filter(
            ({ checkpoint }) => checkpoint > to
          );
          args.onRealtimeEvent({
            type: "block",
            checkpoint: to,
            status: structuredClone(status),
            events: events2
          }).then(() => {
            if (events2.length > 0 && isKilled === false) {
              args.common.logger.info({
                service: "app",
                msg: `Indexed ${events2.length} events`
              });
            }
          });
        }
        break;
      }
      case "finalize": {
        const interval = [
          hexToNumber8(syncProgress.finalized.number),
          hexToNumber8(event.block.number)
        ];
        const prev = getOmnichainCheckpoint("finalized");
        syncProgress.finalized = event.block;
        const checkpoint = getOmnichainCheckpoint("finalized");
        if (checkpoint > prev) {
          args.onRealtimeEvent({ type: "finalize", checkpoint });
        }
        const finalizedEventData = unfinalizedEventData.filter(
          (ued) => hexToNumber8(ued.block.number) <= hexToNumber8(event.block.number)
        );
        localSyncContext.get(network).unfinalizedEventData = unfinalizedEventData.filter(
          (ued) => hexToNumber8(ued.block.number) > hexToNumber8(event.block.number)
        );
        if (getChainCheckpoint({ syncProgress, network, tag: "finalized" }) > getOmnichainCheckpoint("current")) {
          args.common.logger.warn({
            service: "sync",
            msg: `Finalized block for '${network.name}' has surpassed overall indexing checkpoint`
          });
        }
        await Promise.all([
          args.syncStore.insertBlocks({
            blocks: finalizedEventData.filter(({ filters }) => filters.size > 0).map(({ block }) => block),
            chainId: network.chainId
          }),
          args.syncStore.insertLogs({
            logs: finalizedEventData.flatMap(
              ({ logs, block }) => logs.map((log) => ({ log, block }))
            ),
            shouldUpdateCheckpoint: true,
            chainId: network.chainId
          }),
          args.syncStore.insertLogs({
            logs: finalizedEventData.flatMap(
              ({ factoryLogs }) => factoryLogs.map((log) => ({ log }))
            ),
            shouldUpdateCheckpoint: false,
            chainId: network.chainId
          }),
          args.syncStore.insertTransactions({
            transactions: finalizedEventData.flatMap(
              ({ transactions }) => transactions
            ),
            chainId: network.chainId
          }),
          args.syncStore.insertTransactionReceipts({
            transactionReceipts: finalizedEventData.flatMap(
              ({ transactionReceipts }) => transactionReceipts
            ),
            chainId: network.chainId
          }),
          args.syncStore.insertCallTraces({
            callTraces: finalizedEventData.flatMap(
              ({ callTraces, block }) => callTraces.map((callTrace) => ({ callTrace, block }))
            ),
            chainId: network.chainId
          })
        ]);
        await Promise.all(
          args.sources.filter(({ filter }) => filter.chainId === network.chainId).map(
            ({ filter }) => args.syncStore.insertInterval({ filter, interval })
          )
        );
        if (isSyncEnd(syncProgress)) {
          args.common.metrics.ponder_sync_is_realtime.set(
            { network: network.name },
            0
          );
          args.common.metrics.ponder_sync_is_complete.set(
            { network: network.name },
            1
          );
          args.common.logger.info({
            service: "sync",
            msg: `Synced final end block for '${network.name}' (${hexToNumber8(syncProgress.end.number)}), killing realtime sync service`
          });
          realtimeSync.kill();
        }
        break;
      }
      case "reorg": {
        syncProgress.current = event.block;
        const checkpoint = getOmnichainCheckpoint("current");
        args.common.metrics.ponder_sync_block.set(
          { network: network.name },
          hexToNumber8(syncProgress.current.number)
        );
        localSyncContext.get(network).unfinalizedEventData = unfinalizedEventData.filter(
          (led) => hexToNumber8(led.block.number) <= hexToNumber8(event.block.number)
        );
        const reorgedHashes = /* @__PURE__ */ new Set();
        for (const b of event.reorgedBlocks) {
          reorgedHashes.add(b.hash);
        }
        pendingEvents = pendingEvents.filter(
          (e) => reorgedHashes.has(e.block.hash) === false
        );
        await args.syncStore.pruneRpcRequestResult({
          blocks: event.reorgedBlocks,
          chainId: network.chainId
        });
        args.onRealtimeEvent({ type: "reorg", checkpoint });
        break;
      }
      default:
        never(event);
    }
  };
  return {
    getEvents,
    async startRealtime() {
      for (const network of args.networks) {
        const { syncProgress, realtimeSync } = localSyncContext.get(network);
        status[network.name].block = {
          number: hexToNumber8(syncProgress.current.number),
          timestamp: hexToNumber8(syncProgress.current.timestamp)
        };
        status[network.name].ready = true;
        if (isSyncEnd(syncProgress)) {
          args.common.metrics.ponder_sync_is_complete.set(
            { network: network.name },
            1
          );
        } else {
          args.common.metrics.ponder_sync_is_realtime.set(
            { network: network.name },
            1
          );
          const initialChildAddresses = /* @__PURE__ */ new Map();
          for (const { filter } of args.sources) {
            if (filter.chainId === network.chainId && "address" in filter && isAddressFactory(filter.address)) {
              const addresses = await args.syncStore.getChildAddresses({
                filter: filter.address
              });
              initialChildAddresses.set(filter.address, new Set(addresses));
            }
          }
          realtimeSync.start({ syncProgress, initialChildAddresses });
        }
      }
    },
    getStartCheckpoint() {
      return getOmnichainCheckpoint("start");
    },
    getFinalizedCheckpoint() {
      return getOmnichainCheckpoint("finalized");
    },
    getStatus() {
      return status;
    },
    getCachedTransport(network) {
      const { requestQueue } = localSyncContext.get(network);
      return cachedTransport({ requestQueue, syncStore: args.syncStore });
    },
    async kill() {
      isKilled = true;
      const promises = [];
      for (const network of args.networks) {
        const { historicalSync, realtimeSync } = localSyncContext.get(network);
        historicalSync.kill();
        promises.push(realtimeSync.kill());
      }
      await Promise.all(promises);
    }
  };
};
var syncDiagnostic = async ({
  common,
  sources,
  network,
  requestQueue
}) => {
  const start3 = Math.min(...sources.map(({ filter }) => filter.fromBlock ?? 0));
  const end = sources.some(({ filter }) => filter.toBlock === void 0) ? void 0 : Math.max(...sources.map(({ filter }) => filter.toBlock));
  const [remoteChainId, startBlock, endBlock, latestBlock] = await Promise.all([
    requestQueue.request({ method: "eth_chainId" }),
    _eth_getBlockByNumber(requestQueue, { blockNumber: start3 }),
    end === void 0 ? void 0 : _eth_getBlockByNumber(requestQueue, { blockNumber: end }),
    _eth_getBlockByNumber(requestQueue, { blockTag: "latest" })
  ]);
  if (hexToNumber8(remoteChainId) !== network.chainId) {
    common.logger.warn({
      service: "sync",
      msg: `Remote chain ID (${remoteChainId}) does not match configured chain ID (${network.chainId}) for network "${network.name}"`
    });
  }
  const finalizedBlockNumber = Math.max(
    0,
    hexToNumber8(latestBlock.number) - network.finalityBlockCount
  );
  const finalizedBlock = await _eth_getBlockByNumber(requestQueue, {
    blockNumber: finalizedBlockNumber
  });
  return {
    start: startBlock,
    end: endBlock,
    finalized: finalizedBlock
  };
};
var getCachedBlock = ({
  sources,
  requestQueue,
  historicalSync
}) => {
  const latestCompletedBlocks = sources.map(({ filter }) => {
    const requiredInterval = [
      filter.fromBlock,
      filter.toBlock ?? Number.POSITIVE_INFINITY
    ];
    const cachedIntervals = historicalSync.intervalsCache.get(filter);
    const completedIntervals = sortIntervals(
      intervalIntersection([requiredInterval], cachedIntervals)
    );
    if (completedIntervals.length === 0)
      return void 0;
    const earliestCompletedInterval = completedIntervals[0];
    if (earliestCompletedInterval[0] !== filter.fromBlock)
      return void 0;
    return earliestCompletedInterval[1];
  });
  const minCompletedBlock = Math.min(
    ...latestCompletedBlocks.filter(
      (block) => block !== void 0
    )
  );
  if (latestCompletedBlocks.every(
    (block, i) => block !== void 0 || sources[i].filter.fromBlock > minCompletedBlock
  )) {
    return _eth_getBlockByNumber(requestQueue, {
      blockNumber: minCompletedBlock
    });
  }
  return void 0;
};
async function* localHistoricalSyncGenerator({
  common,
  network,
  syncProgress,
  historicalSync,
  showLogs
}) {
  if (hexToNumber8(syncProgress.start.number) > hexToNumber8(syncProgress.finalized.number)) {
    syncProgress.current = syncProgress.finalized;
    common.metrics.ponder_sync_block.set(
      { network: network.name },
      hexToNumber8(syncProgress.current.number)
    );
    if (showLogs) {
      common.logger.warn({
        service: "historical",
        msg: `Skipped historical sync for '${network.name}' because the start block is not finalized`
      });
    }
    const label2 = { network: network.name };
    common.metrics.ponder_historical_total_blocks.set(label2, 0);
    common.metrics.ponder_historical_cached_blocks.set(label2, 0);
    return;
  }
  const historicalLast = getHistoricalLast(syncProgress);
  const totalInterval = [
    hexToNumber8(syncProgress.start.number),
    hexToNumber8(historicalLast.number)
  ];
  const requiredIntervals = Array.from(
    historicalSync.intervalsCache.entries()
  ).flatMap(
    ([filter, interval]) => intervalDifference(
      [
        [
          filter.fromBlock,
          Math.min(
            filter.toBlock ?? Number.POSITIVE_INFINITY,
            totalInterval[1]
          )
        ]
      ],
      interval
    )
  );
  const required = intervalSum(intervalUnion(requiredIntervals));
  const total = totalInterval[1] - totalInterval[0] + 1;
  const label = { network: network.name };
  common.metrics.ponder_historical_total_blocks.set(label, total);
  common.metrics.ponder_historical_cached_blocks.set(label, total - required);
  if (showLogs) {
    common.logger.info({
      service: "historical",
      msg: `Started syncing '${network.name}' with ${formatPercentage(
        (total - required) / total
      )} cached`
    });
  }
  let estimateRange = 25;
  let fromBlock = hexToNumber8(syncProgress.start.number);
  if (syncProgress.current !== void 0 && (syncProgress.cached === void 0 || hexToNumber8(syncProgress.current.number) > hexToNumber8(syncProgress.cached.number))) {
    fromBlock = hexToNumber8(syncProgress.current.number) + 1;
  } else if (syncProgress.cached !== void 0) {
    yield;
    if (hexToNumber8(syncProgress.cached.number) === hexToNumber8(historicalLast.number)) {
      if (showLogs) {
        common.logger.info({
          service: "historical",
          msg: `Skipped historical sync for '${network.name}' because all blocks are cached.`
        });
      }
      return;
    }
    fromBlock = hexToNumber8(syncProgress.cached.number) + 1;
  }
  while (true) {
    const interval = [
      Math.min(fromBlock, hexToNumber8(historicalLast.number)),
      Math.min(fromBlock + estimateRange, hexToNumber8(historicalLast.number))
    ];
    const endClock = startClock();
    const syncBlock = await historicalSync.sync(interval);
    fromBlock = interval[1] + 1;
    if (syncBlock === void 0) {
      if (interval[1] === hexToNumber8(historicalLast.number)) {
        syncProgress.current = historicalLast;
      } else {
        continue;
      }
    } else {
      if (interval[1] === hexToNumber8(historicalLast.number)) {
        syncProgress.current = historicalLast;
      } else {
        syncProgress.current = syncBlock;
      }
      const duration = endClock();
      common.metrics.ponder_sync_block.set(
        { network: network.name },
        hexToNumber8(syncProgress.current.number)
      );
      common.metrics.ponder_historical_duration.observe(label, duration);
      common.metrics.ponder_historical_completed_blocks.inc(
        label,
        interval[1] - interval[0] + 1
      );
      estimateRange = Math.min(
        Math.max(
          25,
          Math.round(1e3 * (interval[1] - interval[0]) / duration)
        ),
        estimateRange * 2,
        1e5
      );
    }
    yield;
    if (isSyncEnd(syncProgress) || isSyncFinalized(syncProgress)) {
      return;
    }
  }
}

// src/bin/utils/run.ts
async function run({
  common,
  build,
  database,
  onFatalError,
  onReloadableError
}) {
  const {
    instanceId,
    networks,
    sources,
    schema,
    indexingFunctions,
    graphqlSchema
  } = build;
  let isKilled = false;
  const { checkpoint: initialCheckpoint } = await database.setup();
  const syncStore = createSyncStore({
    common,
    db: database.qb.sync
  });
  const metadataStore = getMetadataStore({
    db: database.qb.user,
    instanceId
  });
  await database.migrateSync();
  runCodegen({ common, graphqlSchema });
  const sync = await createSync({
    common,
    syncStore,
    networks,
    sources,
    // Note: this is not great because it references the
    // `realtimeQueue` which isn't defined yet
    onRealtimeEvent: (realtimeEvent) => {
      return realtimeQueue.add(realtimeEvent);
    },
    onFatalError,
    initialCheckpoint
  });
  const handleEvents = async (events, checkpoint) => {
    if (events.length === 0)
      return { status: "success" };
    indexingService.updateTotalSeconds(decodeCheckpoint(checkpoint));
    return await indexingService.processEvents({ events });
  };
  const realtimeQueue = createQueue({
    initialStart: true,
    browser: false,
    concurrency: 1,
    worker: async (event) => {
      switch (event.type) {
        case "block": {
          for (const events of splitEvents(event.events)) {
            const result = await handleEvents(
              decodeEvents(common, sources, events),
              event.checkpoint
            );
            if (result.status === "error")
              onReloadableError(result.error);
            await database.complete({ checkpoint: event.checkpoint });
          }
          await metadataStore.setStatus(event.status);
          break;
        }
        case "reorg":
          await database.removeTriggers();
          await database.revert({ checkpoint: event.checkpoint });
          await database.createTriggers();
          break;
        case "finalize":
          await database.finalize({ checkpoint: event.checkpoint });
          break;
        default:
          never(event);
      }
    }
  });
  const indexingService = createIndexingService({
    indexingFunctions,
    common,
    sources,
    networks,
    sync
  });
  const historicalIndexingStore = createHistoricalIndexingStore({
    common,
    database,
    schema,
    initialCheckpoint
  });
  indexingService.setIndexingStore(historicalIndexingStore);
  await metadataStore.setStatus(sync.getStatus());
  const start3 = async () => {
    if (encodeCheckpoint(zeroCheckpoint) === initialCheckpoint) {
      const result = await indexingService.processSetupEvents({
        sources,
        networks
      });
      if (result.status === "killed") {
        return;
      } else if (result.status === "error") {
        onReloadableError(result.error);
        return;
      }
    }
    let end;
    let lastFlush = Date.now();
    for await (const { events, checkpoint } of sync.getEvents()) {
      end = checkpoint;
      const result = await handleEvents(
        decodeEvents(common, sources, events),
        checkpoint
      );
      if (historicalIndexingStore.isCacheFull() && events.length > 0 || common.options.command === "dev" && lastFlush + 5e3 < Date.now() && events.length > 0) {
        await database.finalize({
          checkpoint: encodeCheckpoint(zeroCheckpoint)
        });
        await historicalIndexingStore.flush();
        await database.complete({
          checkpoint: encodeCheckpoint(zeroCheckpoint)
        });
        await database.finalize({
          checkpoint: events[events.length - 1].checkpoint
        });
        lastFlush = Date.now();
      }
      await metadataStore.setStatus(sync.getStatus());
      if (result.status === "killed") {
        return;
      } else if (result.status === "error") {
        onReloadableError(result.error);
        return;
      }
    }
    if (isKilled)
      return;
    await database.finalize({ checkpoint: encodeCheckpoint(zeroCheckpoint) });
    await historicalIndexingStore.flush();
    await database.complete({ checkpoint: encodeCheckpoint(zeroCheckpoint) });
    await database.finalize({ checkpoint: sync.getFinalizedCheckpoint() });
    const start4 = sync.getStartCheckpoint();
    common.metrics.ponder_indexing_completed_seconds.set(
      decodeCheckpoint(end ?? start4).blockTimestamp - decodeCheckpoint(start4).blockTimestamp
    );
    common.metrics.ponder_indexing_total_seconds.set(
      decodeCheckpoint(end ?? start4).blockTimestamp - decodeCheckpoint(start4).blockTimestamp
    );
    common.metrics.ponder_indexing_completed_timestamp.set(
      decodeCheckpoint(end ?? start4).blockTimestamp
    );
    common.logger.info({
      service: "indexing",
      msg: "Completed historical indexing"
    });
    await database.createIndexes();
    await database.createLiveViews();
    await database.createTriggers();
    indexingService.setIndexingStore(
      createRealtimeIndexingStore({
        database,
        schema,
        common
      })
    );
    await sync.startRealtime();
    await metadataStore.setStatus(sync.getStatus());
    common.logger.info({
      service: "server",
      msg: "Started responding as healthy"
    });
  };
  const startPromise = start3();
  return async () => {
    isKilled = true;
    indexingService.kill();
    await sync.kill();
    realtimeQueue.pause();
    realtimeQueue.clear();
    await realtimeQueue.onIdle();
    await startPromise;
    await database.unlock();
  };
}

// src/server/index.ts
import http from "node:http";

// src/hono/index.ts
var applyHonoRoutes = (hono, routes, customContext) => {
  const customContextWrapper = (handler) => (c, next) => {
    for (const key of Object.keys(customContext ?? {})) {
      c[key] = customContext[key];
    }
    return handler(c, next);
  };
  for (const {
    method,
    pathOrHandlers: [maybePathOrHandler, ...handlers]
  } of routes) {
    let path9 = "/";
    if (method === "GET" || method === "POST") {
      if (typeof maybePathOrHandler === "string") {
        path9 = maybePathOrHandler;
      } else {
        hono[method === "GET" ? "get" : "put"](
          path9,
          customContextWrapper(maybePathOrHandler)
        );
      }
      for (const handler of handlers) {
        if (typeof handler !== "string") {
          hono[method === "GET" ? "get" : "put"](
            path9,
            customContextWrapper(handler)
          );
        }
      }
    } else {
      if (typeof maybePathOrHandler === "string") {
        path9 = maybePathOrHandler;
      } else {
        path9 = "*";
        handlers.unshift(maybePathOrHandler);
      }
      for (const handler of handlers) {
        hono.use(path9, customContextWrapper(handler));
      }
    }
  }
  return hono;
};

// src/server/index.ts
import { serve } from "@hono/node-server";
import { Hono } from "hono";
import { cors } from "hono/cors";
import { createMiddleware } from "hono/factory";
import { createHttpTerminator } from "http-terminator";

// src/server/error.ts
import { html } from "hono/html";
var onError = async (_error, c, common) => {
  const error = _error;
  const regex = /(\S+\.(?:js|ts|mjs|cjs)):\d+:\d+/;
  const matches = error.stack?.match(regex);
  const errorFile = (() => {
    if (!matches?.[0])
      return void 0;
    const path9 = matches[0].trim();
    if (path9.startsWith("(")) {
      return path9.slice(1);
    } else if (path9.startsWith("file://")) {
      return path9.slice(7);
    }
    return path9;
  })();
  addStackTrace(error, common.options);
  error.meta = Array.isArray(error.meta) ? error.meta : [];
  error.meta.push(
    `Request:
${prettyPrint({
      path: c.req.path,
      method: c.req.method,
      body: await tryExtractRequestBody(c.req)
    })}`
  );
  common.logger.warn({
    service: "server",
    msg: `An error occurred while handling a '${c.req.method}' request to the route '${c.req.path}'`,
    error
  });
  return c.text(
    `${error.name}: ${error.message} occurred in '${errorFile}' while handling a '${c.req.method}' request to the route '${c.req.path}'`,
    500
  );
};
var tryExtractRequestBody = async (request) => {
  try {
    return await request.json();
  } catch {
    try {
      const text = await request.text();
      if (text !== "")
        return text;
    } catch {
    }
  }
  return void 0;
};

// src/server/index.ts
async function createServer2({
  app: userApp,
  routes: userRoutes,
  common,
  graphqlSchema,
  database,
  instanceId
}) {
  const metadataStore = instanceId === void 0 ? getLiveMetadataStore({ db: database.qb.readonly }) : getMetadataStore({
    db: database.qb.readonly,
    instanceId
  });
  const metricsMiddleware = createMiddleware(async (c, next) => {
    const matchedPathLabels = c.req.matchedRoutes.filter((r) => r.path !== "/*").map((r) => ({ method: c.req.method, path: r.path }));
    for (const labels of matchedPathLabels) {
      common.metrics.ponder_http_server_active_requests.inc(labels);
    }
    const endClock = startClock();
    try {
      await next();
    } finally {
      const requestSize = Number(c.req.header("Content-Length") ?? 0);
      const responseSize = Number(c.res.headers.get("Content-Length") ?? 0);
      const responseDuration = endClock();
      const status = c.res.status >= 200 && c.res.status < 300 ? "2XX" : c.res.status >= 300 && c.res.status < 400 ? "3XX" : c.res.status >= 400 && c.res.status < 500 ? "4XX" : "5XX";
      for (const labels of matchedPathLabels) {
        common.metrics.ponder_http_server_active_requests.dec(labels);
        common.metrics.ponder_http_server_request_size_bytes.observe(
          { ...labels, status },
          requestSize
        );
        common.metrics.ponder_http_server_response_size_bytes.observe(
          { ...labels, status },
          responseSize
        );
        common.metrics.ponder_http_server_request_duration_ms.observe(
          { ...labels, status },
          responseDuration
        );
      }
    }
  });
  const contextMiddleware = createMiddleware(async (c, next) => {
    c.set("db", database.drizzle);
    c.set("metadataStore", metadataStore);
    c.set("graphqlSchema", graphqlSchema);
    await next();
  });
  const hono = new Hono().use(metricsMiddleware).use(cors({ origin: "*", maxAge: 86400 })).get("/metrics", async (c) => {
    try {
      const metrics = await common.metrics.getMetrics();
      return c.text(metrics);
    } catch (error) {
      return c.json(error, 500);
    }
  }).get("/health", (c) => {
    return c.text("", 200);
  }).get("/ready", async (c) => {
    const status = await metadataStore.getStatus();
    if (status !== null && Object.values(status).every(({ ready }) => ready === true)) {
      return c.text("", 200);
    }
    return c.text("Historical indexing is not complete.", 503);
  }).get("/status", async (c) => {
    const status = await metadataStore.getStatus();
    return c.json(status);
  }).use(contextMiddleware);
  if (userRoutes.length === 0 && userApp.routes.length === 0) {
    hono.use("/graphql", graphql());
    hono.use("/", graphql());
  } else {
    applyHonoRoutes(hono, userRoutes, { db: database.drizzle }).onError(
      (error, c) => onError(error, c, common)
    );
    common.logger.debug({
      service: "server",
      msg: `Detected a custom server with routes: [${userRoutes.map(({ pathOrHandlers: [maybePathOrHandler] }) => maybePathOrHandler).filter((maybePathOrHandler) => typeof maybePathOrHandler === "string").join(", ")}]`
    });
    hono.route("/", userApp);
  }
  let port = common.options.port;
  const createServerWithNextAvailablePort = (...args) => {
    const httpServer2 = http.createServer(...args);
    const errorHandler = (error) => {
      if (error.code === "EADDRINUSE") {
        common.logger.warn({
          service: "server",
          msg: `Port ${port} was in use, trying port ${port + 1}`
        });
        port += 1;
        setTimeout(() => {
          httpServer2.close();
          httpServer2.listen(port, common.options.hostname);
        }, 5);
      }
    };
    const listenerHandler = () => {
      common.metrics.ponder_http_server_port.set(port);
      common.logger.info({
        service: "server",
        msg: `Started listening on port ${port}`
      });
      httpServer2.off("error", errorHandler);
    };
    httpServer2.on("error", errorHandler);
    httpServer2.on("listening", listenerHandler);
    return httpServer2;
  };
  const httpServer = await new Promise((resolve2, reject) => {
    const timeout = setTimeout(() => {
      reject(new Error("HTTP server failed to start within 5 seconds."));
    }, 5e3);
    const httpServer2 = serve(
      {
        fetch: hono.fetch,
        createServer: createServerWithNextAvailablePort,
        port,
        // Note that common.options.hostname can be undefined if the user did not specify one.
        // In this case, Node.js uses `::` if IPv6 is available and `0.0.0.0` otherwise.
        // https://nodejs.org/api/net.html#serverlistenport-host-backlog-callback
        hostname: common.options.hostname
      },
      () => {
        clearTimeout(timeout);
        resolve2(httpServer2);
      }
    );
  });
  const terminator = createHttpTerminator({
    server: httpServer,
    gracefulTerminationTimeout: 1e3
  });
  return {
    hono,
    port,
    kill: () => terminator.terminate()
  };
}

// src/bin/utils/runServer.ts
async function runServer({
  common,
  build,
  database
}) {
  const { instanceId, graphqlSchema } = build;
  const server = await createServer2({
    app: build.app,
    routes: build.routes,
    common,
    graphqlSchema,
    database,
    instanceId
  });
  return async () => {
    await server.kill();
  };
}

// src/bin/commands/dev.ts
async function dev({ cliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat
  });
  const [major, minor, _patch] = process.versions.node.split(".").map(Number);
  if (major < 18 || major === 18 && minor < 14) {
    logger.fatal({
      service: "process",
      msg: `Invalid Node.js version. Expected >=18.14, detected ${major}.${minor}.`
    });
    await logger.kill();
    process.exit(1);
  }
  if (!existsSync2(path6.join(options.rootDir, ".env.local"))) {
    logger.warn({
      service: "app",
      msg: "Local environment file (.env.local) not found"
    });
  }
  const configRelPath = path6.relative(options.rootDir, options.configFile);
  logger.debug({
    service: "app",
    msg: `Started using config file: ${configRelPath}`
  });
  const metrics = new MetricsService();
  const telemetry = createTelemetry({ options, logger });
  const common = { options, logger, metrics, telemetry };
  const buildService = await createBuildService({ common });
  const ui = createUi({ common });
  let indexingCleanupReloadable = () => Promise.resolve();
  let apiCleanupReloadable = () => Promise.resolve();
  const cleanup = async () => {
    await indexingCleanupReloadable();
    await apiCleanupReloadable();
    if (database) {
      await database.kill();
    }
    await buildService.kill();
    await telemetry.kill();
    ui.kill();
  };
  const shutdown = setupShutdown({ common, cleanup });
  const buildQueue = createQueue({
    initialStart: true,
    concurrency: 1,
    worker: async (result) => {
      if (result.kind === "indexing") {
        await indexingCleanupReloadable();
      }
      await apiCleanupReloadable();
      if (result.status === "success") {
        if (result.kind === "indexing") {
          metrics.resetIndexingMetrics();
          if (database) {
            await database.kill();
          }
          database = createDatabase({
            common,
            schema: result.indexingBuild.schema,
            databaseConfig: result.indexingBuild.databaseConfig,
            buildId: result.indexingBuild.buildId,
            instanceId: result.indexingBuild.instanceId,
            namespace: result.indexingBuild.namespace,
            statements: result.indexingBuild.statements
          });
          indexingCleanupReloadable = await run({
            common,
            build: result.indexingBuild,
            database,
            onFatalError: () => {
              shutdown({ reason: "Received fatal error", code: 1 });
            },
            onReloadableError: (error) => {
              buildQueue.clear();
              buildQueue.add({ status: "error", kind: "indexing", error });
            }
          });
        }
        metrics.resetApiMetrics();
        apiCleanupReloadable = await runServer({
          common,
          build: result.apiBuild,
          database
        });
      } else {
        metrics.ponder_indexing_has_error.set(1);
        if (result.kind === "indexing") {
          indexingCleanupReloadable = () => Promise.resolve();
        }
        apiCleanupReloadable = () => Promise.resolve();
      }
    }
  });
  let database;
  const buildResult = await buildService.start({
    watch: true,
    onBuild: (buildResult2) => {
      buildQueue.clear();
      buildQueue.add(buildResult2);
    }
  });
  if (buildResult.status === "error") {
    await shutdown({ reason: "Failed intial build", code: 1 });
    return cleanup;
  }
  telemetry.record({
    name: "lifecycle:session_start",
    properties: {
      cli_command: "dev",
      ...buildPayload(buildResult.indexingBuild)
    }
  });
  buildQueue.add({ ...buildResult, kind: "indexing" });
  return async () => {
    buildQueue.pause();
    await cleanup();
  };
}

// src/bin/commands/serve.ts
import path7 from "node:path";
async function serve2({ cliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat
  });
  const [major, minor, _patch] = process.versions.node.split(".").map(Number);
  if (major < 18 || major === 18 && minor < 14) {
    logger.fatal({
      service: "process",
      msg: `Invalid Node.js version. Expected >=18.14, detected ${major}.${minor}.`
    });
    await logger.kill();
    process.exit(1);
  }
  const configRelPath = path7.relative(options.rootDir, options.configFile);
  logger.debug({
    service: "app",
    msg: `Started using config file: ${configRelPath}`
  });
  const metrics = new MetricsService();
  const telemetry = createTelemetry({ options, logger });
  const common = { options, logger, metrics, telemetry };
  const buildService = await createBuildService({ common });
  let cleanupReloadable = () => Promise.resolve();
  const cleanup = async () => {
    await cleanupReloadable();
    await telemetry.kill();
  };
  const shutdown = setupShutdown({ common, cleanup });
  const buildResult = await buildService.start({ watch: false });
  await buildService.kill();
  if (buildResult.status === "error") {
    await shutdown({ reason: "Failed intial build", code: 1 });
    return cleanup;
  }
  telemetry.record({
    name: "lifecycle:session_start",
    properties: {
      cli_command: "serve",
      ...buildPayload(buildResult.indexingBuild)
    }
  });
  const { databaseConfig, schema, instanceId, buildId, statements, namespace } = buildResult.apiBuild;
  if (databaseConfig.kind === "pglite") {
    await shutdown({
      reason: "The 'ponder serve' command does not support PGlite",
      code: 1
    });
    return cleanup;
  }
  const database = createDatabase({
    common,
    schema,
    databaseConfig,
    instanceId,
    buildId,
    statements,
    namespace
  });
  const server = await createServer2({
    common,
    app: buildResult.apiBuild.app,
    routes: buildResult.apiBuild.routes,
    graphqlSchema: buildResult.indexingBuild.graphqlSchema,
    database,
    instanceId: process.env.PONDER_EXPERIMENTAL_INSTANCE_ID === void 0 ? void 0 : instanceId
  });
  cleanupReloadable = async () => {
    await server.kill();
    await database.kill();
  };
  return cleanup;
}

// src/bin/commands/start.ts
import path8 from "node:path";
async function start2({ cliOptions }) {
  const options = buildOptions({ cliOptions });
  const logger = createLogger({
    level: options.logLevel,
    mode: options.logFormat
  });
  const [major, minor, _patch] = process.versions.node.split(".").map(Number);
  if (major < 18 || major === 18 && minor < 14) {
    logger.fatal({
      service: "process",
      msg: `Invalid Node.js version. Expected >=18.14, detected ${major}.${minor}.`
    });
    await logger.kill();
    process.exit(1);
  }
  const configRelPath = path8.relative(options.rootDir, options.configFile);
  logger.debug({
    service: "app",
    msg: `Started using config file: ${configRelPath}`
  });
  const metrics = new MetricsService();
  const telemetry = createTelemetry({ options, logger });
  const common = { options, logger, metrics, telemetry };
  const buildService = await createBuildService({ common });
  let cleanupReloadable = () => Promise.resolve();
  let cleanupReloadableServer = () => Promise.resolve();
  const cleanup = async () => {
    await cleanupReloadable();
    await cleanupReloadableServer();
    if (database) {
      await database.kill();
    }
    await telemetry.kill();
  };
  const shutdown = setupShutdown({ common, cleanup });
  const buildResult = await buildService.start({ watch: false });
  await buildService.kill();
  if (buildResult.status === "error") {
    await shutdown({ reason: "Failed intial build", code: 1 });
    return cleanup;
  }
  telemetry.record({
    name: "lifecycle:session_start",
    properties: {
      cli_command: "start",
      ...buildPayload(buildResult.indexingBuild)
    }
  });
  const database = createDatabase({
    common,
    schema: buildResult.indexingBuild.schema,
    databaseConfig: buildResult.indexingBuild.databaseConfig,
    buildId: buildResult.indexingBuild.buildId,
    instanceId: buildResult.indexingBuild.instanceId,
    namespace: buildResult.indexingBuild.namespace,
    statements: buildResult.indexingBuild.statements
  });
  cleanupReloadable = await run({
    common,
    build: buildResult.indexingBuild,
    database,
    onFatalError: () => {
      shutdown({ reason: "Received fatal error", code: 1 });
    },
    onReloadableError: () => {
      shutdown({ reason: "Encountered indexing error", code: 1 });
    }
  });
  cleanupReloadableServer = await runServer({
    common,
    build: buildResult.apiBuild,
    database
  });
  return cleanup;
}

// src/bin/ponder.ts
dotenv.config({ path: ".env.local" });
var __dirname = dirname(fileURLToPath(import.meta.url));
var packageJsonPath = resolve(__dirname, "../../package.json");
var packageJson = JSON.parse(
  readFileSync4(packageJsonPath, { encoding: "utf8" })
);
var ponder = new Command("ponder").usage("<command> [OPTIONS]").helpOption("-h, --help", "Show this help message").helpCommand(false).option(
  "--root <PATH>",
  "Path to the project root directory (default: working directory)"
).option(
  "--config <PATH>",
  "Path to the project config file",
  "ponder.config.ts"
).option(
  "-v, --debug",
  "Enable debug logs, e.g. realtime blocks, internal events"
).option(
  "-vv, --trace",
  "Enable trace logs, e.g. db queries, indexing checkpoints"
).option(
  "--log-level <LEVEL>",
  'Minimum log level ("error", "warn", "info", "debug", or "trace", default: "info")'
).option(
  "--log-format <FORMAT>",
  'The log format ("pretty" or "json")',
  "pretty"
).version(packageJson.version, "-V, --version", "Show the version number").configureHelp({ showGlobalOptions: true }).allowExcessArguments(false).showHelpAfterError().enablePositionalOptions(false);
var devCommand = new Command("dev").description("Start the development server with hot reloading").option("-p, --port <PORT>", "Port for the web server", Number, 42069).option(
  "-H, --hostname <HOSTNAME>",
  'Hostname for the web server (default: "0.0.0.0" or "::")'
).showHelpAfterError().action(async (_, command) => {
  const cliOptions = {
    ...command.optsWithGlobals(),
    command: command.name()
  };
  await dev({ cliOptions });
});
var startCommand = new Command("start").description("Start the production server").option("-p, --port <PORT>", "Port for the web server", Number, 42069).option(
  "-H, --hostname <HOSTNAME>",
  'Hostname for the web server (default: "0.0.0.0" or "::")'
).showHelpAfterError().action(async (_, command) => {
  const cliOptions = {
    ...command.optsWithGlobals(),
    command: command.name()
  };
  await start2({ cliOptions });
});
var serveCommand = new Command("serve").description("Start the production HTTP server without the indexer").option("-p, --port <PORT>", "Port for the web server", Number, 42069).option(
  "-H, --hostname <HOSTNAME>",
  'Hostname for the web server (default: "0.0.0.0" or "::")'
).showHelpAfterError().action(async (_, command) => {
  const cliOptions = {
    ...command.optsWithGlobals(),
    command: command.name()
  };
  await serve2({ cliOptions });
});
var codegenCommand = new Command("codegen").description("Generate the schema.graphql file, then exit").showHelpAfterError().action(async (_, command) => {
  const cliOptions = {
    ...command.optsWithGlobals(),
    command: command.name()
  };
  await codegen({ cliOptions });
});
ponder.addCommand(devCommand);
ponder.addCommand(startCommand);
ponder.addCommand(serveCommand);
ponder.addCommand(codegenCommand);
await ponder.parseAsync();
//# sourceMappingURL=ponder.js.map